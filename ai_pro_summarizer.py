# -*- coding: utf-8 -*-

import google.generativeai as genai
import os
import json
import re
import time
from typing import Optional, Dict, Any, List
import logging
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ProSummaryConfig:
    """Proçµ±åˆè¦ç´„ã®è¨­å®šã‚¯ãƒ©ã‚¹"""
    enabled: bool = True
    min_articles_threshold: int = 10
    max_daily_executions: int = 3
    execution_hours: List[int] = None  # [9, 15, 21] # JST
    cost_limit_monthly: float = 50.0  # USD
    timeout_seconds: int = 180
    model_name: str = "gemini-2.5-pro"
    
    def __post_init__(self):
        if self.execution_hours is None:
            self.execution_hours = list(range(24))  # 24æ™‚é–“ã„ã¤ã§ã‚‚å®Ÿè¡Œå¯èƒ½


class ProSummarizer:
    """Gemini 2.5 Proã«ã‚ˆã‚‹çµ±åˆè¦ç´„æ©Ÿèƒ½"""
    
    def __init__(self, api_key: str, config: ProSummaryConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key (str): Google Gemini APIã‚­ãƒ¼
            config (ProSummaryConfig): è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.api_key = api_key
        self.config = config or ProSummaryConfig()
        self.logger = logging.getLogger(__name__)
        
        if not api_key:
            raise ValueError("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if len(api_key) < 20:  # åŸºæœ¬çš„ãªé•·ã•ãƒã‚§ãƒƒã‚¯
            raise ValueError("Gemini APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ï¼ˆé•·ã•ãŒçŸ­ã™ãã¾ã™ï¼‰")
        
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(self.config.model_name)
            self.logger.info(f"Gemini APIãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ (ãƒ¢ãƒ‡ãƒ«: {self.config.model_name})")
        except Exception as e:
            self.logger.error(f"Gemini APIåˆæœŸåŒ–å¤±æ•—: {e}")
            raise
    
    
    def generate_unified_summary(self, grouped_articles: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """
        ä¸€æ‹¬çµ±åˆè¦ç´„ã‚’ç”Ÿæˆï¼ˆåœ°åŸŸé–¢é€£æ€§ã‚’è€ƒæ…®ï¼‰
        
        Args:
            grouped_articles (Dict[str, List[Dict]]): åœ°åŸŸåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸè¨˜äº‹ç¾¤
                ä¾‹: {"japan": [{"summary": "...", "title": "...", "category": "..."}], ...}
        
        Returns:
            Dict[str, Any]: çµ±åˆè¦ç´„çµæœï¼ˆåœ°åŸŸåˆ¥+å…¨ä½“+é–¢é€£æ€§åˆ†æï¼‰
        """
        start_time = time.time()
        total_articles = sum(len(articles) for articles in grouped_articles.values())
        self.logger.info(f"ä¸€æ‹¬çµ±åˆè¦ç´„ç”Ÿæˆé–‹å§‹ (ç·è¨˜äº‹æ•°: {total_articles}, åœ°åŸŸæ•°: {len(grouped_articles)})")
        
        # è¨˜äº‹æ•°åˆ¶é™ãªã—ï¼ˆå…¨è¨˜äº‹ã‚’çµ±åˆè¦ç´„ã«ä½¿ç”¨ï¼‰
        try:
            prompt = self._build_unified_prompt(grouped_articles)
            self.logger.info(f"çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº†: {len(prompt)}æ–‡å­—")
            
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=8192,  # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å€å¢—ï¼ˆåœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã®å®Œå…¨æ€§ç¢ºä¿ï¼‰
                    temperature=0.3,
                ),
                request_options={"timeout": 600}  # 600ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ10åˆ†ï¼‰
            )
            
            if not response:
                raise Exception("Gemini APIã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            if not hasattr(response, 'text') or not response.text:
                raise Exception(f"Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“: {response}")
            
            self.logger.info(f"çµ±åˆè¦ç´„APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡: {len(response.text)}æ–‡å­—")
            
            processing_time_ms = int((time.time() - start_time) * 1000)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            parsed_result = self._parse_unified_response(response.text)
            
            if parsed_result:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å®Œå…¨æ€§ã‚’æ¤œè¨¼
                validation_result = self._validate_response_completeness(parsed_result)
                
                result = {
                    "unified_summary": parsed_result,
                    "total_articles": total_articles,
                    "processing_time_ms": processing_time_ms,
                    "model_version": self.config.model_name,
                    "validation": validation_result  # æ¤œè¨¼çµæœã‚’è¿½åŠ 
                }
                
                if validation_result['is_complete']:
                    self.logger.info(f"ä¸€æ‹¬çµ±åˆè¦ç´„å®Œäº† ({processing_time_ms}ms) - å®Œå…¨")
                else:
                    self.logger.warning(f"ä¸€æ‹¬çµ±åˆè¦ç´„å®Œäº† ({processing_time_ms}ms) - ä¸å®Œå…¨: {validation_result['issues']}")
                
                return result
            else:
                self.logger.error("çµ±åˆè¦ç´„ã®è§£æã«å¤±æ•—")
                return None
                
        except Exception as e:
            self.logger.error(f"ğŸš¨ ä¸€æ‹¬çµ±åˆè¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"ğŸš¨ UNIFIED SUMMARY FAILED: {e}")
            return None
    
    
    
    
    def _build_unified_prompt(self, grouped_articles: Dict[str, List[Dict[str, Any]]]) -> str:
        """çµ±åˆè¦ç´„ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ï¼ˆåœ°åŸŸé–“é–¢é€£æ€§åˆ†æã‚’é‡è¦–ï¼‰"""
        
        total_articles = sum(len(articles) for articles in grouped_articles.values())
        
        prompt = f"""ã‚ãªãŸã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«é‡‘èå¸‚å ´ã®ä¸“é–€ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®{total_articles}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã€åœ°åŸŸé–“ã®ç›¸äº’é–¢é€£æ€§ã¨å½±éŸ¿ã‚’æ·±ãè€ƒæ…®ã—ãŸåŒ…æ‹¬çš„ãªå¸‚å ´åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘"""
        
        # åœ°åŸŸåˆ¥è¨˜äº‹ã‚’æ•´ç†ï¼ˆè¡¨ç¤ºé †åºã‚’èª¿æ•´ï¼šç±³å›½â†’æ¬§å·â†’æ—¥æœ¬â†’ä¸­å›½ãƒ»æ–°èˆˆå›½ï¼‰
        region_order = ["usa", "europe", "japan", "china", "asia", "global", "other"]
        sorted_regions = sorted(grouped_articles.keys(), key=lambda x: region_order.index(x) if x in region_order else 999)
        
        for region in sorted_regions:
            articles = grouped_articles[region]
            region_names = {
                "japan": "æ—¥æœ¬", "usa": "ç±³å›½", "china": "ä¸­å›½ãƒ»ãã®ä»–æ–°èˆˆå›½", 
                "europe": "æ¬§å·", "asia": "ã‚¢ã‚¸ã‚¢", "global": "ã‚°ãƒ­ãƒ¼ãƒãƒ«", "other": "ãã®ä»–"
            }
            region_ja = region_names.get(region, region)
            
            prompt += f"\n\nâ– â–  {region_ja}å¸‚å ´ ({len(articles)}ä»¶)\n"
            
            for i, article in enumerate(articles, 1):
                title = article.get("title", "").strip()
                summary = article.get("summary", "").strip()
                category = article.get("category", "ãã®ä»–")
                
                prompt += f"{i}. ã€{category}ã€‘{title}\n   è¦ç´„: {summary}\n"
        
        prompt += f"""

ã€é‡è¦ï¼šHTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‘
ä»¥ä¸‹ã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã«å¾“ã£ã¦ã€åœ°åŸŸé–“ã®ç›¸äº’ä½œç”¨ã¨æ³¢åŠåŠ¹æœã‚’é‡è¦–ã—ãŸç·åˆåˆ†æã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

## åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³
ä»¥ä¸‹ã®HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«å¾“ã£ã¦å‡ºåŠ›ï¼š
<div class="regional-summaries">
<div class="region-item">
<h4>ç±³å›½å¸‚å ´</h4>
<p>[ç±³å›½å¸‚å ´ã®åˆ†æå†…å®¹]</p>
</div>
<div class="region-item">
<h4>æ¬§å·å¸‚å ´</h4>
<p>[æ¬§å·å¸‚å ´ã®åˆ†æå†…å®¹]</p>
</div>
<div class="region-item">
<h4>æ—¥æœ¬å¸‚å ´</h4>
<p>[æ—¥æœ¬å¸‚å ´ã®åˆ†æå†…å®¹]</p>
</div>
<div class="region-item">
<h4>ä¸­å›½ãƒ»ãã®ä»–æ–°èˆˆå›½å¸‚å ´</h4>
<p>[ä¸­å›½ãƒ»æ–°èˆˆå›½å¸‚å ´ã®åˆ†æå†…å®¹]</p>
</div>
</div>

## ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬
<div class="global-overview">
<p>[ä¸–ç•Œå…¨ä½“ã®å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã€ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥å‹•å‘ã‚’400å­—ç¨‹åº¦ã§ç·åˆåˆ†æ]</p>
</div>

## åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æ
<div class="cross-regional-analysis">
<div class="influence-item">
<h5>ç±³å›½é‡‘èæ”¿ç­–ã®å½±éŸ¿</h5>
<p>[ç±³å›½ã®æ”¿ç­–ãŒä»–åœ°åŸŸã«ä¸ãˆã‚‹å½±éŸ¿]</p>
</div>
<div class="influence-item">
<h5>ä¸­å›½çµŒæ¸ˆã®ã‚°ãƒ­ãƒ¼ãƒãƒ«æ³¢åŠ</h5>
<p>[ä¸­å›½çµŒæ¸ˆå‹•å‘ã®ä¸–ç•Œã¸ã®å½±éŸ¿]</p>
</div>
<div class="influence-item">
<h5>æ¬§å·ãƒ»æ—¥æœ¬ã®å¸‚å ´å‹•å‘</h5>
<p>[æ¬§å·ãƒ»æ—¥æœ¬ã®å‹•å‘ã¨åœ°åŸŸé–“ç›¸äº’ä½œç”¨]</p>
</div>
</div>

## æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›
<div class="key-trends">
<p>[é‡è¦ãªå¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã€æŠ€è¡“é€²æ­©ã€æ”¿ç­–æ–¹å‘æ€§ã‚’300å­—ç¨‹åº¦]</p>
</div>

## ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼š
<div class="risk-factors">
<div class="risk-item">
<h5>çŸ­æœŸãƒªã‚¹ã‚¯è¦å› </h5>
<p>[çŸ­æœŸçš„ãªãƒªã‚¹ã‚¯è¦å› ã®åˆ†æ]</p>
</div>
<div class="risk-item">
<h5>æŠ•è³‡æ©Ÿä¼š</h5>
<p>[æŠ•è³‡æ©Ÿä¼šã®ç‰¹å®š]</p>
</div>
</div>

ã€å‡ºåŠ›æŒ‡å®šã€‘
- å¿…ãšä¸Šè¨˜HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã«å¾“ã£ã¦å‡ºåŠ›ã™ã‚‹
- èª¬æ˜æ–‡ã‚„æŒ‡ç¤ºæ–‡ã¯ä¸€åˆ‡å«ã‚ãªã„ï¼ˆ[å†…å®¹]éƒ¨åˆ†ã®ã¿å®Ÿéš›ã®åˆ†æå†…å®¹ã«ç½®æ›ï¼‰
- æŠ•è³‡å®¶ãƒ»å¸‚å ´é–¢ä¿‚è€…ã«ã¨ã£ã¦å®Ÿç”¨çš„ãªæƒ…å ±ã‚’æä¾›
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚„å…·ä½“ä¾‹ã‚’ç©æ¥µçš„ã«å«ã‚ã‚‹
- ç°¡æ½”ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã§è¨˜è¿°ã™ã‚‹"""

        return prompt
    
    def _parse_unified_response(self, response_text: str) -> Optional[Dict[str, str]]:
        """HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã§ã®çµ±åˆè¦ç´„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        if not response_text:
            return None
        
        import re
        
        # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ ã‹ã‚‰å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        sections = {}
        
        # åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³
        regional_match = re.search(r'<div class="regional-summaries">(.*?)</div>(?=\s*##|\s*<div class="global-overview"|$)', response_text, re.DOTALL)
        if regional_match:
            sections['regional_summaries'] = regional_match.group(1).strip()
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬
        global_match = re.search(r'<div class="global-overview">(.*?)</div>(?=\s*##|\s*<div class="cross-regional-analysis"|$)', response_text, re.DOTALL)
        if global_match:
            sections['global_overview'] = global_match.group(1).strip()
        
        # åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æ
        cross_regional_match = re.search(r'<div class="cross-regional-analysis">(.*?)</div>(?=\s*##|\s*<div class="key-trends"|$)', response_text, re.DOTALL)
        if cross_regional_match:
            sections['cross_regional_analysis'] = cross_regional_match.group(1).strip()
        
        # æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›
        trends_match = re.search(r'<div class="key-trends">(.*?)</div>(?=\s*##|\s*<div class="risk-factors"|$)', response_text, re.DOTALL)
        if trends_match:
            sections['key_trends'] = trends_match.group(1).strip()
        
        # ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼š
        risk_match = re.search(r'<div class="risk-factors">(.*?)</div>(?=\s*##|$)', response_text, re.DOTALL)
        if risk_match:
            sections['risk_factors'] = risk_match.group(1).strip()
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã‚‚è©¦è¡Œ
        if not sections:
            return self._parse_traditional_response(response_text)
        
        return sections if sections else None
    
    def _parse_traditional_response(self, response_text: str) -> Optional[Dict[str, str]]:
        """å¾“æ¥å½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        sections = {}
        current_section = None
        current_content = []
        
        lines = response_text.split('\n')
        
        section_headers = {
            'åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³': 'regional_summaries',
            'ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬': 'global_overview', 
            'åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æ': 'cross_regional_analysis',
            'æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰': 'key_trends',
            'ãƒªã‚¹ã‚¯è¦å› ': 'risk_factors',
        }
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡º
            header_found = False
            for header, key in section_headers.items():
                if header in line and ('##' in line or 'â– ' in line or 'â—' in line):
                    # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                    if current_section and current_content:
                        sections[current_section] = '\n'.join(current_content).strip()
                    
                    current_section = key
                    current_content = []
                    header_found = True
                    break
            
            if not header_found and current_section:
                current_content.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections if sections else None
    
    def _validate_response_completeness(self, parsed_sections: Dict[str, str]) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®Œå…¨æ€§ã‚’æ¤œè¨¼
        
        Args:
            parsed_sections: è§£æã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³è¾æ›¸
            
        Returns:
            æ¤œè¨¼çµæœè¾æ›¸
        """
        issues = []
        is_complete = True
        
        # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        required_sections = [
            'regional_summaries',
            'global_overview', 
            'cross_regional_analysis'
        ]
        
        for section in required_sections:
            if section not in parsed_sections:
                issues.append(f"å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ '{section}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                is_complete = False
                continue
                
            content = parsed_sections[section].strip()
            
            # æœ€ä½æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
            min_lengths = {
                'regional_summaries': 100,
                'global_overview': 100,
                'cross_regional_analysis': 50  # åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã®æœ€ä½æ–‡å­—æ•°
            }
            
            min_length = min_lengths.get(section, 50)
            if len(content) < min_length:
                issues.append(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ '{section}' ã®æ–‡å­—æ•°ãŒä¸è¶³ ({len(content)}å­— < {min_length}å­—)")
                is_complete = False
            
            # åˆ‡ã‚Šè©°ã‚ã‚‰ã‚ŒãŸå¯èƒ½æ€§ã®ãƒã‚§ãƒƒã‚¯ï¼ˆä¸å®Œå…¨ãªæ–‡ã§çµ‚ã‚ã£ã¦ã„ã‚‹ã‹ï¼‰
            if section == 'cross_regional_analysis':
                # åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã®ç‰¹åˆ¥ãƒã‚§ãƒƒã‚¯
                if content.endswith('**') or content.endswith('- ') or content.endswith('**ç±³å›½ã®é€š'):
                    issues.append("åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æãŒé€”ä¸­ã§åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¦ã„ã¾ã™")
                    is_complete = False
                elif len(content) < 200:  # åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã¯ç‰¹ã«é‡è¦ãªã®ã§200å­—ä»¥ä¸Šå¿…è¦
                    issues.append(f"åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã®å†…å®¹ãŒçŸ­ã™ãã¾ã™ ({len(content)}å­—)")
                    is_complete = False
        
        return {
            'is_complete': is_complete,
            'issues': issues,
            'section_lengths': {section: len(content) for section, content in parsed_sections.items()}
        }
    
    def _extract_summary_text(self, response_text: str) -> Optional[str]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¦ç´„éƒ¨åˆ†ã‚’æŠ½å‡º"""
        if not response_text:
            return None
        
        # ä¸è¦ãªå‰å¾Œã®è£…é£¾ã‚’é™¤å»
        text = response_text.strip()
        
        # ```markdown ãªã©ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯é™¤å»
        text = re.sub(r"```[a-zA-Z]*\n(.*?)\n```", r"\1", text, flags=re.DOTALL)
        
        # æœ€çµ‚çš„ãªãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        return text.strip() if text.strip() else None


def create_integrated_summaries(api_key: str, grouped_articles: Dict[str, List[Dict[str, Any]]], 
                               config: ProSummaryConfig = None) -> Optional[Dict[str, Any]]:
    """
    1å›ã®APIå‘¼ã³å‡ºã—ã§çµ±åˆè¦ç´„ã‚’ä½œæˆã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆåœ°åŸŸé–“é–¢é€£æ€§åˆ†æé‡è¦–ï¼‰
    
    Args:
        api_key (str): Gemini APIã‚­ãƒ¼
        grouped_articles (Dict): åœ°åŸŸåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸè¨˜äº‹
        config (ProSummaryConfig): è¨­å®š
    
    Returns:
        Optional[Dict[str, Any]]: çµ±åˆè¦ç´„çµæœï¼ˆåœ°åŸŸåˆ¥+ã‚°ãƒ­ãƒ¼ãƒãƒ«+é–¢é€£æ€§åˆ†æï¼‰ã€å¤±æ•—æ™‚ã¯None
    """
    logger = logging.getLogger(__name__)
    
    try:
        # è¨˜äº‹æ•°ãƒã‚§ãƒƒã‚¯
        total_articles = sum(len(articles) for articles in grouped_articles.values())
        if config and total_articles < config.min_articles_threshold:
            logger.warning(f"è¨˜äº‹æ•°ãŒé–¾å€¤æœªæº€ã§ã™ ({total_articles} < {config.min_articles_threshold})")
            return None
        
        # Pro Summarizerã‚’åˆæœŸåŒ–
        summarizer = ProSummarizer(api_key, config)
        
        # 1å›APIå‘¼ã³å‡ºã—ã§çµ±åˆè¦ç´„ç”Ÿæˆ
        unified_result = summarizer.generate_unified_summary(grouped_articles)
        
        if not unified_result:
            logger.error("çµ±åˆè¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # çµæœã‚’æ•´ç†
        articles_by_region = {region: len(articles) for region, articles in grouped_articles.items()}
        
        result = {
            "unified_summary": unified_result["unified_summary"],
            "metadata": {
                "total_articles": total_articles,
                "articles_by_region": articles_by_region,
                "processing_timestamp": datetime.utcnow().isoformat(),
                "processing_time_ms": unified_result.get("processing_time_ms", 0),
                "model_version": unified_result.get("model_version", "gemini-2.5-pro")
            }
        }
        
        logger.info(f"1å›APIå‘¼ã³å‡ºã—çµ±åˆè¦ç´„ç”Ÿæˆå®Œäº†: {total_articles}ä»¶ã®è¨˜äº‹ã‚’å‡¦ç†")
        return result
        
    except Exception as e:
        logger.error(f"çµ±åˆè¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return None


if __name__ == '__main__':
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    from dotenv import load_dotenv
    load_dotenv()
    
    test_api_key = os.getenv("GEMINI_API_KEY")
    if not test_api_key:
        raise ValueError("ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
    test_grouped_articles = {
        "japan": [
            {
                "title": "æ—¥éŠ€ã€æ”¿ç­–é‡‘åˆ©æ®ãˆç½®ãæ±ºå®š",
                "summary": "æ—¥æœ¬éŠ€è¡Œã¯é‡‘èæ”¿ç­–æ±ºå®šä¼šåˆã§æ”¿ç­–é‡‘åˆ©ã‚’æ®ãˆç½®ãã€ç·©å’Œçš„ãªé‡‘èæ”¿ç­–ã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ã‚’æ±ºå®šã—ãŸã€‚",
                "category": "é‡‘èæ”¿ç­–"
            },
            {
                "title": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã€å¢—ç›Šã‚’ç™ºè¡¨",
                "summary": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã¯å››åŠæœŸæ±ºç®—ã§å‰å¹´åŒæœŸæ¯”å¢—ç›Šã‚’ç™ºè¡¨ã€æµ·å¤–è²©å£²ãŒå¥½èª¿ã ã£ãŸã€‚",
                "category": "ä¼æ¥­æ¥­ç¸¾"
            }
        ],
        "usa": [
            {
                "title": "FRBã€åˆ©ä¸Šã’ã‚’æ¤œè¨",
                "summary": "ç±³é€£é‚¦æº–å‚™åˆ¶åº¦ç†äº‹ä¼šã¯æ¬¡å›ä¼šåˆã§ã®åˆ©ä¸Šã’å®Ÿæ–½ã‚’ç¤ºå”†ã€ã‚¤ãƒ³ãƒ•ãƒ¬æŠ‘åˆ¶ã‚’å„ªå…ˆã™ã‚‹å§¿å‹¢ã€‚",
                "category": "é‡‘èæ”¿ç­–"
            }
        ]
    }
    
    print("--- Proçµ±åˆè¦ç´„ãƒ†ã‚¹ãƒˆ ---")
    config = ProSummaryConfig(min_articles_threshold=2)
    result = create_integrated_summaries(test_api_key, test_grouped_articles, config)
    
    if result:
        print(f"\n=== çµ±åˆè¦ç´„çµæœ ===")
        unified_summary = result['unified_summary']
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
        for section_name, content in unified_summary.items():
            section_names = {
                'regional_summaries': 'åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³',
                'global_overview': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬',
                'cross_regional_analysis': 'åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æ',
                'key_trends': 'æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›',
                'risk_factors': 'ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼š'
            }
            display_name = section_names.get(section_name, section_name)
            print(f"\n--- {display_name} ---")
            print(f"æ–‡å­—æ•°: {len(content)}å­—")
            print(f"å†…å®¹: {content[:100]}...")
        
        print(f"\n=== ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ ===")
        print(f"ç·è¨˜äº‹æ•°: {result['metadata']['total_articles']}")
        print(f"åœ°åŸŸåˆ¥: {result['metadata']['articles_by_region']}")
        print(f"å‡¦ç†æ™‚é–“: {result['metadata']['processing_time_ms']}ms")
        print(f"ãƒ¢ãƒ‡ãƒ«: {result['metadata']['model_version']}")
    else:
        print("ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")