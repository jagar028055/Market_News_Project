# -*- coding: utf-8 -*-

"""
A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¯”è¼ƒè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ™‚å®Ÿè¡Œã—ã¦æ¯”è¼ƒè©•ä¾¡
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import time

from .prompt_manager import PromptManager


class ABTestManager:
    """A/Bãƒ†ã‚¹ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, prompt_manager: Optional[PromptManager] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            prompt_manager: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.logger = logging.getLogger(__name__)
        self.prompt_manager = prompt_manager or PromptManager()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.results_dir = self.prompt_manager.prompts_dir / "ab_test_results"
        self.results_dir.mkdir(exist_ok=True)
        
        self.logger.info("A/Bãƒ†ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def setup_comparison_test(self, comparison_mode: str) -> List[str]:
        """
        æ¯”è¼ƒãƒ†ã‚¹ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        
        Args:
            comparison_mode: æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆsingle/ab_test/multi_compareï¼‰
            
        Returns:
            List[str]: ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
        """
        if comparison_mode == "single":
            # å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
            pattern = self.prompt_manager.get_environment_prompt_pattern()
            return [pattern]
            
        elif comparison_mode == "ab_test":
            # A/Bãƒ†ã‚¹ãƒˆï¼ˆ2ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒï¼‰
            return self.prompt_manager.setup_ab_test()
            
        elif comparison_mode == "multi_compare":
            # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
            return self.prompt_manager.get_available_patterns()
            
        else:
            self.logger.warning(f"æœªçŸ¥ã®æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰: {comparison_mode}")
            return [self.prompt_manager.get_environment_prompt_pattern()]
    
    async def run_comparison_test(
        self, 
        script_generator,
        articles, 
        target_duration: float = 10.0,
        comparison_mode: str = "single"
    ) -> Dict[str, Any]:
        """
        æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        
        Args:
            script_generator: å°æœ¬ç”Ÿæˆå™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            articles: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            target_duration: ç›®æ¨™æ™‚é–“
            comparison_mode: æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            Dict[str, Any]: æ¯”è¼ƒãƒ†ã‚¹ãƒˆçµæœ
        """
        try:
            test_id = f"comparison_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.logger.info(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹: {test_id} (ãƒ¢ãƒ¼ãƒ‰: {comparison_mode})")
            
            # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºå®š
            test_patterns = self.setup_comparison_test(comparison_mode)
            
            if not test_patterns:
                raise ValueError("ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            self.logger.info(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³: {test_patterns}")
            
            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å°æœ¬ç”Ÿæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
            results = {}
            start_time = time.time()
            
            # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
            with ThreadPoolExecutor(max_workers=min(len(test_patterns), 3)) as executor:
                futures = {}
                
                for pattern in test_patterns:
                    future = executor.submit(
                        self._generate_script_for_pattern,
                        script_generator, articles, target_duration, pattern
                    )
                    futures[pattern] = future
                
                # çµæœåé›†
                for pattern, future in futures.items():
                    try:
                        pattern_result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        results[pattern] = pattern_result
                        self.logger.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} å®Œäº†")
                    except Exception as e:
                        self.logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} ã‚¨ãƒ©ãƒ¼: {e}")
                        results[pattern] = {"error": str(e)}
            
            total_time = time.time() - start_time
            
            # æ¯”è¼ƒåˆ†æ
            comparison_analysis = self._analyze_comparison_results(results)
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            final_result = {
                "test_id": test_id,
                "comparison_mode": comparison_mode,
                "test_patterns": test_patterns,
                "total_execution_time": total_time,
                "pattern_results": results,
                "comparison_analysis": comparison_analysis,
                "best_pattern": comparison_analysis.get("best_pattern"),
                "test_timestamp": datetime.now().isoformat()
            }
            
            # çµæœä¿å­˜
            self._save_comparison_results(test_id, final_result)
            
            self.logger.info(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†: {test_id} (å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’)")
            return final_result
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _generate_script_for_pattern(
        self, script_generator, articles, target_duration: float, pattern: str
    ) -> Dict[str, Any]:
        """
        ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®å°æœ¬ç”Ÿæˆ
        
        Args:
            script_generator: å°æœ¬ç”Ÿæˆå™¨
            articles: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            target_duration: ç›®æ¨™æ™‚é–“
            pattern: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
            
        Returns:
            Dict[str, Any]: ç”Ÿæˆçµæœ
        """
        try:
            start_time = time.time()
            result = script_generator.generate_professional_script(
                articles, target_duration, prompt_pattern=pattern
            )
            generation_time = time.time() - start_time
            
            # ç”Ÿæˆæ™‚é–“ã‚’è¿½åŠ 
            result["generation_time"] = generation_time
            result["pattern_id"] = pattern
            
            return result
            
        except Exception as e:
            self.logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} ã§ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "pattern_id": pattern,
                "generation_time": 0
            }
    
    def _analyze_comparison_results(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """
        æ¯”è¼ƒçµæœã®åˆ†æï¼ˆæ‹¡å¼µç‰ˆï¼‰
        
        Args:
            results: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµæœ
            
        Returns:
            Dict[str, Any]: è©³ç´°åˆ†æçµæœ
        """
        analysis = {
            "total_patterns": len(results),
            "successful_patterns": 0,
            "failed_patterns": 0,
            "pattern_scores": {},
            "metrics_comparison": {},
            "quality_analysis": {},
            "performance_analysis": {},
            "cost_analysis": {},
            "best_pattern": None,
            "recommendations": [],
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        successful_results = {}
        failed_patterns = []
        
        # æˆåŠŸãƒ»å¤±æ•—ã®åˆ†é¡
        for pattern, result in results.items():
            if "error" in result:
                analysis["failed_patterns"] += 1
                failed_patterns.append({
                    "pattern": pattern,
                    "error": result["error"]
                })
            else:
                analysis["successful_patterns"] += 1
                successful_results[pattern] = result
        
        if not successful_results:
            analysis["recommendations"].append("ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            analysis["failed_pattern_details"] = failed_patterns
            return analysis
        
        # æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
        metrics = ["char_count", "quality_score", "generation_time", "estimated_duration"]
        
        for metric in metrics:
            metric_values = {}
            for pattern, result in successful_results.items():
                if metric in result:
                    metric_values[pattern] = result[metric]
            
            if metric_values:
                values_list = list(metric_values.values())
                analysis["metrics_comparison"][metric] = {
                    "values": metric_values,
                    "best": max(metric_values.items(), key=lambda x: x[1] if metric != "generation_time" else -x[1]),
                    "worst": min(metric_values.items(), key=lambda x: x[1] if metric != "generation_time" else -x[1]),
                    "average": sum(values_list) / len(values_list),
                    "median": sorted(values_list)[len(values_list) // 2],
                    "std_deviation": self._calculate_std_deviation(values_list),
                    "range": max(values_list) - min(values_list)
                }
        
        # å“è³ªåˆ†æ
        analysis["quality_analysis"] = self._analyze_quality_metrics(successful_results)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        analysis["performance_analysis"] = self._analyze_performance_metrics(successful_results)
        
        # ã‚³ã‚¹ãƒˆåˆ†æï¼ˆæ¨å®šï¼‰
        analysis["cost_analysis"] = self._analyze_cost_metrics(successful_results)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        analysis["pattern_scores"] = self._calculate_comprehensive_scores(successful_results)
        
        # æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³æ±ºå®š
        if analysis["pattern_scores"]:
            best_pattern = max(analysis["pattern_scores"].items(), key=lambda x: x[1])
            analysis["best_pattern"] = {
                "pattern": best_pattern[0],
                "score": best_pattern[1],
                "details": successful_results[best_pattern[0]],
                "reasons": self._get_best_pattern_reasons(best_pattern[0], successful_results, analysis)
            }
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
        analysis["recommendations"] = self._generate_enhanced_recommendations(analysis, successful_results, failed_patterns)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict, successful_results: Dict) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if analysis["best_pattern"]:
            best = analysis["best_pattern"]
            recommendations.append(
                f"æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³: {best['pattern']} (ã‚¹ã‚³ã‚¢: {best['score']:.3f})"
            )
            
            pattern_info = self.prompt_manager.get_pattern_info(best['pattern'])
            if pattern_info:
                recommendations.append(
                    f"æ¨å¥¨è¨­å®š: {pattern_info['name']} - {pattern_info['description']}"
                )
        
        # å“è³ªã«åŸºã¥ãæ¨å¥¨
        quality_scores = {p: r.get("quality_score", 0) for p, r in successful_results.items()}
        if quality_scores:
            highest_quality = max(quality_scores.items(), key=lambda x: x[1])
            if highest_quality[1] > 0.8:
                recommendations.append(f"é«˜å“è³ª: {highest_quality[0]} (å“è³ªã‚¹ã‚³ã‚¢: {highest_quality[1]:.3f})")
        
        # é€Ÿåº¦ã«åŸºã¥ãæ¨å¥¨
        generation_times = {p: r.get("generation_time", float('inf')) for p, r in successful_results.items()}
        if generation_times:
            fastest = min(generation_times.items(), key=lambda x: x[1])
            if fastest[1] < 60:  # 1åˆ†ä»¥å†…
                recommendations.append(f"é«˜é€Ÿç”Ÿæˆ: {fastest[0]} (ç”Ÿæˆæ™‚é–“: {fastest[1]:.1f}ç§’)")
        
        return recommendations
    
    def _save_comparison_results(self, test_id: str, results: Dict[str, Any]) -> None:
        """æ¯”è¼ƒçµæœä¿å­˜"""
        try:
            result_file = self.results_dir / f"{test_id}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"æ¯”è¼ƒçµæœä¿å­˜å®Œäº†: {result_file}")
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_test_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆå±¥æ­´å–å¾—"""
        try:
            result_files = sorted(
                self.results_dir.glob("*.json"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            history = []
            for result_file in result_files[:limit]:
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        history.append({
                            "test_id": result.get("test_id"),
                            "timestamp": result.get("test_timestamp"),
                            "comparison_mode": result.get("comparison_mode"),
                            "best_pattern": result.get("best_pattern", {}).get("pattern"),
                            "total_patterns": result.get("total_patterns", 0)
                        })
                except Exception as e:
                    self.logger.warning(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {result_file} - {e}")
            
            return history
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚¹ãƒˆå±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def create_comparison_report(self, test_id: str) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            result_file = self.results_dir / f"{test_id}.json"
            if not result_file.exists():
                return f"ãƒ†ã‚¹ãƒˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_id}"
            
            with open(result_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            report_lines = [
                f"# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ",
                f"",
                f"**ãƒ†ã‚¹ãƒˆID**: {results['test_id']}",
                f"**å®Ÿè¡Œæ—¥æ™‚**: {results['test_timestamp']}",
                f"**æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰**: {results['comparison_mode']}",
                f"**å®Ÿè¡Œæ™‚é–“**: {results['total_execution_time']:.1f}ç§’",
                f"",
                f"## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼",
                f"",
            ]
            
            analysis = results.get("comparison_analysis", {})
            
            if analysis.get("best_pattern"):
                best = analysis["best_pattern"]
                report_lines.extend([
                    f"**ğŸ† æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³**: {best['pattern']}",
                    f"**ç·åˆã‚¹ã‚³ã‚¢**: {best['score']:.3f}",
                    f"",
                ])
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµæœ
            report_lines.extend([
                f"## ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è©³ç´°çµæœ",
                f"",
                f"| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ–‡å­—æ•° | å“è³ªã‚¹ã‚³ã‚¢ | ç”Ÿæˆæ™‚é–“ | æ¨å®šæ™‚é–“ | çŠ¶æ…‹ |",
                f"|---------|-------|----------|---------|---------|------|",
            ])
            
            for pattern, result in results.get("pattern_results", {}).items():
                if "error" in result:
                    status = f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error'][:30]}..."
                    report_lines.append(f"| {pattern} | - | - | - | - | {status} |")
                else:
                    char_count = result.get("char_count", "-")
                    quality_score = f"{result.get('quality_score', 0):.3f}"
                    gen_time = f"{result.get('generation_time', 0):.1f}s"
                    est_duration = f"{result.get('estimated_duration', 0):.1f}min"
                    status = "âœ… æˆåŠŸ"
                    report_lines.append(f"| {pattern} | {char_count} | {quality_score} | {gen_time} | {est_duration} | {status} |")
            
            # æ¨å¥¨äº‹é …
            if analysis.get("recommendations"):
                report_lines.extend([
                    f"",
                    f"## æ¨å¥¨äº‹é …",
                    f"",
                ])
                for rec in analysis["recommendations"]:
                    report_lines.append(f"- {rec}")
            
            return "\n".join(report_lines)
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    async def create_google_document_report(
        self, 
        comparison_results: Dict[str, Any], 
        title: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ¯”è¼ƒçµæœã‚’Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§å‡ºåŠ›
        
        Args:
            comparison_results: æ¯”è¼ƒçµæœãƒ‡ãƒ¼ã‚¿
            title: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict[str, Any]: Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆçµæœ
        """
        try:
            # Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¯”è¼ƒç”Ÿæˆå™¨ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from ..gdocs.comparison_doc_generator import ComparisonDocGenerator
            
            doc_generator = ComparisonDocGenerator()
            
            # Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
            doc_result = await doc_generator.create_comparison_document(
                comparison_results=comparison_results,
                title=title
            )
            
            self.logger.info(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {doc_result.get('success', False)}")
            return doc_result
            
        except ImportError as e:
            self.logger.warning(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå™¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": "ComparisonDocGenerator not available", "success": False}
        except Exception as e:
            self.logger.error(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e), "success": False}
    
    def _calculate_std_deviation(self, values: List[float]) -> float:
        """æ¨™æº–åå·®è¨ˆç®—"""
        if len(values) < 2:
            return 0.0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5
    
    def _analyze_quality_metrics(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°åˆ†æ"""
        quality_analysis = {
            "high_quality_patterns": [],
            "medium_quality_patterns": [],
            "low_quality_patterns": [],
            "quality_distribution": {},
            "consistency_analysis": {}
        }
        
        # å“è³ªã‚¹ã‚³ã‚¢åˆ¥åˆ†é¡
        for pattern, result in results.items():
            quality_score = result.get("quality_score", 0)
            char_count = result.get("char_count", 0)
            
            if quality_score >= 0.8:
                quality_analysis["high_quality_patterns"].append({
                    "pattern": pattern,
                    "quality_score": quality_score,
                    "char_count": char_count
                })
            elif quality_score >= 0.6:
                quality_analysis["medium_quality_patterns"].append({
                    "pattern": pattern, 
                    "quality_score": quality_score,
                    "char_count": char_count
                })
            else:
                quality_analysis["low_quality_patterns"].append({
                    "pattern": pattern,
                    "quality_score": quality_score, 
                    "char_count": char_count
                })
        
        # å“è³ªåˆ†å¸ƒ
        quality_scores = [r.get("quality_score", 0) for r in results.values()]
        if quality_scores:
            quality_analysis["quality_distribution"] = {
                "average": sum(quality_scores) / len(quality_scores),
                "median": sorted(quality_scores)[len(quality_scores) // 2],
                "min": min(quality_scores),
                "max": max(quality_scores),
                "std_dev": self._calculate_std_deviation(quality_scores)
            }
        
        return quality_analysis
    
    def _analyze_performance_metrics(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°åˆ†æ"""
        performance_analysis = {
            "fast_patterns": [],
            "medium_speed_patterns": [],
            "slow_patterns": [],
            "generation_time_analysis": {},
            "efficiency_ranking": []
        }
        
        # ç”Ÿæˆæ™‚é–“åˆ¥åˆ†é¡
        for pattern, result in results.items():
            gen_time = result.get("generation_time", 0)
            quality_score = result.get("quality_score", 0)
            
            efficiency = quality_score / max(gen_time, 1)  # å“è³ª/æ™‚é–“ã®åŠ¹ç‡æ€§
            
            if gen_time <= 60:  # 1åˆ†ä»¥å†…
                performance_analysis["fast_patterns"].append({
                    "pattern": pattern,
                    "generation_time": gen_time,
                    "efficiency": efficiency
                })
            elif gen_time <= 120:  # 2åˆ†ä»¥å†…
                performance_analysis["medium_speed_patterns"].append({
                    "pattern": pattern,
                    "generation_time": gen_time,
                    "efficiency": efficiency
                })
            else:  # 2åˆ†ä»¥ä¸Š
                performance_analysis["slow_patterns"].append({
                    "pattern": pattern,
                    "generation_time": gen_time,
                    "efficiency": efficiency
                })
            
            performance_analysis["efficiency_ranking"].append({
                "pattern": pattern,
                "efficiency": efficiency,
                "generation_time": gen_time,
                "quality_score": quality_score
            })
        
        # åŠ¹ç‡æ€§ã§ã‚½ãƒ¼ãƒˆ
        performance_analysis["efficiency_ranking"].sort(key=lambda x: x["efficiency"], reverse=True)
        
        # ç”Ÿæˆæ™‚é–“åˆ†æ
        generation_times = [r.get("generation_time", 0) for r in results.values()]
        if generation_times:
            performance_analysis["generation_time_analysis"] = {
                "average": sum(generation_times) / len(generation_times),
                "median": sorted(generation_times)[len(generation_times) // 2],
                "min": min(generation_times),
                "max": max(generation_times),
                "total": sum(generation_times),
                "std_dev": self._calculate_std_deviation(generation_times)
            }
        
        return performance_analysis
    
    def _analyze_cost_metrics(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """ã‚³ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†æï¼ˆæ¨å®šï¼‰"""
        cost_analysis = {
            "estimated_api_costs": {},
            "cost_efficiency": {},
            "total_estimated_cost": 0.0,
            "cost_per_quality_point": {}
        }
        
        # Gemini APIæ¨å®šã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
        # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³å˜ä¾¡: $0.000125/1K tokens
        # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å˜ä¾¡: $0.000375/1K tokens
        input_cost_per_1k = 0.000125
        output_cost_per_1k = 0.000375
        
        for pattern, result in results.items():
            char_count = result.get("char_count", 0)
            generation_time = result.get("generation_time", 0)
            quality_score = result.get("quality_score", 0)
            
            # æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ—¥æœ¬èªæ–‡å­—æ•°ã®ç´„1.5å€ï¼‰
            estimated_output_tokens = char_count * 1.5
            estimated_input_tokens = 2000  # è¨˜äº‹+ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
            # æ¨å®šã‚³ã‚¹ãƒˆ
            estimated_cost = (
                (estimated_input_tokens / 1000) * input_cost_per_1k + 
                (estimated_output_tokens / 1000) * output_cost_per_1k
            )
            
            cost_analysis["estimated_api_costs"][pattern] = estimated_cost
            cost_analysis["total_estimated_cost"] += estimated_cost
            
            # ã‚³ã‚¹ãƒˆåŠ¹ç‡æ€§ï¼ˆå“è³ª/ã‚³ã‚¹ãƒˆï¼‰
            if estimated_cost > 0:
                cost_efficiency = quality_score / estimated_cost
                cost_analysis["cost_efficiency"][pattern] = cost_efficiency
                cost_analysis["cost_per_quality_point"][pattern] = estimated_cost / max(quality_score, 0.1)
        
        return cost_analysis
    
    def _calculate_comprehensive_scores(self, results: Dict[str, Dict]) -> Dict[str, float]:
        """åŒ…æ‹¬çš„ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        pattern_scores = {}
        
        # é‡ã¿è¨­å®šï¼ˆåˆè¨ˆ1.0ï¼‰
        weights = {
            "quality_score": 0.35,      # å“è³ªã‚’é‡è¦–
            "char_count": 0.15,         # æ–‡å­—æ•°é©åˆæ€§
            "generation_time": 0.25,    # ç”ŸæˆåŠ¹ç‡
            "estimated_duration": 0.15, # é…ä¿¡æ™‚é–“é©åˆæ€§
            "consistency": 0.10         # ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        }
        
        target_chars = int(os.getenv('PODCAST_TARGET_SCRIPT_CHARS', '2700'))
        target_duration = float(os.getenv('PODCAST_TARGET_DURATION_MINUTES', '10.0'))
        
        for pattern, result in results.items():
            score = 0.0
            
            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
            if "quality_score" in result:
                quality = result["quality_score"]
                score += quality * weights["quality_score"]
            
            # æ–‡å­—æ•°é©åˆæ€§ï¼ˆç›®æ¨™ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰
            if "char_count" in result:
                char_count = result["char_count"]
                char_deviation = abs(char_count - target_chars) / target_chars
                char_score = max(0, 1.0 - char_deviation)
                score += char_score * weights["char_count"]
            
            # ç”ŸæˆåŠ¹ç‡ï¼ˆçŸ­ã„ã»ã©è‰¯ã„ã€ãŸã ã—æ¥µç«¯ã«çŸ­ã„å ´åˆã¯æ¸›ç‚¹ï¼‰
            if "generation_time" in result:
                gen_time = result["generation_time"]
                if gen_time < 10:  # 10ç§’æœªæº€ã¯ä¸è‡ªç„¶
                    time_score = 0.5
                elif gen_time > 300:  # 5åˆ†ä»¥ä¸Šã¯é…ã™ãã‚‹
                    time_score = 0.0
                else:
                    time_score = max(0, 1.0 - (gen_time - 10) / 290)
                score += time_score * weights["generation_time"]
            
            # é…ä¿¡æ™‚é–“é©åˆæ€§ï¼ˆç›®æ¨™ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰
            if "estimated_duration" in result:
                duration = result["estimated_duration"]
                duration_deviation = abs(duration - target_duration) / target_duration
                duration_score = max(0, 1.0 - duration_deviation)
                score += duration_score * weights["estimated_duration"]
            
            # ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ã§å®Œäº†ã—ãŸå ´åˆï¼‰
            consistency_bonus = 1.0 if "error" not in result else 0.0
            score += consistency_bonus * weights["consistency"]
            
            pattern_scores[pattern] = score
        
        return pattern_scores
    
    def _get_best_pattern_reasons(self, best_pattern: str, results: Dict, analysis: Dict) -> List[str]:
        """æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¸å®šç†ç”±"""
        reasons = []
        result = results[best_pattern]
        
        # å“è³ªé¢ã§ã®å„ªä½æ€§
        quality_score = result.get("quality_score", 0)
        if quality_score >= 0.8:
            reasons.append(f"é«˜ã„å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.3f}/1.000")
        
        # åŠ¹ç‡æ€§ã§ã®å„ªä½æ€§
        gen_time = result.get("generation_time", 0)
        if gen_time <= 60:
            reasons.append(f"é«˜é€Ÿãªç”Ÿæˆæ™‚é–“: {gen_time:.1f}ç§’")
        
        # æ–‡å­—æ•°é©åˆæ€§
        char_count = result.get("char_count", 0)
        target_chars = int(os.getenv('PODCAST_TARGET_SCRIPT_CHARS', '2700'))
        char_deviation = abs(char_count - target_chars) / target_chars
        if char_deviation < 0.1:
            reasons.append(f"é©åˆ‡ãªæ–‡å­—æ•°: {char_count:,}æ–‡å­—ï¼ˆç›®æ¨™: {target_chars:,}æ–‡å­—ï¼‰")
        
        # é…ä¿¡æ™‚é–“é©åˆæ€§
        est_duration = result.get("estimated_duration", 0)
        target_duration = float(os.getenv('PODCAST_TARGET_DURATION_MINUTES', '10.0'))
        duration_deviation = abs(est_duration - target_duration) / target_duration
        if duration_deviation < 0.1:
            reasons.append(f"é©åˆ‡ãªé…ä¿¡æ™‚é–“: {est_duration:.1f}åˆ†ï¼ˆç›®æ¨™: {target_duration}åˆ†ï¼‰")
        
        return reasons
    
    def _generate_enhanced_recommendations(
        self, 
        analysis: Dict, 
        successful_results: Dict, 
        failed_patterns: List[Dict]
    ) -> List[str]:
        """æ‹¡å¼µç‰ˆæ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨å¥¨
        if analysis.get("best_pattern"):
            best = analysis["best_pattern"]
            recommendations.append(
                f"ğŸ† æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³: {best['pattern']} (ã‚¹ã‚³ã‚¢: {best['score']:.3f})"
            )
            
            # é¸å®šç†ç”±ã‚’è¿½åŠ 
            for reason in best.get("reasons", []):
                recommendations.append(f"  â†’ {reason}")
        
        # å“è³ªåˆ†æã«åŸºã¥ãæ¨å¥¨
        quality_analysis = analysis.get("quality_analysis", {})
        high_quality = quality_analysis.get("high_quality_patterns", [])
        if len(high_quality) > 1:
            recommendations.append(f"é«˜å“è³ªãƒ‘ã‚¿ãƒ¼ãƒ³: {len(high_quality)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ0.8ä»¥ä¸Šã®å“è³ªã‚¹ã‚³ã‚¢")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã«åŸºã¥ãæ¨å¥¨
        performance_analysis = analysis.get("performance_analysis", {})
        fast_patterns = performance_analysis.get("fast_patterns", [])
        if fast_patterns:
            fastest = min(fast_patterns, key=lambda x: x["generation_time"])
            recommendations.append(f"âš¡ æœ€é«˜é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³: {fastest['pattern']} ({fastest['generation_time']:.1f}ç§’)")
        
        # åŠ¹ç‡æ€§ã«åŸºã¥ãæ¨å¥¨
        efficiency_ranking = performance_analysis.get("efficiency_ranking", [])
        if efficiency_ranking:
            most_efficient = efficiency_ranking[0]
            recommendations.append(
                f"ğŸ¯ æœ€é«˜åŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³: {most_efficient['pattern']} "
                f"(åŠ¹ç‡å€¤: {most_efficient['efficiency']:.3f})"
            )
        
        # ã‚³ã‚¹ãƒˆåˆ†æã«åŸºã¥ãæ¨å¥¨
        cost_analysis = analysis.get("cost_analysis", {})
        cost_efficiency = cost_analysis.get("cost_efficiency", {})
        if cost_efficiency:
            most_cost_efficient = max(cost_efficiency.items(), key=lambda x: x[1])
            recommendations.append(
                f"ğŸ’° æœ€é«˜ã‚³ã‚¹ãƒˆåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³: {most_cost_efficient[0]} "
                f"(åŠ¹ç‡å€¤: {most_cost_efficient[1]:.3f})"
            )
        
        # å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã™ã‚‹å¯¾ç­–
        if failed_patterns:
            recommendations.append(f"âš ï¸ å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(failed_patterns)}å€‹")
            for failed in failed_patterns:
                recommendations.append(f"  â†’ {failed['pattern']}: {failed['error'][:50]}...")
        
        # ç·åˆçš„ãªæ¨å¥¨äº‹é …
        successful_count = len(successful_results)
        total_count = analysis["total_patterns"]
        success_rate = (successful_count / total_count) * 100 if total_count > 0 else 0
        
        if success_rate >= 80:
            recommendations.append(f"âœ… ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§è‰¯å¥½ (æˆåŠŸç‡: {success_rate:.1f}%)")
        elif success_rate >= 60:
            recommendations.append(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§è¦æ³¨æ„ (æˆåŠŸç‡: {success_rate:.1f}%)")
        else:
            recommendations.append(f"ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„ãŒå¿…è¦ (æˆåŠŸç‡: {success_rate:.1f}%)")
        
        return recommendations