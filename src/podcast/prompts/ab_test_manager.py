# -*- coding: utf-8 -*-

"""
A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¯”è¼ƒè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ™‚å®Ÿè¡Œã—ã¦æ¯”è¼ƒè©•ä¾¡
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import time

from .prompt_manager import PromptManager


class ABTestManager:
    """A/Bãƒ†ã‚¹ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, prompt_manager: Optional[PromptManager] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            prompt_manager: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.logger = logging.getLogger(__name__)
        self.prompt_manager = prompt_manager or PromptManager()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.results_dir = self.prompt_manager.prompts_dir / "ab_test_results"
        self.results_dir.mkdir(exist_ok=True)
        
        self.logger.info("A/Bãƒ†ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def setup_comparison_test(self, comparison_mode: str) -> List[str]:
        """
        æ¯”è¼ƒãƒ†ã‚¹ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        
        Args:
            comparison_mode: æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆsingle/ab_test/multi_compareï¼‰
            
        Returns:
            List[str]: ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
        """
        if comparison_mode == "single":
            # å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
            pattern = self.prompt_manager.get_environment_prompt_pattern()
            return [pattern]
            
        elif comparison_mode == "ab_test":
            # A/Bãƒ†ã‚¹ãƒˆï¼ˆ2ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒï¼‰
            return self.prompt_manager.setup_ab_test()
            
        elif comparison_mode == "multi_compare":
            # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
            return self.prompt_manager.get_available_patterns()
            
        else:
            self.logger.warning(f"æœªçŸ¥ã®æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰: {comparison_mode}")
            return [self.prompt_manager.get_environment_prompt_pattern()]
    
    async def run_comparison_test(
        self, 
        script_generator,
        articles, 
        target_duration: float = 10.0,
        comparison_mode: str = "single"
    ) -> Dict[str, Any]:
        """
        æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        
        Args:
            script_generator: å°æœ¬ç”Ÿæˆå™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            articles: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            target_duration: ç›®æ¨™æ™‚é–“
            comparison_mode: æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            Dict[str, Any]: æ¯”è¼ƒãƒ†ã‚¹ãƒˆçµæœ
        """
        try:
            test_id = f"comparison_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.logger.info(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹: {test_id} (ãƒ¢ãƒ¼ãƒ‰: {comparison_mode})")
            
            # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºå®š
            test_patterns = self.setup_comparison_test(comparison_mode)
            
            if not test_patterns:
                raise ValueError("ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            self.logger.info(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³: {test_patterns}")
            
            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å°æœ¬ç”Ÿæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰
            results = {}
            start_time = time.time()
            
            # ThreadPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
            with ThreadPoolExecutor(max_workers=min(len(test_patterns), 3)) as executor:
                futures = {}
                
                for pattern in test_patterns:
                    future = executor.submit(
                        self._generate_script_for_pattern,
                        script_generator, articles, target_duration, pattern
                    )
                    futures[pattern] = future
                
                # çµæœåé›†
                for pattern, future in futures.items():
                    try:
                        pattern_result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        results[pattern] = pattern_result
                        self.logger.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} å®Œäº†")
                    except Exception as e:
                        self.logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} ã‚¨ãƒ©ãƒ¼: {e}")
                        results[pattern] = {"error": str(e)}
            
            total_time = time.time() - start_time
            
            # æ¯”è¼ƒåˆ†æ
            comparison_analysis = self._analyze_comparison_results(results)
            
            # çµæœã‚’ã¾ã¨ã‚ã‚‹
            final_result = {
                "test_id": test_id,
                "comparison_mode": comparison_mode,
                "test_patterns": test_patterns,
                "total_execution_time": total_time,
                "pattern_results": results,
                "comparison_analysis": comparison_analysis,
                "best_pattern": comparison_analysis.get("best_pattern"),
                "test_timestamp": datetime.now().isoformat()
            }
            
            # çµæœä¿å­˜
            self._save_comparison_results(test_id, final_result)
            
            self.logger.info(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Œäº†: {test_id} (å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’)")
            return final_result
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _generate_script_for_pattern(
        self, script_generator, articles, target_duration: float, pattern: str
    ) -> Dict[str, Any]:
        """
        ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®å°æœ¬ç”Ÿæˆ
        
        Args:
            script_generator: å°æœ¬ç”Ÿæˆå™¨
            articles: è¨˜äº‹ãƒ‡ãƒ¼ã‚¿
            target_duration: ç›®æ¨™æ™‚é–“
            pattern: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
            
        Returns:
            Dict[str, Any]: ç”Ÿæˆçµæœ
        """
        try:
            start_time = time.time()
            result = script_generator.generate_professional_script(
                articles, target_duration, prompt_pattern=pattern
            )
            generation_time = time.time() - start_time
            
            # ç”Ÿæˆæ™‚é–“ã‚’è¿½åŠ 
            result["generation_time"] = generation_time
            result["pattern_id"] = pattern
            
            return result
            
        except Exception as e:
            self.logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ {pattern} ã§ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "pattern_id": pattern,
                "generation_time": 0
            }
    
    def _analyze_comparison_results(self, results: Dict[str, Dict]) -> Dict[str, Any]:
        """
        æ¯”è¼ƒçµæœã®åˆ†æ
        
        Args:
            results: ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµæœ
            
        Returns:
            Dict[str, Any]: åˆ†æçµæœ
        """
        analysis = {
            "total_patterns": len(results),
            "successful_patterns": 0,
            "failed_patterns": 0,
            "pattern_scores": {},
            "metrics_comparison": {},
            "best_pattern": None,
            "recommendations": []
        }
        
        successful_results = {}
        
        # æˆåŠŸãƒ»å¤±æ•—ã®åˆ†é¡
        for pattern, result in results.items():
            if "error" in result:
                analysis["failed_patterns"] += 1
            else:
                analysis["successful_patterns"] += 1
                successful_results[pattern] = result
        
        if not successful_results:
            analysis["recommendations"].append("ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return analysis
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
        metrics = ["char_count", "quality_score", "generation_time", "estimated_duration"]
        
        for metric in metrics:
            metric_values = {}
            for pattern, result in successful_results.items():
                if metric in result:
                    metric_values[pattern] = result[metric]
            
            if metric_values:
                analysis["metrics_comparison"][metric] = {
                    "values": metric_values,
                    "best": max(metric_values.items(), key=lambda x: x[1] if metric != "generation_time" else -x[1]),
                    "worst": min(metric_values.items(), key=lambda x: x[1] if metric != "generation_time" else -x[1]),
                    "average": sum(metric_values.values()) / len(metric_values)
                }
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        for pattern, result in successful_results.items():
            score = 0.0
            weights = {"quality_score": 0.4, "char_count": 0.2, "generation_time": 0.2, "estimated_duration": 0.2}
            
            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
            if "quality_score" in result:
                score += result["quality_score"] * weights["quality_score"]
            
            # æ–‡å­—æ•°ï¼ˆç›®æ¨™2700ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰
            if "char_count" in result:
                char_score = 1.0 - abs(result["char_count"] - 2700) / 2700
                score += max(0, char_score) * weights["char_count"]
            
            # ç”Ÿæˆæ™‚é–“ï¼ˆçŸ­ã„ã»ã©è‰¯ã„ï¼‰
            if "generation_time" in result:
                time_score = max(0, 1.0 - result["generation_time"] / 300)  # 5åˆ†ã‚’æœ€å¤§ã¨ã™ã‚‹
                score += time_score * weights["generation_time"]
            
            # æ¨å®šæ™‚é–“ï¼ˆ10åˆ†ã«è¿‘ã„ã»ã©è‰¯ã„ï¼‰
            if "estimated_duration" in result:
                duration_score = 1.0 - abs(result["estimated_duration"] - 10.0) / 10.0
                score += max(0, duration_score) * weights["estimated_duration"]
            
            analysis["pattern_scores"][pattern] = score
        
        # æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³æ±ºå®š
        if analysis["pattern_scores"]:
            best_pattern = max(analysis["pattern_scores"].items(), key=lambda x: x[1])
            analysis["best_pattern"] = {
                "pattern": best_pattern[0],
                "score": best_pattern[1],
                "details": successful_results[best_pattern[0]]
            }
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        analysis["recommendations"] = self._generate_recommendations(analysis, successful_results)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict, successful_results: Dict) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if analysis["best_pattern"]:
            best = analysis["best_pattern"]
            recommendations.append(
                f"æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³: {best['pattern']} (ã‚¹ã‚³ã‚¢: {best['score']:.3f})"
            )
            
            pattern_info = self.prompt_manager.get_pattern_info(best['pattern'])
            if pattern_info:
                recommendations.append(
                    f"æ¨å¥¨è¨­å®š: {pattern_info['name']} - {pattern_info['description']}"
                )
        
        # å“è³ªã«åŸºã¥ãæ¨å¥¨
        quality_scores = {p: r.get("quality_score", 0) for p, r in successful_results.items()}
        if quality_scores:
            highest_quality = max(quality_scores.items(), key=lambda x: x[1])
            if highest_quality[1] > 0.8:
                recommendations.append(f"é«˜å“è³ª: {highest_quality[0]} (å“è³ªã‚¹ã‚³ã‚¢: {highest_quality[1]:.3f})")
        
        # é€Ÿåº¦ã«åŸºã¥ãæ¨å¥¨
        generation_times = {p: r.get("generation_time", float('inf')) for p, r in successful_results.items()}
        if generation_times:
            fastest = min(generation_times.items(), key=lambda x: x[1])
            if fastest[1] < 60:  # 1åˆ†ä»¥å†…
                recommendations.append(f"é«˜é€Ÿç”Ÿæˆ: {fastest[0]} (ç”Ÿæˆæ™‚é–“: {fastest[1]:.1f}ç§’)")
        
        return recommendations
    
    def _save_comparison_results(self, test_id: str, results: Dict[str, Any]) -> None:
        """æ¯”è¼ƒçµæœä¿å­˜"""
        try:
            result_file = self.results_dir / f"{test_id}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"æ¯”è¼ƒçµæœä¿å­˜å®Œäº†: {result_file}")
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_test_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆå±¥æ­´å–å¾—"""
        try:
            result_files = sorted(
                self.results_dir.glob("*.json"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            history = []
            for result_file in result_files[:limit]:
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                        history.append({
                            "test_id": result.get("test_id"),
                            "timestamp": result.get("test_timestamp"),
                            "comparison_mode": result.get("comparison_mode"),
                            "best_pattern": result.get("best_pattern", {}).get("pattern"),
                            "total_patterns": result.get("total_patterns", 0)
                        })
                except Exception as e:
                    self.logger.warning(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {result_file} - {e}")
            
            return history
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚¹ãƒˆå±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def create_comparison_report(self, test_id: str) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            result_file = self.results_dir / f"{test_id}.json"
            if not result_file.exists():
                return f"ãƒ†ã‚¹ãƒˆçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_id}"
            
            with open(result_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            report_lines = [
                f"# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ",
                f"",
                f"**ãƒ†ã‚¹ãƒˆID**: {results['test_id']}",
                f"**å®Ÿè¡Œæ—¥æ™‚**: {results['test_timestamp']}",
                f"**æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰**: {results['comparison_mode']}",
                f"**å®Ÿè¡Œæ™‚é–“**: {results['total_execution_time']:.1f}ç§’",
                f"",
                f"## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼",
                f"",
            ]
            
            analysis = results.get("comparison_analysis", {})
            
            if analysis.get("best_pattern"):
                best = analysis["best_pattern"]
                report_lines.extend([
                    f"**ğŸ† æœ€å„ªç§€ãƒ‘ã‚¿ãƒ¼ãƒ³**: {best['pattern']}",
                    f"**ç·åˆã‚¹ã‚³ã‚¢**: {best['score']:.3f}",
                    f"",
                ])
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çµæœ
            report_lines.extend([
                f"## ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è©³ç´°çµæœ",
                f"",
                f"| ãƒ‘ã‚¿ãƒ¼ãƒ³ | æ–‡å­—æ•° | å“è³ªã‚¹ã‚³ã‚¢ | ç”Ÿæˆæ™‚é–“ | æ¨å®šæ™‚é–“ | çŠ¶æ…‹ |",
                f"|---------|-------|----------|---------|---------|------|",
            ])
            
            for pattern, result in results.get("pattern_results", {}).items():
                if "error" in result:
                    status = f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error'][:30]}..."
                    report_lines.append(f"| {pattern} | - | - | - | - | {status} |")
                else:
                    char_count = result.get("char_count", "-")
                    quality_score = f"{result.get('quality_score', 0):.3f}"
                    gen_time = f"{result.get('generation_time', 0):.1f}s"
                    est_duration = f"{result.get('estimated_duration', 0):.1f}min"
                    status = "âœ… æˆåŠŸ"
                    report_lines.append(f"| {pattern} | {char_count} | {quality_score} | {gen_time} | {est_duration} | {status} |")
            
            # æ¨å¥¨äº‹é …
            if analysis.get("recommendations"):
                report_lines.extend([
                    f"",
                    f"## æ¨å¥¨äº‹é …",
                    f"",
                ])
                for rec in analysis["recommendations"]:
                    report_lines.append(f"- {rec}")
            
            return "\n".join(report_lines)
            
        except Exception as e:
            self.logger.error(f"æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"