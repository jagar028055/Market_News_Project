# -*- coding: utf-8 -*-

"""
Google Document Data Fetcher
Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æä¾›
"""

import re
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

from gdocs.client import authenticate_google_services
from src.database.models import Article, AIAnalysis
from src.podcast.data_fetcher.enhanced_database_article_fetcher import ArticleScore


@dataclass
class ParsedArticle:
    """è§£ææ¸ˆã¿è¨˜äº‹ãƒ‡ãƒ¼ã‚¿"""

    title: str
    url: str
    published_time: str
    body: str
    sentiment_icon: str


class GoogleDocumentDataFetcher:
    """Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""

    def __init__(self, document_id: str):
        """
        åˆæœŸåŒ–

        Args:
            document_id: å–å¾—å¯¾è±¡ã®Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
        """
        self.document_id = document_id
        self.logger = logging.getLogger(__name__)

        # æ„Ÿæƒ…ã‚¢ã‚¤ã‚³ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°
        self.sentiment_mapping = {
            "ğŸ˜Š": ("Positive", 0.7),
            "ğŸ˜ ": ("Negative", -0.7),
            "ğŸ˜": ("Neutral", 0.0),
            "ğŸ¤”": ("N/A", 0.0),
            "âš ï¸": ("Error", 0.0),
        }

    def fetch_articles_for_podcast(self, target_count: int = 6, **kwargs) -> List[ArticleScore]:
        """
        Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”¨è¨˜äº‹ã‚’å–å¾—

        Args:
            target_count: ç›®æ¨™è¨˜äº‹æ•°
            **kwargs: äº’æ›æ€§ã®ãŸã‚ã®å¼•æ•°ï¼ˆä½¿ç”¨ã—ãªã„ï¼‰

        Returns:
            è¨˜äº‹ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆã¸å¤‰æ›ã•ã‚ŒãŸArticleScoreå½¢å¼
        """
        try:
            self.logger.info(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜äº‹å–å¾—é–‹å§‹ - ç›®æ¨™æ•°: {target_count}")

            # Google Docsã‚µãƒ¼ãƒ“ã‚¹èªè¨¼
            drive_service, docs_service, _ = authenticate_google_services()
            if not docs_service:
                self.logger.error("Google Docsèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return []

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹å–å¾—
            parsed_articles = self._fetch_and_parse_document(docs_service)
            if not parsed_articles:
                self.logger.warning("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return []

            # Article/AIAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¸å¤‰æ›
            article_scores = self._convert_to_article_scores(parsed_articles)

            # ç›®æ¨™æ•°ã«åˆ¶é™
            selected_articles = article_scores[:target_count]

            self.logger.info(f"é¸æŠè¨˜äº‹æ•°: {len(selected_articles)}")
            for i, article_score in enumerate(selected_articles, 1):
                self.logger.info(
                    f"é¸æŠè¨˜äº‹{i}: {article_score.article.title[:50]}... "
                    f"(æ¨å®šã‚¹ã‚³ã‚¢: {article_score.score:.2f})"
                )

            return selected_articles

        except Exception as e:
            self.logger.error(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []

    def _fetch_and_parse_document(self, docs_service) -> List[ParsedArticle]:
        """
        Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ãƒ»è§£æ

        Args:
            docs_service: Google Docs APIã‚µãƒ¼ãƒ“ã‚¹

        Returns:
            è§£ææ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        try:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—
            doc = docs_service.documents().get(documentId=self.document_id).execute()
            doc_content = doc.get("body", {}).get("content", [])

            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’æŠ½å‡º
            full_text = ""
            for element in doc_content:
                if "paragraph" in element:
                    paragraph = element.get("paragraph", {})
                    elements = paragraph.get("elements", [])
                    for elem in elements:
                        if "textRun" in elem:
                            content = elem.get("textRun", {}).get("content", "")
                            full_text += content

            self.logger.info(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹å–å¾—å®Œäº† - æ–‡å­—æ•°: {len(full_text)}")

            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
            parsed_articles = self._parse_article_content(full_text)

            return parsed_articles

        except Exception as e:
            self.logger.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»è§£æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []

    def _parse_article_content(self, content: str) -> List[ParsedArticle]:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ

        Args:
            content: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹

        Returns:
            è§£ææ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        articles = []

        try:
            # è¨˜äº‹åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåŒºåˆ‡ã‚Šç·šã§åˆ†å‰²ï¼‰
            article_sections = re.split(r"-{10,}", content)

            for section in article_sections:
                section = section.strip()
                if not section:
                    continue

                # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
                parsed_article = self._extract_article_info(section)
                if parsed_article:
                    articles.append(parsed_article)

            self.logger.info(f"è¨˜äº‹è§£æå®Œäº† - è§£æè¨˜äº‹æ•°: {len(articles)}")
            return articles

        except Exception as e:
            self.logger.error(f"è¨˜äº‹å†…å®¹è§£æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []

    def _extract_article_info(self, section: str) -> Optional[ParsedArticle]:
        """
        ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º

        Args:
            section: è¨˜äº‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³

        Returns:
            è§£ææ¸ˆã¿è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            lines = section.split("\n")
            lines = [line.strip() for line in lines if line.strip()]

            if len(lines) < 3:
                return None

            # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’æ¢ã™ï¼ˆæ—¥æ™‚+ã‚¢ã‚¤ã‚³ãƒ³+ã‚¿ã‚¤ãƒˆãƒ«å½¢å¼ï¼‰
            title_line = None
            url_line = None
            body_start_idx = None

            for i, line in enumerate(lines):
                # æ—¥æ™‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚¢ã‚¤ã‚³ãƒ³ã‚’å«ã‚€è¡Œã‚’æ¢ã™
                if re.match(r"\(\d{4}-\d{2}-\d{2} \d{2}:\d{2}\)", line):
                    title_line = line
                    # æ¬¡ã®è¡ŒãŒURLã®å¯èƒ½æ€§
                    if i + 1 < len(lines) and lines[i + 1].startswith("http"):
                        url_line = lines[i + 1]
                        body_start_idx = i + 2
                    break

            if not title_line or not url_line:
                return None

            # ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‹ã‚‰æƒ…å ±æŠ½å‡º
            title_match = re.match(
                r"\((\d{4}-\d{2}-\d{2} \d{2}:\d{2})\)\s*([ğŸ˜ŠğŸ˜ ğŸ˜ğŸ¤”âš ï¸]?)\s*(.+)", title_line
            )
            if not title_match:
                return None

            published_time = title_match.group(1)
            sentiment_icon = title_match.group(2) or "ğŸ˜"
            title = title_match.group(3).strip()
            url = url_line.strip()

            # è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
            body = ""
            body_found = False

            # è¤‡æ•°ã®è¨˜äº‹æœ¬æ–‡é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
            body_start_patterns = [
                "--- è¨˜äº‹å…¨æ–‡ ---",
                "è¨˜äº‹å…¨æ–‡",
                "å…¨æ–‡",
                "æœ¬æ–‡",
                "---"
            ]

            for i in range(body_start_idx or 2, len(lines)):
                line = lines[i].strip()
                
                # è¨˜äº‹æœ¬æ–‡é–‹å§‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                if not body_found:
                    for pattern in body_start_patterns:
                        if pattern in line:
                            body_found = True
                            break
                    if body_found:
                        continue
                
                # è¨˜äº‹æœ¬æ–‡é–‹å§‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€3è¡Œç›®ä»¥é™ã‚’æœ¬æ–‡ã¨ã—ã¦æ‰±ã†
                if not body_found and i >= (body_start_idx or 2) + 1:
                    body_found = True

                if body_found:
                    # ç©ºè¡Œã‚„åŒºåˆ‡ã‚Šç·šã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if line and not line.startswith("---") and not line.startswith("="):
                        body += line + "\n"
                    # æ¬¡ã®è¨˜äº‹ã®é–‹å§‹ã‚’æ¤œå‡ºã—ãŸã‚‰çµ‚äº†
                    elif line.startswith("(202") or re.match(r"^\(\d{4}-\d{2}-\d{2}", line):
                        break

            body = body.strip()

            # æœ¬æ–‡ãŒçŸ­ã™ãã‚‹å ´åˆã€ã‚¿ã‚¤ãƒˆãƒ«ã®å¾Œã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
            if len(body) < 100:
                self.logger.info(f"æœ¬æ–‡ãŒçŸ­ã„ãŸã‚ã€ä»£æ›¿æŠ½å‡ºã‚’å®Ÿè¡Œ: {title[:50]}...")
                body = ""
                for i in range(2, len(lines)):
                    line = lines[i].strip()
                    if line and not line.startswith("---") and not line.startswith("="):
                        body += line + "\n"
                    # æ¬¡ã®è¨˜äº‹ã®é–‹å§‹ã‚’æ¤œå‡ºã—ãŸã‚‰çµ‚äº†
                    elif line.startswith("(202") or re.match(r"^\(\d{4}-\d{2}-\d{2}", line):
                        break
                body = body.strip()

            if not body or len(body) < 50:
                self.logger.warning(f"è¨˜äº‹æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹çŸ­ã™ãã¾ã™: {title[:50]}...")
                return None

            return ParsedArticle(
                title=title,
                url=url,
                published_time=published_time,
                body=body,
                sentiment_icon=sentiment_icon,
            )

        except Exception as e:
            self.logger.error(f"è¨˜äº‹æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return None

    def _convert_to_article_scores(
        self, parsed_articles: List[ParsedArticle]
    ) -> List[ArticleScore]:
        """
        è§£ææ¸ˆã¿è¨˜äº‹ã‚’ArticleScoreã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›

        Args:
            parsed_articles: è§£ææ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆ

        Returns:
            ArticleScoreãƒªã‚¹ãƒˆ
        """
        article_scores = []

        for i, parsed in enumerate(parsed_articles):
            try:
                # ç–‘ä¼¼Articleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                article = Article()
                article.id = f"gdoc_{i + 1}"
                article.title = parsed.title
                article.url = parsed.url
                article.body = parsed.body
                article.source = self._detect_source(parsed.url)
                article.scraped_at = self._parse_published_time(parsed.published_time)

                # ç–‘ä¼¼AIAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                analysis = AIAnalysis()
                analysis.article_id = article.id

                # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¨­å®š
                sentiment_label, sentiment_score = self.sentiment_mapping.get(
                    parsed.sentiment_icon, ("Neutral", 0.0)
                )
                analysis.sentiment_label = sentiment_label
                analysis.sentiment_score = sentiment_score

                # ç°¡æ˜“è¦ç´„ç”Ÿæˆï¼ˆæœ¬æ–‡ã®æœ€åˆã®200æ–‡å­—ï¼‰
                analysis.summary = self._generate_simple_summary(parsed.body)

                # ã‚«ãƒ†ã‚´ãƒªã¨åœ°åŸŸã‚’æ¨å®š
                analysis.category = self._estimate_category(parsed.title, parsed.body)
                analysis.region = self._estimate_region(parsed.title, parsed.body)

                # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ„Ÿæƒ…å¼·åº¦ + æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
                score = abs(sentiment_score) + (min(len(parsed.body) / 1000.0, 1.0) * 2.0)

                article_score = ArticleScore(
                    article=article,
                    analysis=analysis,
                    score=score,
                    score_breakdown={
                        "sentiment": abs(sentiment_score),
                        "content_length": min(len(parsed.body) / 1000.0, 1.0) * 2.0,
                        "source": "google_document",
                    },
                )

                article_scores.append(article_score)

            except Exception as e:
                self.logger.error(f"è¨˜äº‹å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                continue

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        article_scores.sort(key=lambda x: x.score, reverse=True)

        return article_scores

    def _detect_source(self, url: str) -> str:
        """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚’æ¤œå‡º"""
        if "reuters.com" in url.lower():
            return "Reuters"
        elif "bloomberg.com" in url.lower():
            return "Bloomberg"
        else:
            return "Unknown"

    def _parse_published_time(self, time_str: str) -> datetime:
        """å…¬é–‹æ™‚é–“ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        try:
            return datetime.strptime(time_str, "%Y-%m-%d %H:%M")
        except:
            return datetime.now()

    def _generate_simple_summary(self, body: str) -> str:
        """ç°¡æ˜“è¦ç´„ç”Ÿæˆ"""
        # æœ€åˆã®200æ–‡å­—ã‚’è¦ç´„ã¨ã—ã¦ä½¿ç”¨
        summary = body[:200].strip()
        if len(body) > 200:
            summary += "..."
        return summary

    def _estimate_category(self, title: str, body: str) -> str:
        """ã‚«ãƒ†ã‚´ãƒªæ¨å®š"""
        content = (title + " " + body).lower()

        if any(word in content for word in ["é‡‘åˆ©", "æ”¿ç­–é‡‘åˆ©", "æ—¥éŠ€", "frb", "fomc", "é€£é‚¦æº–å‚™"]):
            return "é‡‘èæ”¿ç­–"
        elif any(word in content for word in ["gdp", "cpi", "å¤±æ¥­ç‡", "çµŒæ¸ˆæŒ‡æ¨™", "çµŒæ¸ˆæˆé•·"]):
            return "çµŒæ¸ˆæŒ‡æ¨™"
        elif any(word in content for word in ["æ±ºç®—", "æ¥­ç¸¾", "å£²ä¸Š", "åˆ©ç›Š", "æ ªä¾¡"]):
            return "ä¼æ¥­æ¥­ç¸¾"
        elif any(word in content for word in ["å¸‚å ´", "æ ªå¼", "å‚µåˆ¸", "ç‚ºæ›¿"]):
            return "ãƒãƒ¼ã‚±ãƒƒãƒˆ"
        elif any(word in content for word in ["æŠ€è¡“", "ai", "it", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ãƒ‡ã‚¸ã‚¿ãƒ«"]):
            return "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼"
        else:
            return "ãƒ“ã‚¸ãƒã‚¹"

    def _estimate_region(self, title: str, body: str) -> str:
        """åœ°åŸŸæ¨å®š"""
        content = (title + " " + body).lower()

        if any(word in content for word in ["æ—¥éŠ€", "æ—¥æœ¬", "æ±äº¬", "å††"]):
            return "japan"
        elif any(word in content for word in ["fed", "frb", "fomc", "ã‚¢ãƒ¡ãƒªã‚«", "ç±³å›½", "ãƒ‰ãƒ«"]):
            return "usa"
        elif any(word in content for word in ["ä¸­å›½", "äººæ°‘éŠ€è¡Œ", "å…ƒ", "åŒ—äº¬", "ä¸Šæµ·"]):
            return "china"
        elif any(word in content for word in ["æ¬§å·", "ecb", "ãƒ¦ãƒ¼ãƒ­", "ãƒ‰ã‚¤ãƒ„", "ãƒ•ãƒ©ãƒ³ã‚¹"]):
            return "europe"
        else:
            return "other"

    def get_article_statistics(self, **kwargs) -> Dict[str, Any]:
        """
        çµ±è¨ˆæƒ…å ±å–å¾—ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰

        Returns:
            åŸºæœ¬çµ±è¨ˆæƒ…å ±
        """
        return {
            "data_source": "google_document",
            "document_id": self.document_id,
            "last_fetch_time": datetime.now().isoformat(),
        }
