# -*- coding: utf-8 -*-

"""
Professional Dialogue Script Generator
ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆå¯¾è©±å°æœ¬ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
Gemini 2.5 Proä½¿ç”¨ã«ã‚ˆã‚‹é«˜å“è³ª10åˆ†å®Œå…¨ç‰ˆå°æœ¬ç”Ÿæˆ
"""

import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import google.generativeai as genai
from dataclasses import dataclass

from src.podcast.data_fetcher.enhanced_database_article_fetcher import ArticleScore


@dataclass
class ScriptQuality:
    """å°æœ¬å“è³ªè©•ä¾¡çµæœ"""
    char_count: int
    estimated_duration_minutes: float
    structure_score: float
    readability_score: float
    professional_score: float
    overall_score: float
    issues: List[str]


class ProfessionalDialogueScriptGenerator:
    """ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆå¯¾è©±å°æœ¬ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.5-pro-001"):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Gemini APIã‚­ãƒ¼
            model_name: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å
        """
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        
        # Geminiè¨­å®š
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        
        # å“è³ªåŸºæº–è¨­å®š
        self.target_char_count = (2600, 2800)
        self.target_duration_minutes = (9.0, 11.0)
        
        self.logger.info(f"Gemini {model_name} åˆæœŸåŒ–å®Œäº†")
    
    def generate_professional_script(
        self,
        articles: List[ArticleScore],
        target_duration: float = 10.0
    ) -> Dict[str, Any]:
        """
        ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆå°æœ¬ç”Ÿæˆ
        
        Args:
            articles: é¸æŠæ¸ˆã¿è¨˜äº‹ãƒªã‚¹ãƒˆ
            target_duration: ç›®æ¨™é…ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰
            
        Returns:
            ç”Ÿæˆçµæœè¾æ›¸
        """
        try:
            self.logger.info(f"ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å°æœ¬ç”Ÿæˆé–‹å§‹ - è¨˜äº‹æ•°: {len(articles)}, ç›®æ¨™æ™‚é–“: {target_duration}åˆ†")
            
            # è¨˜äº‹æƒ…å ±æº–å‚™
            article_summaries = self._prepare_article_summaries(articles)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            prompt = self._create_professional_prompt(article_summaries, target_duration)
            
            # Gemini 2.5 Pro ã§å°æœ¬ç”Ÿæˆ
            self.logger.info("Gemini 2.5 Pro ã«ã‚ˆã‚‹é«˜å“è³ªå°æœ¬ç”Ÿæˆä¸­...")
            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.4,
                    max_output_tokens=4096,
                    candidate_count=1
                )
            )
            
            if not response.text:
                raise ValueError("Geminiã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
            
            raw_script = response.text.strip()
            self.logger.info(f"å°æœ¬ç”Ÿæˆå®Œäº† - æ–‡å­—æ•°: {len(raw_script)}")
            
            # å“è³ªè©•ä¾¡ãƒ»èª¿æ•´
            quality_result = self._evaluate_script_quality(raw_script)
            adjusted_script = self._adjust_script_quality(raw_script, quality_result)
            
            # æœ€çµ‚å“è³ªç¢ºèª
            final_quality = self._evaluate_script_quality(adjusted_script)
            
            result = {
                'script': adjusted_script,
                'char_count': len(adjusted_script),
                'estimated_duration': final_quality.estimated_duration_minutes,
                'quality_score': final_quality.overall_score,
                'quality_details': final_quality,
                'articles_used': len(articles),
                'generation_model': self.model_name,
                'generated_at': datetime.now().isoformat()
            }
            
            self.logger.info(
                f"å°æœ¬ç”ŸæˆæˆåŠŸ - æ–‡å­—æ•°: {result['char_count']}, "
                f"æ¨å®šæ™‚é–“: {result['estimated_duration']:.1f}åˆ†, "
                f"å“è³ªã‚¹ã‚³ã‚¢: {result['quality_score']:.2f}"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"å°æœ¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise
    
    def _prepare_article_summaries(self, articles: List[ArticleScore]) -> List[Dict[str, Any]]:
        """è¨˜äº‹æƒ…å ±æº–å‚™"""
        summaries = []
        
        for i, article_score in enumerate(articles, 1):
            article = article_score.article
            analysis = article_score.analysis
            
            category = getattr(analysis, 'category', None) or 'ãã®ä»–'
            region = getattr(analysis, 'region', None) or 'other'
            
            summaries.append({
                'index': i,
                'title': article.title,
                'summary': analysis.summary,
                'sentiment_score': analysis.sentiment_score,
                'category': category,
                'region': region,
                'importance_score': article_score.score,
                'published_at': article.published_at.strftime('%Yå¹´%mæœˆ%dæ—¥') if article.published_at else 'ä¸æ˜',
                'source': article.source
            })
        
        return summaries
    
    def _create_professional_prompt(
        self,
        article_summaries: List[Dict[str, Any]],
        target_duration: float
    ) -> str:
        """ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        target_chars = int(target_duration * 270)  # 1åˆ†ã‚ãŸã‚Šç´„270æ–‡å­—
        
        articles_text = ""
        for summary in article_summaries:
            articles_text += f"""
ã€è¨˜äº‹{summary['index']}ã€‘{summary['title']}
- è¦ç´„: {summary['summary']}
- ã‚«ãƒ†ã‚´ãƒª: {summary['category']}
- åœ°åŸŸ: {summary['region']}
- é‡è¦åº¦: {summary['importance_score']:.2f}
- é…ä¿¡æ—¥: {summary['published_at']}
- æƒ…å ±æº: {summary['source']}
"""
        
        prompt = f"""ã‚ãªãŸã¯15å¹´ä»¥ä¸Šã®çµŒé¨“ã‚’æŒã¤é‡‘èå¸‚å ´å°‚é–€ã®ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒ›ã‚¹ãƒˆã§ã™ã€‚
æ©Ÿé–¢æŠ•è³‡å®¶ãƒ»çµŒå–¶è€…å‘ã‘ã®é«˜å“è³ªãªå¸‚å ´åˆ†æç•ªçµ„ã‚’æ‹…å½“ã—ã€è¤‡é›‘ãªé‡‘èæƒ…å ±ã‚’å°‚é–€æ€§ã‚’ä¿ã¡ãªãŒã‚‰åˆ†ã‹ã‚Šã‚„ã™ãä¼ãˆã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚

## å°æœ¬ä½œæˆæŒ‡ç¤º

### ğŸ“Š ç•ªçµ„ä»•æ§˜
- **é…ä¿¡æ™‚é–“**: {target_duration}åˆ†å®Œå…¨ç‰ˆï¼ˆç´„{target_chars}æ–‡å­—ï¼‰
- **å¯¾è±¡è€…**: æŠ•è³‡å®¶ãƒ»çµŒå–¶è€…ãƒ»é‡‘èå°‚é–€å®¶
- **å“è³ªãƒ¬ãƒ™ãƒ«**: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç´šï¼ˆBloomberg, Reutersæ°´æº–ï¼‰
- **é…ä¿¡å½¢å¼**: éŸ³å£°ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆï¼ˆTTSåˆæˆå¯¾å¿œï¼‰

### ğŸ¯ å°æœ¬æ§‹æˆï¼ˆå¿…é ˆæ§‹é€ ï¼‰

#### **1. ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°** (200æ–‡å­—ç¨‹åº¦)
- æ—¥ä»˜ãƒ»æ›œæ—¥ã®ç¢ºèªï¼ˆ{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ãƒ»%A')}ï¼‰
- ä»Šæ—¥ã®å¸‚å ´æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ3ç‚¹ã®äºˆå‘Š
- èãæ‰‹ã¸ã®è¦ªã—ã¿ã‚„ã™ã„èªã‚Šã‹ã‘

#### **2. ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„** ({target_chars-400}æ–‡å­—ç¨‹åº¦)
**é‡è¦åº¦é †è¨˜äº‹åˆ†æ**:
- **æœ€é‡è¦è¨˜äº‹**: 400æ–‡å­—ï¼ˆè©³ç´°åˆ†æãƒ»å¸‚å ´å½±éŸ¿ãƒ»èƒŒæ™¯è§£èª¬ï¼‰
- **é‡è¦è¨˜äº‹**: 350æ–‡å­—ï¼ˆæŠ•è³‡å®¶è¦–ç‚¹ãƒ»ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æï¼‰
- **è£œå®Œè¨˜äº‹**: 250-300æ–‡å­—Ã—4è¨˜äº‹ï¼ˆç°¡æ½”ãƒ»è¦ç‚¹æ•´ç†ãƒ»ç›¸äº’é–¢é€£ï¼‰

**ç·åˆå¸‚å ´åˆ†æ**: 300æ–‡å­—
- æœ¬æ—¥ã®å¸‚å ´å…¨ä½“å‹•å‘
- æŠ•è³‡å®¶ãŒæ³¨æ„ã™ã¹ããƒªã‚¹ã‚¯è¦å› 
- ä»Šå¾Œ1é€±é–“ã®æ³¨ç›®ææ–™

#### **3. ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°** (200æ–‡å­—ç¨‹åº¦)
- æœ¬æ—¥ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•´ç†
- æ˜æ—¥ä»¥é™ã®æ³¨ç›®äº‹é …
- æ„Ÿè¬ã®è¨€è‘‰ãƒ»æ¬¡å›äºˆå‘Š

### ğŸ¨ è¡¨ç¾è¦ä»¶

**å¿…é ˆè¦ç´ **:
- **è©±ã—è¨€è‘‰**: ã€Œã€œã§ã™ã­ã€ã€Œã€œã¾ã™ã‹ã‚‰ã€ã€Œã€œã§ã—ã‚‡ã†ã€ç­‰ã®è‡ªç„¶ãªè¡¨ç¾
- **å°‚é–€ç”¨èªè§£èª¬**: ã€ŒFRBï¼ˆç±³é€£é‚¦æº–å‚™åˆ¶åº¦ç†äº‹ä¼šï¼‰ã€ã€ŒFOMCï¼ˆé€£é‚¦å…¬é–‹å¸‚å ´å§”å“¡ä¼šï¼‰ã€ç­‰
- **æ•°å€¤èª­ã¿ä¸Šã’**: ã€Œ1å…†2,500å„„å††ã€â†’ã€Œ1å…†2500å„„å††ã€ï¼ˆå¥èª­ç‚¹ãªã—ï¼‰
- **é©åˆ‡ãªé–“**: å¥èª­ç‚¹ã«ã‚ˆã‚‹è‡ªç„¶ãªåŒºåˆ‡ã‚Šï¼ˆ1æ–‡30æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰

**é¿ã‘ã‚‹è¦ç´ **:
- æŠ•è³‡æ¨å¥¨ãƒ»æ–­å®šçš„äºˆæ¸¬
- 30æ–‡å­—è¶…ã®é•·æ–‡
- æ„Ÿæƒ…çš„ã™ãã‚‹è¡¨ç¾
- è¤‡é›‘ãªå°‚é–€ç”¨èªã®é€£ç¶š

### ğŸ“ˆ åˆ†æå¯¾è±¡è¨˜äº‹
{articles_text}

### ğŸ¯ å“è³ªåŸºæº–
- æ–‡å­—æ•°: {target_chars-100}ã€œ{target_chars+100}æ–‡å­—ï¼ˆå³å¯†ï¼‰
- èª­ã¿ã‚„ã™ã•: TTSéŸ³å£°ã§ã®è‡ªç„¶ãªç™ºè©±
- å°‚é–€æ€§: æŠ•è³‡åˆ¤æ–­ã«è³‡ã™ã‚‹æ·±ã„æ´å¯Ÿ
- å®Ÿè·µæ€§: å…·ä½“çš„ãªãƒªã‚¹ã‚¯è©•ä¾¡ãƒ»å¸‚å ´è¦‹é€šã—

---

**ä¸Šè¨˜è¦ä»¶ã«å¾“ã„ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç´šã®10åˆ†é–“ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå°æœ¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚**
å°æœ¬ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚"""

        return prompt
    
    def _evaluate_script_quality(self, script: str) -> ScriptQuality:
        """å°æœ¬å“è³ªè©•ä¾¡"""
        char_count = len(script)
        estimated_duration = char_count / 270.0  # 1åˆ†ã‚ãŸã‚Š270æ–‡å­—æƒ³å®š
        
        issues = []
        
        # æ–‡å­—æ•°è©•ä¾¡
        char_min, char_max = self.target_char_count
        if char_count < char_min:
            issues.append(f"æ–‡å­—æ•°ä¸è¶³: {char_count} < {char_min}")
        elif char_count > char_max:
            issues.append(f"æ–‡å­—æ•°è¶…é: {char_count} > {char_max}")
        
        char_score = 1.0
        if char_count < char_min:
            char_score = char_count / char_min
        elif char_count > char_max:
            char_score = char_max / char_count
        
        # æ™‚é–“è©•ä¾¡
        duration_min, duration_max = self.target_duration_minutes
        duration_score = 1.0
        if estimated_duration < duration_min:
            duration_score = estimated_duration / duration_min
        elif estimated_duration > duration_max:
            duration_score = duration_max / estimated_duration
        
        # æ§‹é€ è©•ä¾¡ï¼ˆã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ãƒ»ãƒ¡ã‚¤ãƒ³ãƒ»ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã®å­˜åœ¨ï¼‰
        structure_indicators = ['ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™', 'ã“ã‚“ã«ã¡ã¯', 'æœ¬æ—¥', 'ä»Šæ—¥']
        closing_indicators = ['ä»¥ä¸Š', 'ã‚ã‚ŠãŒã¨ã†', 'æ¬¡å›', 'ã¾ãŸ']
        
        has_opening = any(indicator in script[:300] for indicator in structure_indicators)
        has_closing = any(indicator in script[-300:] for indicator in closing_indicators)
        
        structure_score = (int(has_opening) + int(has_closing)) / 2.0
        
        # èª­ã¿ã‚„ã™ã•è©•ä¾¡ï¼ˆé©åˆ‡ãªå¥èª­ç‚¹ãƒ»æ–‡é•·ï¼‰
        sentences = script.split('ã€‚')
        long_sentences = [s for s in sentences if len(s) > 40]
        readability_score = max(0.0, 1.0 - len(long_sentences) / len(sentences))
        
        # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«åº¦è©•ä¾¡ï¼ˆå°‚é–€ç”¨èªãƒ»åˆ†ææ·±åº¦ï¼‰
        professional_terms = ['å¸‚å ´', 'æŠ•è³‡', 'ä¼æ¥­', 'æ¥­ç¸¾', 'çµŒæ¸ˆ', 'é‡‘è', 'æ”¿ç­–', 'åˆ†æ']
        professional_count = sum(script.count(term) for term in professional_terms)
        professional_score = min(1.0, professional_count / 20.0)  # 20å›ä»¥ä¸Šã§æº€ç‚¹
        
        # ç·åˆè©•ä¾¡
        overall_score = (
            char_score * 0.3 +
            duration_score * 0.2 +
            structure_score * 0.2 +
            readability_score * 0.15 +
            professional_score * 0.15
        )
        
        return ScriptQuality(
            char_count=char_count,
            estimated_duration_minutes=estimated_duration,
            structure_score=structure_score,
            readability_score=readability_score,
            professional_score=professional_score,
            overall_score=overall_score,
            issues=issues
        )
    
    def _adjust_script_quality(self, script: str, quality: ScriptQuality) -> str:
        """å°æœ¬å“è³ªèª¿æ•´"""
        if quality.overall_score >= 0.8 and not quality.issues:
            return script
        
        self.logger.info(f"å°æœ¬å“è³ªèª¿æ•´å®Ÿè¡Œ - ç¾å“è³ª: {quality.overall_score:.2f}")
        
        # æ–‡å­—æ•°èª¿æ•´ãŒå¿…è¦ãªå ´åˆ
        char_min, char_max = self.target_char_count
        if quality.char_count < char_min or quality.char_count > char_max:
            try:
                adjustment_prompt = f"""ä»¥ä¸‹ã®å°æœ¬ã®æ–‡å­—æ•°ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

ç›®æ¨™æ–‡å­—æ•°: {char_min}-{char_max}æ–‡å­—
ç¾åœ¨æ–‡å­—æ•°: {quality.char_count}æ–‡å­—

èª¿æ•´æ–¹é‡:
- æ–‡å­—æ•°ä¸è¶³ã®å ´åˆ: å¸‚å ´åˆ†æã‚’æ·±æ˜ã‚Šã€å…·ä½“ä¾‹ã‚„èƒŒæ™¯æƒ…å ±ã‚’è¿½åŠ 
- æ–‡å­—æ•°è¶…éã®å ´åˆ: é‡è¤‡è¡¨ç¾ã‚’å‰Šé™¤ã€ç°¡æ½”ãªè¡¨ç¾ã«å¤‰æ›´
- å“è³ªãƒ»å°‚é–€æ€§ã¯ç¶­æŒ

å°æœ¬:
{script}

èª¿æ•´å¾Œã®å°æœ¬ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""
                
                response = self.model.generate_content(
                    adjustment_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        max_output_tokens=4096
                    )
                )
                
                if response.text:
                    adjusted = response.text.strip()
                    self.logger.info(f"æ–‡å­—æ•°èª¿æ•´å®Œäº†: {len(script)} â†’ {len(adjusted)}")
                    return adjusted
                    
            except Exception as e:
                self.logger.error(f"å°æœ¬èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
        
        return script