# -*- coding: utf-8 -*-

"""
ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆçµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
10åˆ†å®Œå…¨ç‰ˆãƒ»é«˜å“è³ªè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

from src.database.database_manager import DatabaseManager
from src.podcast.data_fetcher.enhanced_database_article_fetcher import (
    EnhancedDatabaseArticleFetcher,
)
from src.podcast.data_fetcher.google_document_data_fetcher import GoogleDocumentDataFetcher
from src.podcast.script_generation.professional_dialogue_script_generator import (
    ProfessionalDialogueScriptGenerator,
)
from src.podcast.integration.podcast_integration_manager import PodcastIntegrationManager


class ProductionPodcastIntegrationManager:
    """ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆçµ±åˆç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config=None):
        """
        åˆæœŸåŒ–

        Args:
            config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆçœç•¥æ™‚ã¯ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’ä½¿ç”¨ï¼‰
        """
        self.logger = logging.getLogger(__name__)

        try:
            # è¨­å®šç®¡ç†ã‚’ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›´
            self.config = self._load_config_from_env()

            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
            self.data_source = os.getenv(
                "PODCAST_DATA_SOURCE", "database"
            )  # database | google_document
            self.google_doc_id = os.getenv("GOOGLE_DOCUMENT_ID") or os.getenv(
                "GOOGLE_OVERWRITE_DOC_ID"
            )

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›
            self.logger.info(f"ç’°å¢ƒå¤‰æ•°ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
            self.logger.info(f"  PODCAST_DATA_SOURCE = {os.getenv('PODCAST_DATA_SOURCE', 'æœªè¨­å®š')}")
            self.logger.info(f"  GOOGLE_DOCUMENT_ID = {os.getenv('GOOGLE_DOCUMENT_ID', 'æœªè¨­å®š')}")
            self.logger.info(f"  GOOGLE_OVERWRITE_DOC_ID = {os.getenv('GOOGLE_OVERWRITE_DOC_ID', 'æœªè¨­å®š')}")
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š: {self.data_source}")
            if self.google_doc_id:
                self.logger.info(f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID: {self.google_doc_id}")
            else:
                self.logger.warning("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # åŸºæœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self._initialize_data_fetcher()

            # Gemini 2.5 Proå°æœ¬ç”Ÿæˆå™¨
            self._initialize_script_generator()

            # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ æ´»ç”¨ï¼ˆéŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡ï¼‰
            self._initialize_base_manager(config)

            self.logger.info("ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆçµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            self.logger.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

    def _load_config_from_env(self) -> Dict[str, Any]:
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        return {
            "gemini_model": os.getenv("GEMINI_PODCAST_MODEL", "gemini-2.5-pro"),
            "gemini_api_key": os.getenv("GEMINI_API_KEY"),
            "production_mode": os.getenv("PODCAST_PRODUCTION_MODE", "true").lower() == "true",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’trueã«å¤‰æ›´
            "target_duration": float(os.getenv("PODCAST_TARGET_DURATION_MINUTES", "10.0")),
            "target_script_chars": int(os.getenv("PODCAST_TARGET_SCRIPT_CHARS", "2700")),
            "data_source": os.getenv("PODCAST_DATA_SOURCE", "database"),  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’databaseã«æ˜ç¤º
            "google_doc_id": os.getenv("GOOGLE_DOCUMENT_ID") 
            or os.getenv("GOOGLE_OVERWRITE_DOC_ID")
            or os.getenv("GOOGLE_DAILY_SUMMARY_DOC_ID")  # AIè¦ç´„Docå¯¾å¿œ
            or os.getenv("GOOGLE_AI_SUMMARY_DOC_ID"),     # AIè¦ç´„Docå¯¾å¿œ
        }

    def _create_database_config(self):
        """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰DatabaseConfigã‚’ç”Ÿæˆ"""
        try:
            from config.base import DatabaseConfig

            return DatabaseConfig(
                url=os.getenv("DATABASE_URL", "sqlite:///market_news.db"),
                echo=os.getenv("DATABASE_ECHO", "false").lower() == "true",
            )
        except ImportError as e:
            self.logger.warning(f"DatabaseConfig importå¤±æ•—: {e}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            class SimpleDatabaseConfig:
                def __init__(self):
                    self.url = os.getenv("DATABASE_URL", "sqlite:///market_news.db")
                    self.echo = os.getenv("DATABASE_ECHO", "false").lower() == "true"

            return SimpleDatabaseConfig()
        except Exception as e:
            self.logger.error(f"DatabaseConfigç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            class DefaultDatabaseConfig:
                def __init__(self):
                    self.url = "sqlite:///market_news.db"
                    self.echo = False

            return DefaultDatabaseConfig()

    def _initialize_data_fetcher(self):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®æœ€é©åŒ–
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šåˆ¤å®š:")
            self.logger.info(f"  PODCAST_DATA_SOURCE = {os.getenv('PODCAST_DATA_SOURCE', 'æœªè¨­å®š')}")
            self.logger.info(f"  GOOGLE_DOCUMENT_ID = {'è¨­å®šæ¸ˆã¿' if self.google_doc_id else 'æœªè¨­å®š'}")
            
            # å„ªå…ˆé †ä½: æ˜ç¤ºçš„ãªdatabaseè¨­å®š > Google Doc IDå­˜åœ¨ > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(database)
            if self.data_source == "database":
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰å„ªå…ˆå®Ÿè¡Œ
                try:
                    self.logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–ã‚’è©¦è¡Œ")
                    
                    # ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹ã§DatabaseConfigã‚’ç”Ÿæˆ
                    db_config = self._create_database_config()
                    self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š: URL={db_config.url}")

                    # DatabaseManagerã‚’æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åˆæœŸåŒ–
                    self.db_manager = DatabaseManager(db_config)
                    self.article_fetcher = EnhancedDatabaseArticleFetcher(self.db_manager)
                    self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–æˆåŠŸ")
                    return

                except Exception as db_error:
                    self.logger.warning(
                        f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å¤±æ•—: {type(db_error).__name__}: {str(db_error)[:100]}"
                    )
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¤å®š
                    if self.google_doc_id:
                        self.logger.info("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        self.data_source = "google_document"
                    else:
                        self.logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¤±æ•—ã€Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚æœªè¨­å®š")
                        raise ValueError(
                            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—ã€ã‹ã¤Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDæœªè¨­å®šã€‚"
                            "GOOGLE_DOCUMENT_IDç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„"
                        )
            
            # Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
            if self.data_source == "google_document":
                if not self.google_doc_id:
                    # AIè¦ç´„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•æ¤œç´¢ã‚’è©¦è¡Œ
                    temp_fetcher = GoogleDocumentDataFetcher("")
                    auto_found_id = temp_fetcher._search_daily_summary_document()
                    
                    if auto_found_id:
                        self.google_doc_id = auto_found_id
                        self.logger.info(f"AIè¦ç´„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•ç™ºè¦‹: {auto_found_id}")
                    else:
                        raise ValueError(
                            "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDæœªè¨­å®šã€‚ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„: "
                            "GOOGLE_DOCUMENT_ID, GOOGLE_DAILY_SUMMARY_DOC_ID, GOOGLE_AI_SUMMARY_DOC_ID"
                        )
                
                self.article_fetcher = GoogleDocumentDataFetcher(self.google_doc_id)
                self.logger.info(f"âœ… Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–æˆåŠŸ (ID: {self.google_doc_id})")
            
        except Exception as e:
            self.logger.error(
                f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}", exc_info=True
            )
            raise

    def _initialize_script_generator(self):
        """ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–"""
        try:
            gemini_model = self.config["gemini_model"]
            gemini_api_key = self.config["gemini_api_key"]

            if not gemini_api_key:
                raise ValueError("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

            self.script_generator = ProfessionalDialogueScriptGenerator(
                api_key=gemini_api_key, model_name=gemini_model
            )

            self.logger.info(f"Geminiå°æœ¬ç”Ÿæˆå™¨åˆæœŸåŒ–å®Œäº† - ãƒ¢ãƒ‡ãƒ«: {gemini_model}")

        except Exception as e:
            self.logger.error(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

    def _initialize_base_manager(self, config):
        """ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        try:
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯éŸ³å£°å‡¦ç†ä¾å­˜ã‚’é¿ã‘ã‚‹
            test_mode = os.getenv("PODCAST_TEST_MODE", "false").lower() == "true"
            
            if test_mode:
                self.logger.info("ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: éŸ³å£°å‡¦ç†ç³»ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
                self.base_manager = None
                return
            
            # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã§ã¯è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ãŸã‚ã€
            # å¼•æ•°ã§å—ã‘å–ã£ãŸè¨­å®šã‹ã€Noneã‚’æ¸¡ã™
            self.base_manager = PodcastIntegrationManager(config=config)
            self.logger.info("ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            self.logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–è­¦å‘Š: {e}")
            # éŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡æ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã—ã¦ã‚‚ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½ã¯å‹•ä½œå¯èƒ½
            self.base_manager = None

    def generate_complete_podcast(
        self, test_mode: bool = False, target_duration: float = None
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆï¼ˆå“è³ªè©•ä¾¡ãƒ»è‡ªå‹•èª¿æ•´ä»˜ãï¼‰

        Args:
            test_mode: ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®é…ä¿¡ã‚’è¡Œã‚ãªã„ï¼‰
            target_duration: ç›®æ¨™é…ä¿¡æ™‚é–“ï¼ˆåˆ†ï¼‰

        Returns:
            ç”Ÿæˆçµæœè¾æ›¸
        """
        try:
            start_time = datetime.now()

            # è¨­å®šå€¤å–å¾—
            target_duration = target_duration or float(
                os.getenv("PODCAST_TARGET_DURATION_MINUTES", "10.0")
            )
            production_mode = os.getenv("PODCAST_PRODUCTION_MODE", "false").lower() == "true"

            self.logger.info(
                f"ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆé–‹å§‹ - "
                f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {test_mode}, ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰: {production_mode}, "
                f"ç›®æ¨™æ™‚é–“: {target_duration}åˆ†, ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {self.data_source}"
            )

            # Phase 1: é«˜åº¦è¨˜äº‹é¸æŠ
            self.logger.info("Phase 1: é«˜åº¦è¨˜äº‹é¸æŠã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ")
            articles = self.article_fetcher.fetch_articles_for_podcast(
                target_count=18, hours_back=48
            )

            if not articles:
                raise ValueError("é¸æŠå¯èƒ½ãªè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            self.logger.info(f"è¨˜äº‹é¸æŠå®Œäº† - é¸æŠæ•°: {len(articles)}")

            # Phase 2: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å°æœ¬ç”Ÿæˆ
            self.logger.info("Phase 2: Gemini 2.5 Pro ã«ã‚ˆã‚‹é«˜å“è³ªå°æœ¬ç”Ÿæˆ")
            script_result = self.script_generator.generate_professional_script(
                articles=articles, target_duration=target_duration
            )

            # Phase 2.5: å“è³ªè©•ä¾¡ã¨è‡ªå‹•èª¿æ•´
            self.logger.info("Phase 2.5: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡ãƒ»è‡ªå‹•èª¿æ•´")
            quality_evaluation = self._evaluate_content_quality(articles, script_result)
            
            # å“è³ªãŒé–¾å€¤æœªæº€ã®å ´åˆã€1å›ã ã‘å†ç”Ÿæˆã‚’è©¦è¡Œ
            regeneration_attempted = False
            if not quality_evaluation["meets_quality_threshold"] and not test_mode:
                self.logger.warning(
                    f"å“è³ªåŸºæº–æœªé” (ç·åˆã‚¹ã‚³ã‚¢: {quality_evaluation['total_quality_score']:.2f}, "
                    f"ã‚«ãƒãƒ¬ãƒƒã‚¸: {quality_evaluation['coverage_score']:.2f}), å†ç”Ÿæˆã‚’è©¦è¡Œ"
                )
                
                # è¨˜äº‹å†é¸æŠï¼ˆã‚ˆã‚Šåºƒç¯„å›²ã‹ã‚‰ï¼‰
                self.logger.info("è¨˜äº‹å†é¸æŠå®Ÿè¡Œï¼ˆç¯„å›²æ‹¡å¤§ï¼‰")
                retry_articles = self.article_fetcher.fetch_articles_for_podcast(
                    target_count=8, hours_back=36  # ç¯„å›²æ‹¡å¤§
                )
                
                if retry_articles and len(retry_articles) > len(articles):
                    self.logger.info("å°æœ¬å†ç”Ÿæˆå®Ÿè¡Œ")
                    retry_script_result = self.script_generator.generate_professional_script(
                        articles=retry_articles, target_duration=target_duration
                    )
                    
                    # å†è©•ä¾¡
                    retry_quality = self._evaluate_content_quality(retry_articles, retry_script_result)
                    
                    # ã‚ˆã‚Šè‰¯ã„çµæœãªã‚‰æ¡ç”¨
                    if retry_quality["total_quality_score"] > quality_evaluation["total_quality_score"]:
                        self.logger.info(
                            f"å†ç”Ÿæˆæ¡ç”¨ (æ”¹å–„: {retry_quality['total_quality_score']:.2f} > "
                            f"{quality_evaluation['total_quality_score']:.2f})"
                        )
                        articles = retry_articles
                        script_result = retry_script_result
                        quality_evaluation = retry_quality
                        regeneration_attempted = True
                    else:
                        self.logger.info("å†ç”Ÿæˆå“è³ªãŒæ”¹å–„ã•ã‚Œãšã€å…ƒã®çµæœã‚’æ¡ç”¨")
                else:
                    self.logger.warning("å†é¸æŠã§ååˆ†ãªè¨˜äº‹ãŒå¾—ã‚‰ã‚Œãšã€å…ƒã®çµæœã‚’æ¡ç”¨")

            # Phase 3: éŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ æ´»ç”¨ï¼‰
            self.logger.info("Phase 3: éŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ")

            # å°æœ¬ã‚’ä¸€æ™‚ä¿å­˜
            script_content = script_result["script"]
            temp_script_path = Path("temp_podcast_script.txt")
            with open(temp_script_path, "w", encoding="utf-8") as f:
                f.write(script_content)

            try:
                # æ—¢å­˜ã®éŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                if self.base_manager:
                    broadcast_result = self.base_manager.run_daily_podcast_workflow(
                        test_mode=test_mode, custom_script_path=str(temp_script_path)
                    )
                else:
                    self.logger.warning(
                        "ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚éŸ³å£°ç”Ÿæˆãƒ»é…ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                    )
                    broadcast_result = True  # ãƒ†ã‚¹ãƒˆç›®çš„ã§ã¯æˆåŠŸã¨ã¿ãªã™

            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                if temp_script_path.exists():
                    temp_script_path.unlink()

            # çµæœçµ±åˆ
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            # è¨˜äº‹çµ±è¨ˆå–å¾—
            article_stats = self.article_fetcher.get_article_statistics(hours_back=24)

            result = {
                "success": broadcast_result,
                "production_mode": production_mode,
                "test_mode": test_mode,
                "regeneration_attempted": regeneration_attempted,
                "script_generation": script_result,
                "articles_analysis": {
                    "selected_count": len(articles),
                    "data_source": self.data_source,
                    "article_scores": [
                        {
                            "title": a.get("article", {}).get("title", "")[:100],
                            "score": a.get("score", 0),
                            "category": a.get("analysis", {}).get("category", "ãã®ä»–"),
                            "region": a.get("analysis", {}).get("region", "other"),
                            "source": a.get("article", {}).get("source", "Unknown"),
                        }
                        for a in articles
                    ],
                },
                "system_metrics": {
                    "total_processing_time_seconds": processing_time,
                    "script_generation_model": script_result.get("generation_model"),
                    "database_statistics": article_stats,
                },
                "quality_assessment": {
                    "script_char_count": script_result.get("char_count"),
                    "estimated_duration_minutes": script_result.get("estimated_duration"),
                    "quality_score": script_result.get("quality_score"),
                    "quality_details": script_result.get("quality_details"),
                    "coverage_evaluation": quality_evaluation,  # æ–°è¦è¿½åŠ 
                },
                "generated_at": start_time.isoformat(),
                "completed_at": end_time.isoformat(),
            }

            # å“è³ªãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
            self._generate_quality_report(result)

            if broadcast_result:
                final_quality = quality_evaluation["total_quality_score"]
                coverage_score = quality_evaluation["coverage_score"]
                self.logger.info(
                    f"ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆå®Œäº† - "
                    f"å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’, "
                    f"å“è³ªã‚¹ã‚³ã‚¢: {final_quality:.2f}, ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_score:.2f}"
                )
            else:
                self.logger.error("ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            return result

        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__,
                "generated_at": datetime.now().isoformat(),
            }

    def run_daily_podcast_workflow(self, test_mode: bool = False) -> bool:
        """
        æ—¥æ¬¡ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ

        Args:
            test_mode: ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰

        Returns:
            æˆåŠŸå¯å¦
        """
        try:
            result = self.generate_complete_podcast(test_mode=test_mode)
            return result.get("success", False)

        except Exception as e:
            self.logger.error(f"æ—¥æ¬¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    def run_script_only_workflow(self) -> bool:
        """
        å°æœ¬ç”Ÿæˆå°‚ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
        å°æœ¬ç”Ÿæˆã¨åˆ†æã®ã¿ã‚’å®Ÿè¡Œã—ã€éŸ³å£°ç”Ÿæˆã¯è¡Œã‚ãªã„
        
        Returns:
            bool: æˆåŠŸæ™‚True
        """
        try:
            self.logger.info("ğŸ¬ å°æœ¬ç”Ÿæˆå°‚ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹")
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—
            self.logger.info("ğŸ“° è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            articles = self._fetch_articles()
            
            if not articles:
                self.logger.error("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return False
                
            self.logger.info(f"âœ… è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(articles)}ä»¶")
            
            # å°æœ¬ç”Ÿæˆ
            self.logger.info("ğŸ“ å°æœ¬ç”Ÿæˆä¸­...")
            script = self.script_generator.generate_script(articles)
            
            if not script:
                self.logger.error("å°æœ¬ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
                
            # å°æœ¬åˆ†æ
            script_info = self._analyze_script_content(script)
            
            # å°æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            script_file_path = self._save_script_file(script, script_info)
            
            # åˆ†æçµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            self._display_script_analysis(script, script_info, script_file_path)
            
            # GitHubã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ç’°å¢ƒå¤‰æ•°å‡ºåŠ›
            self._output_github_actions_script_info(script_info, script_file_path)
            
            self.logger.info("âœ… å°æœ¬ç”Ÿæˆå°‚ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å°æœ¬ç”Ÿæˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False

    def _fetch_articles(self) -> List[Dict]:
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self.article_fetcher.fetch_articles_for_podcast(
            target_count=12, hours_back=48
        )
    
    def _analyze_script_content(self, script: str) -> Dict[str, Any]:
        """å°æœ¬å†…å®¹ã‚’åˆ†æ"""
        lines = script.split('\n')
        total_lines = len(lines)
        char_count = len(script)
        
        # ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼åˆ¥ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        speaker_a_lines = 0
        speaker_b_lines = 0
        other_lines = 0
        
        for line in lines:
            line = line.strip()
            if line.startswith('A:'):
                speaker_a_lines += 1
            elif line.startswith('B:'):
                speaker_b_lines += 1
            elif line:  # ç©ºè¡Œã§ãªã„å ´åˆ
                other_lines += 1
        
        # æ¨å®šèª­ã¿ä¸Šã’æ™‚é–“ï¼ˆæ—¥æœ¬èª: ç´„400æ–‡å­—/åˆ†ï¼‰
        estimated_minutes = char_count / 400
        estimated_duration = f"{int(estimated_minutes)}åˆ†{int((estimated_minutes % 1) * 60)}ç§’"
        
        # å•é¡Œæ¤œå‡º
        issues = []
        if char_count < 1000:
            issues.append("å°æœ¬ãŒçŸ­ã™ãã¾ã™ï¼ˆ1000æ–‡å­—æœªæº€ï¼‰")
        if char_count > 8000:
            issues.append("å°æœ¬ãŒé•·ã™ãã¾ã™ï¼ˆ8000æ–‡å­—è¶…éï¼‰")
        if speaker_a_lines == 0:
            issues.append("ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼Aã®å°è©ãŒã‚ã‚Šã¾ã›ã‚“")
        if speaker_b_lines == 0:
            issues.append("ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼Bã®å°è©ãŒã‚ã‚Šã¾ã›ã‚“")
        if abs(speaker_a_lines - speaker_b_lines) > max(speaker_a_lines, speaker_b_lines) * 0.3:
            issues.append("ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼é–“ã®å°è©æ•°ãƒãƒ©ãƒ³ã‚¹ãŒæ‚ªã„")
        
        return {
            'char_count': char_count,
            'line_count': total_lines,
            'speaker_a_lines': speaker_a_lines,
            'speaker_b_lines': speaker_b_lines,
            'other_lines': other_lines,
            'estimated_duration': estimated_duration,
            'estimated_minutes': estimated_minutes,
            'issues': issues
        }
    
    def _save_script_file(self, script: str, script_info: Dict[str, Any]) -> str:
        """å°æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        episode_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output/podcast")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        script_file_path = output_dir / f"{episode_id}_script.txt"
        
        with open(script_file_path, "w", encoding="utf-8") as f:
            f.write(script)
        
        self.logger.info(f"å°æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {script_file_path}")
        return str(script_file_path)
    
    def _display_script_analysis(self, script: str, script_info: Dict[str, Any], script_file_path: str) -> None:
        """å°æœ¬åˆ†æçµæœã‚’è©³ç´°è¡¨ç¤º"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“„ å°æœ¬åˆ†æçµæœ")
        self.logger.info("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        self.logger.info(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        self.logger.info(f"  æ–‡å­—æ•°: {script_info['char_count']:,}æ–‡å­—")
        self.logger.info(f"  è¡Œæ•°: {script_info['line_count']}è¡Œ")
        self.logger.info(f"  æ¨å®šæ™‚é–“: {script_info['estimated_duration']}")
        self.logger.info(f"  æ¨å®šæ™‚é–“ï¼ˆåˆ†ï¼‰: {script_info['estimated_minutes']:.1f}åˆ†")
        
        # ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼åˆ†æ
        self.logger.info(f"\nğŸ­ ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼åˆ†æ:")
        self.logger.info(f"  ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼A: {script_info['speaker_a_lines']}è¡Œ")
        self.logger.info(f"  ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼B: {script_info['speaker_b_lines']}è¡Œ")
        self.logger.info(f"  ãã®ä»–: {script_info['other_lines']}è¡Œ")
        
        total_speaker_lines = script_info['speaker_a_lines'] + script_info['speaker_b_lines']
        if total_speaker_lines > 0:
            a_ratio = script_info['speaker_a_lines'] / total_speaker_lines * 100
            b_ratio = script_info['speaker_b_lines'] / total_speaker_lines * 100
            self.logger.info(f"  A:Bæ¯”ç‡ = {a_ratio:.1f}% : {b_ratio:.1f}%")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        self.logger.info(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:")
        self.logger.info(f"  ä¿å­˜å…ˆ: {script_file_path}")
        
        # å•é¡Œæ¤œå‡º
        if script_info['issues']:
            self.logger.info(f"\nâš ï¸  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
            for issue in script_info['issues']:
                self.logger.warning(f"  - {issue}")
        else:
            self.logger.info(f"\nâœ… å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # å°æœ¬ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
        self.logger.info(f"\nğŸ“– å°æœ¬ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
        self.logger.info("-" * 40)
        preview = script[:500] + "..." if len(script) > 500 else script
        self.logger.info(preview)
        self.logger.info("-" * 40)
        
        self.logger.info("=" * 60)
    
    def _output_github_actions_script_info(self, script_info: Dict[str, Any], script_file_path: str) -> None:
        """GitHubã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ç’°å¢ƒå¤‰æ•°å‡ºåŠ›"""
        github_output = os.getenv("GITHUB_OUTPUT")
        if not github_output:
            return
            
        try:
            with open(github_output, "a", encoding="utf-8") as f:
                f.write(f"script_char_count={script_info['char_count']}\n")
                f.write(f"script_estimated_duration={script_info['estimated_duration']}\n")
                f.write(f"script_speaker_a_lines={script_info['speaker_a_lines']}\n")
                f.write(f"script_speaker_b_lines={script_info['speaker_b_lines']}\n")
                f.write(f"script_file_path={script_file_path}\n")
                f.write(f"script_issues_count={len(script_info['issues'])}\n")
                f.write(f"script_has_issues={'true' if script_info['issues'] else 'false'}\n")
                
            self.logger.info("GitHubã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.warning(f"GitHubã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")

    def _generate_quality_report(self, result: Dict[str, Any]) -> None:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        try:
            coverage_eval = result.get("quality_assessment", {}).get("coverage_evaluation", {})
            
            report_lines = [
                "=== ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç‰ˆãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆ ===",
                f"ç”Ÿæˆæ—¥æ™‚: {result['generated_at']}",
                f"å®Œäº†æ—¥æ™‚: {result['completed_at']}",
                f"å‡¦ç†æ™‚é–“: {result['system_metrics']['total_processing_time_seconds']:.1f}ç§’",
                f"å†ç”Ÿæˆå®Ÿæ–½: {'ã¯ã„' if result.get('regeneration_attempted', False) else 'ã„ã„ãˆ'}",
                "",
                "## å°æœ¬å“è³ª",
                f"æ–‡å­—æ•°: {result['quality_assessment']['script_char_count']}æ–‡å­—",
                f"æ¨å®šå†ç”Ÿæ™‚é–“: {result['quality_assessment']['estimated_duration_minutes']:.1f}åˆ†",
                f"å“è³ªã‚¹ã‚³ã‚¢: {result['quality_assessment']['quality_score']:.2f}/1.0",
                "",
                "## ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡",
            ]
            
            if coverage_eval:
                report_lines.extend([
                    f"ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {coverage_eval.get('total_quality_score', 0):.2f}/1.0",
                    f"ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢: {coverage_eval.get('coverage_score', 0):.2f}/1.0",
                    f"æ–‡å­—æ•°ç²¾åº¦: {coverage_eval.get('char_accuracy', 0):.2f}/1.0",
                    f"å“è³ªåŸºæº–é”æˆ: {'âœ… é”æˆ' if coverage_eval.get('meets_quality_threshold', False) else 'âŒ æœªé”æˆ'}",
                    "",
                ])
                
                # å“è³ªèª²é¡Œã®è¡¨ç¤º
                quality_issues = coverage_eval.get('quality_issues', [])
                if quality_issues:
                    report_lines.append("âš ï¸ æ¤œå‡ºã•ã‚ŒãŸå“è³ªèª²é¡Œ:")
                    for issue in quality_issues:
                        report_lines.append(f"  - {issue}")
                    report_lines.append("")

            # è¨˜äº‹é¸æŠåˆ†æ
            report_lines.append("## è¨˜äº‹é¸æŠ")
            report_lines.append(f"é¸æŠè¨˜äº‹æ•°: {result['articles_analysis']['selected_count']}")
            report_lines.append(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {result['articles_analysis']['data_source']}")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
            if coverage_eval and 'category_distribution' in coverage_eval:
                category_dist = coverage_eval['category_distribution']
                report_lines.append("ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
                for category, count in category_dist.items():
                    report_lines.append(f"  - {category}: {count}è¨˜äº‹")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
                category_count = {}
                for article in result["articles_analysis"]["article_scores"]:
                    cat = article["category"]
                    category_count[cat] = category_count.get(cat, 0) + 1
                
                report_lines.append("ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
                for category, count in category_count.items():
                    report_lines.append(f"  - {category}: {count}è¨˜äº‹")

            # åœ°åŸŸåˆ†å¸ƒ
            if coverage_eval and 'region_distribution' in coverage_eval:
                region_dist = coverage_eval['region_distribution']
                report_lines.append("\nåœ°åŸŸåˆ†å¸ƒ:")
                for region, count in region_dist.items():
                    report_lines.append(f"  - {region}: {count}è¨˜äº‹")
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
                region_count = {}
                for article in result["articles_analysis"]["article_scores"]:
                    region = article["region"]
                    region_count[region] = region_count.get(region, 0) + 1

                report_lines.append("\nåœ°åŸŸåˆ†å¸ƒ:")
                for region, count in region_count.items():
                    report_lines.append(f"  - {region}: {count}è¨˜äº‹")
                    
            # ã‚½ãƒ¼ã‚¹åˆ†å¸ƒï¼ˆæ–°è¦ï¼‰
            if coverage_eval and 'source_distribution' in coverage_eval:
                source_dist = coverage_eval['source_distribution']
                report_lines.append("\nã‚½ãƒ¼ã‚¹åˆ†å¸ƒ:")
                for source, count in source_dist.items():
                    report_lines.append(f"  - {source}: {count}è¨˜äº‹")

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
            db_stats = result["system_metrics"]["database_statistics"]
            if db_stats:
                report_lines.extend([
                    "",
                    "## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ",
                    f"ç·è¨˜äº‹æ•°: {db_stats.get('total_articles', 0)}",
                    f"åˆ†ææ¸ˆè¨˜äº‹æ•°: {db_stats.get('analyzed_articles', 0)}",
                    f"åˆ†æç‡: {db_stats.get('analysis_rate', 0):.1%}",
                ])

            report_lines.append("\n" + "=" * 50)

            # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
            report_content = "\n".join(report_lines)
            self.logger.info(f"å“è³ªãƒ¬ãƒãƒ¼ãƒˆ:\n{report_content}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_path = (
                Path("logs")
                / f"podcast_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            )
            report_path.parent.mkdir(exist_ok=True)

            with open(report_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            self.logger.info(f"å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {report_path}")

        except Exception as e:
            self.logger.error(f"å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨æ™‚ã®ã¿ï¼‰
            db_status = True
            if self.data_source == "database" and hasattr(self, "db_manager"):
                try:
                    with self.db_manager.get_session() as session:
                        session.execute("SELECT 1")
                except Exception:
                    db_status = False

            # è¨˜äº‹çµ±è¨ˆ
            article_stats = self.article_fetcher.get_article_statistics(hours_back=24)

            # è¨­å®šç¢ºèª
            gemini_model = os.getenv("GEMINI_PODCAST_MODEL", "gemini-2.5-pro")
            production_mode = os.getenv("PODCAST_PRODUCTION_MODE", "false").lower() == "true"

            return {
                "system_healthy": db_status,
                "database_connected": db_status,
                "data_source": self.data_source,
                "google_document_id": (
                    self.google_doc_id if self.data_source == "google_document" else None
                ),
                "article_statistics": article_stats,
                "configuration": {
                    "gemini_model": gemini_model,
                    "production_mode": production_mode,
                    "target_duration": float(os.getenv("PODCAST_TARGET_DURATION_MINUTES", "10.0")),
                    "target_script_chars": int(os.getenv("PODCAST_TARGET_SCRIPT_CHARS", "2700")),
                },
                "checked_at": datetime.now().isoformat(),
            }

        except Exception as e:
            self.logger.error(f"ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {
                "system_healthy": False,
                "error": str(e),
                "checked_at": datetime.now().isoformat(),
            }

    def _evaluate_content_quality(self, articles: List, script_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã¨ã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡
        
        Args:
            articles: é¸æŠã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
            script_result: å°æœ¬ç”Ÿæˆçµæœ
            
        Returns:
            å“è³ªè©•ä¾¡çµæœ
        """
        try:
            # ã‚«ãƒ†ã‚´ãƒªãƒ»åœ°åŸŸãƒ»ã‚½ãƒ¼ã‚¹åˆ†å¸ƒåˆ†æ
            category_counts = {}
            region_counts = {}
            source_counts = {}
            time_distribution = {}
            
            for article_score in articles:
                # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
                category = article_score.get('analysis', {}).get("category", "ãã®ä»–")
                category_counts[category] = category_counts.get(category, 0) + 1
                
                # åœ°åŸŸåˆ†å¸ƒ
                region = article_score.get('analysis', {}).get("region", "other")
                region_counts[region] = region_counts.get(region, 0) + 1
                
                # ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ
                source = article_score.get('article', {}).get("source", "Unknown")
                source_counts[source] = source_counts.get(source, 0) + 1
                
                # æ™‚é–“åˆ†å¸ƒï¼ˆ8æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆï¼‰
                scraped_at = article_score.get('article', {}).get("scraped_at")
                if scraped_at:
                    hour = scraped_at.hour
                    if hour < 8:
                        time_slot = "æ—©æœ"
                    elif hour < 16:
                        time_slot = "åˆå‰ãƒ»åˆå¾Œ"
                    else:
                        time_slot = "å¤œé–“"
                    time_distribution[time_slot] = time_distribution.get(time_slot, 0) + 1
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢è¨ˆç®—
            coverage_score = self._calculate_coverage_score(
                category_counts, region_counts, source_counts, time_distribution, len(articles)
            )
            
            # å“è³ªåŸºæº–è©•ä¾¡
            quality_issues = []
            
            # åœ°åŸŸã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
            if len(region_counts) < 3:
                quality_issues.append(f"åœ°åŸŸã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³ï¼ˆ{len(region_counts)}åœ°åŸŸã®ã¿ï¼‰")
                
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯
            if len(category_counts) < 3:
                quality_issues.append(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³ï¼ˆ{len(category_counts)}ã‚«ãƒ†ã‚´ãƒªã®ã¿ï¼‰")
                
            # ã‚½ãƒ¼ã‚¹ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
            max_source_ratio = max(source_counts.values()) / len(articles) if source_counts else 0
            if max_source_ratio > 0.7:
                quality_issues.append(f"ã‚½ãƒ¼ã‚¹åé‡ï¼ˆ{max_source_ratio:.1%}ãŒå˜ä¸€ã‚½ãƒ¼ã‚¹ï¼‰")
                
            # å°æœ¬å“è³ªãƒã‚§ãƒƒã‚¯
            script_char_count = script_result.get("char_count", 0)
            target_chars = int(self.config.get("target_duration", 10.0) * 280)
            char_deviation = abs(script_char_count - target_chars) / target_chars if target_chars > 0 else 1
            
            if char_deviation > 0.15:
                quality_issues.append(f"æ–‡å­—æ•°å¤§å¹…åå·®ï¼ˆ{char_deviation:.1%}ï¼‰")
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
            base_quality = script_result.get("quality_score", 0.5)
            coverage_weight = 0.3
            char_accuracy_weight = 0.2
            
            char_accuracy = 1.0 - min(char_deviation, 0.5) * 2
            total_quality = (
                base_quality * 0.5 + 
                coverage_score * coverage_weight + 
                char_accuracy * char_accuracy_weight
            )
            
            return {
                "coverage_score": coverage_score,
                "total_quality_score": total_quality,
                "category_distribution": category_counts,
                "region_distribution": region_counts,
                "source_distribution": source_counts,
                "time_distribution": time_distribution,
                "quality_issues": quality_issues,
                "meets_quality_threshold": total_quality >= 0.7 and coverage_score >= 0.6,
                "char_accuracy": char_accuracy,
                "char_deviation": char_deviation
            }
            
        except Exception as e:
            self.logger.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {
                "coverage_score": 0.0,
                "total_quality_score": 0.5,
                "quality_issues": [f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}"],
                "meets_quality_threshold": False
            }
            
    def _calculate_coverage_score(
        self, category_counts: dict, region_counts: dict, 
        source_counts: dict, time_distribution: dict, total_articles: int
    ) -> float:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰"""
        import math
        
        def entropy(counts):
            total = sum(counts.values())
            if total == 0:
                return 0
            return -sum((count/total) * math.log2(count/total) for count in counts.values() if count > 0)
            
        # å„æ¬¡å…ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        category_entropy = entropy(category_counts)
        region_entropy = entropy(region_counts)
        source_entropy = entropy(source_counts)
        time_entropy = entropy(time_distribution)
        
        # ç†è«–æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒãƒ©ãƒ³ã‚¹ç†æƒ³çŠ¶æ…‹ï¼‰
        max_possible_categories = min(total_articles, 5)
        max_possible_regions = min(total_articles, 4)
        max_possible_sources = min(total_articles, 3)
        max_possible_times = min(total_articles, 3)
        
        max_category_entropy = math.log2(max_possible_categories) if max_possible_categories > 1 else 0
        max_region_entropy = math.log2(max_possible_regions) if max_possible_regions > 1 else 0
        max_source_entropy = math.log2(max_possible_sources) if max_possible_sources > 1 else 0
        max_time_entropy = math.log2(max_possible_times) if max_possible_times > 1 else 0
        
        # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—
        def normalize_entropy(entropy, max_entropy):
            return entropy / max_entropy if max_entropy > 0 else 0
            
        category_score = normalize_entropy(category_entropy, max_category_entropy)
        region_score = normalize_entropy(region_entropy, max_region_entropy)
        source_score = normalize_entropy(source_entropy, max_source_entropy)
        time_score = normalize_entropy(time_entropy, max_time_entropy)
        
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆåœ°åŸŸã¨ã‚«ãƒ†ã‚´ãƒªã‚’é‡è¦–ï¼‰
        coverage_score = (
            category_score * 0.35 +
            region_score * 0.35 +
            source_score * 0.2 +
            time_score * 0.1
        )
        
        return min(coverage_score, 1.0)
