# -*- coding: utf-8 -*-

"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
"""

import time
import logging
import concurrent.futures
from typing import List, Dict, Any
from datetime import datetime, timedelta

from src.logging_config import get_logger, log_with_context
from src.config.app_config import get_config, AppConfig
from src.database.database_manager import DatabaseManager
from src.database.models import Article, AIAnalysis
from scrapers import reuters, bloomberg
from ai_summarizer import process_article_with_ai
from src.html.html_generator import HTMLGenerator
from gdocs.client import authenticate_google_services, test_drive_connection, update_google_doc_with_full_text, create_daily_summary_doc_with_cleanup_retry, debug_drive_storage_info, cleanup_old_drive_documents


class NewsProcessor:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.config: AppConfig = get_config()
        self.db_manager = DatabaseManager(self.config.database)
        self.html_generator = HTMLGenerator(self.logger)
        
        # å‹•çš„è¨˜äº‹å–å¾—æ©Ÿèƒ½ã§ä½¿ç”¨ã™ã‚‹å±æ€§
        self.folder_id = self.config.google.drive_output_folder_id
        self.gemini_api_key = self.config.ai.gemini_api_key

    def validate_environment(self) -> bool:
        """ç’°å¢ƒå¤‰æ•°ã®æ¤œè¨¼"""
        self.logger.info("=== ç’°å¢ƒå¤‰æ•°è¨­å®šçŠ¶æ³ ===")
        self.logger.info(f"GOOGLE_DRIVE_OUTPUT_FOLDER_ID: {'è¨­å®šæ¸ˆã¿' if self.folder_id else 'æœªè¨­å®š'}")
        self.logger.info(f"GOOGLE_OVERWRITE_DOC_ID: {'è¨­å®šæ¸ˆã¿' if self.config.google.overwrite_doc_id else 'æœªè¨­å®š'}")
        self.logger.info(f"GEMINI_API_KEY: {'è¨­å®šæ¸ˆã¿' if self.gemini_api_key else 'æœªè¨­å®š'}")
        self.logger.info(f"GOOGLE_SERVICE_ACCOUNT_JSON: {'è¨­å®šæ¸ˆã¿' if self.config.google.service_account_json else 'æœªè¨­å®š'}")
        
        # å‹•çš„è¨˜äº‹å–å¾—è¨­å®šã®è¡¨ç¤º
        self.logger.info("=== å‹•çš„è¨˜äº‹å–å¾—è¨­å®š ===")
        self.logger.info(f"åŸºæœ¬æ™‚é–“ç¯„å›²: {self.config.scraping.hours_limit}æ™‚é–“")
        self.logger.info(f"æœ€ä½è¨˜äº‹æ•°é–¾å€¤: {self.config.scraping.minimum_article_count}ä»¶")
        self.logger.info(f"æœ€å¤§æ™‚é–“ç¯„å›²: {self.config.scraping.max_hours_limit}æ™‚é–“")
        self.logger.info(f"é€±æœ«æ‹¡å¼µæ™‚é–“: {self.config.scraping.weekend_hours_extension}æ™‚é–“")
        
        if not self.gemini_api_key:
            self.logger.error("å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆGEMINI_API_KEYï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        return True
    
    def get_dynamic_hours_limit(self) -> int:
        """
        æ›œæ—¥ã¨è¨˜äº‹æ•°ã«åŸºã¥ãå‹•çš„æ™‚é–“ç¯„å›²æ±ºå®š
        
        Returns:
            int: å‹•çš„ã«æ±ºå®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ï¼ˆæ™‚é–“ï¼‰
        """
        from datetime import datetime
        import pytz
        
        jst_tz = pytz.timezone('Asia/Tokyo')
        jst_now = datetime.now(jst_tz)
        weekday = jst_now.weekday()  # æœˆæ›œæ—¥=0, æ—¥æ›œæ—¥=6
        
        # æœˆæ›œæ—¥ãƒ»åœŸæ›œæ—¥ãƒ»æ—¥æ›œæ—¥ã¯è‡ªå‹•çš„ã«æœ€å¤§æ™‚é–“ç¯„å›²ã‚’é©ç”¨ï¼ˆé€±æœ«ã‚„ä¼‘æ—¥æ˜ã‘ã¯è¨˜äº‹ãŒå°‘ãªã„ãŸã‚ï¼‰
        if weekday == 0:  # Monday
            self.logger.info(f"æœˆæ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨")
            return self.config.scraping.max_hours_limit
        elif weekday == 5:  # Saturday
            self.logger.info(f"åœŸæ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨")
            return self.config.scraping.max_hours_limit
        elif weekday == 6:  # Sunday
            self.logger.info(f"æ—¥æ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨")
            return self.config.scraping.max_hours_limit
        
        # å¹³æ—¥ï¼ˆç«-é‡‘ï¼‰ã¯åŸºæœ¬æ™‚é–“ç¯„å›²ã‹ã‚‰é–‹å§‹
        return self.config.scraping.hours_limit
    
    def collect_articles_with_dynamic_range(self) -> List[Dict[str, Any]]:
        """
        å‹•çš„æ™‚é–“ç¯„å›²ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹åé›†
        è¨˜äº‹æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æ®µéšçš„ã«æ™‚é–“ç¯„å›²ã‚’æ‹¡å¼µ
        """
        import datetime
        import pytz
        
        # å®Ÿè¡Œæ™‚åˆ»ã®è©³ç´°ãƒ­ã‚°
        jst_tz = pytz.timezone('Asia/Tokyo')
        jst_now = datetime.datetime.now(jst_tz)
        weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        
        self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—é–‹å§‹ ===")
        self.logger.info(f"å®Ÿè¡Œæ—¥æ™‚: {jst_now.strftime('%Y/%m/%d %H:%M:%S')} ({weekday_names[jst_now.weekday()]}æ›œæ—¥)")
        
        initial_hours = self.get_dynamic_hours_limit()
        current_hours = initial_hours
        
        self.logger.info(f"åˆæœŸæ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“ (è¨­å®š - åŸºæœ¬: {self.config.scraping.hours_limit}h, æœ€å¤§: {self.config.scraping.max_hours_limit}h)")
        self.logger.info(f"æœ€ä½è¨˜äº‹æ•°é–¾å€¤: {self.config.scraping.minimum_article_count}ä»¶")
        
        attempts = 0
        while current_hours <= self.config.scraping.max_hours_limit:
            attempts += 1
            self.logger.info(f"--- å–å¾—è©¦è¡Œ {attempts}å›ç›® (æ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“) ---")
            
            articles = self._collect_articles_with_hours(current_hours)
            article_count = len(articles)
            
            # è¨˜äº‹ã®è©³ç´°åˆ†æ
            source_breakdown = {}
            for article in articles:
                source = article.get('source', 'Unknown')
                source_breakdown[source] = source_breakdown.get(source, 0) + 1
            
            self.logger.info(f"å–å¾—çµæœ: ç·è¨˜äº‹æ•° {article_count}ä»¶")
            for source, count in source_breakdown.items():
                self.logger.info(f"  - {source}: {count}ä»¶")
            
            # æœ€ä½è¨˜äº‹æ•°ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if article_count >= self.config.scraping.minimum_article_count:
                self.logger.info(f"âœ… æˆåŠŸ: æœ€ä½è¨˜äº‹æ•°({self.config.scraping.minimum_article_count}ä»¶)ã‚’æº€ãŸã—ã¾ã—ãŸ")
                self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (è©¦è¡Œå›æ•°: {attempts}å›, æœ€çµ‚æ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“) ===")
                return articles
            elif current_hours >= self.config.scraping.max_hours_limit:
                self.logger.warning(f"âš ï¸  æœ€å¤§æ™‚é–“ç¯„å›²({self.config.scraping.max_hours_limit}æ™‚é–“)ã«åˆ°é”")
                self.logger.warning(f"   æœ€çµ‚è¨˜äº‹æ•°: {article_count}ä»¶ (ç›®æ¨™: {self.config.scraping.minimum_article_count}ä»¶)")
                self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (è¨˜äº‹ä¸è¶³, è©¦è¡Œå›æ•°: {attempts}å›) ===")
                return articles
            else:
                # æ™‚é–“ç¯„å›²ã‚’æ®µéšçš„ã«æ‹¡å¼µ
                next_hours = min(current_hours + 24, self.config.scraping.max_hours_limit)
                self.logger.info(f"ğŸ“ˆ è¨˜äº‹æ•°ä¸è¶³ â†’ æ™‚é–“ç¯„å›²æ‹¡å¼µ: {current_hours}æ™‚é–“ â†’ {next_hours}æ™‚é–“")
                current_hours = next_hours
        
        self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (ãƒ«ãƒ¼ãƒ—çµ‚äº†) ===")
        return articles

    def _collect_articles_with_hours(self, hours_limit: int) -> List[Dict[str, Any]]:
        """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ã§è¨˜äº‹ã‚’åé›†ï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
        all_articles = []
        
        # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # Reutersã«ã¯ hours_limit ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
            reuters_params = self.config.reuters.to_dict()
            reuters_params['hours_limit'] = hours_limit
            
            # Bloombergç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            bloomberg_params = self.config.bloomberg.to_dict()
            bloomberg_params['hours_limit'] = hours_limit
            
            future_to_scraper = {
                executor.submit(reuters.scrape_reuters_articles, **reuters_params): "Reuters",
                executor.submit(bloomberg.scrape_bloomberg_top_page_articles, **bloomberg_params): "Bloomberg"
            }
            
            for future in concurrent.futures.as_completed(future_to_scraper):
                scraper_name = future_to_scraper[future]
                try:
                    articles = future.result()
                    log_with_context(self.logger, logging.INFO, f"{scraper_name} è¨˜äº‹å–å¾—å®Œäº†", 
                                     operation="collect_articles", scraper=scraper_name, count=len(articles))
                    all_articles.extend(articles)
                except Exception as e:
                    log_with_context(self.logger, logging.ERROR, f"{scraper_name} è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼",
                                     operation="collect_articles", scraper=scraper_name, error=str(e), exc_info=True)
        
        # å…¬é–‹æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
        sorted_articles = sorted(
            all_articles,
            key=lambda x: x.get('published_jst', datetime.min),
            reverse=True
        )
        
        log_with_context(self.logger, logging.INFO, "è¨˜äº‹åé›†å®Œäº†", operation="collect_articles", total_count=len(sorted_articles))
        return sorted_articles
    
    def collect_articles(self) -> List[Dict[str, Any]]:
        """è¨˜äº‹ã®åé›†ï¼ˆå‹•çš„æ™‚é–“ç¯„å›²å¯¾å¿œï¼‰"""
        # æ–°ã—ã„å‹•çš„åé›†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return self.collect_articles_with_dynamic_range()

    def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> List[int]:
        """åé›†ã—ãŸè¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆé‡è¤‡ã¯è‡ªå‹•ã§æ’é™¤ï¼‰"""
        log_with_context(self.logger, logging.INFO, "è¨˜äº‹ã®DBä¿å­˜é–‹å§‹", operation="save_articles_to_db", count=len(articles))
        new_article_ids = []
        
        for article_data in articles:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã€æ–°è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            article_id, is_new = self.db_manager.save_article(article_data)
            if article_id and is_new:
                new_article_ids.append(article_id)

        log_with_context(self.logger, logging.INFO, "è¨˜äº‹ã®DBä¿å­˜å®Œäº†", operation="save_articles_to_db", 
                         new_articles=len(new_article_ids), total_attempted=len(articles))
        return new_article_ids

    def process_new_articles_with_ai(self, new_article_ids: List[int]):
        """æ–°è¦è¨˜äº‹ã®ã¿ã‚’AIã§å‡¦ç†"""
        if not new_article_ids:
            log_with_context(self.logger, logging.INFO, "AIå‡¦ç†å¯¾è±¡ã®æ–°è¦è¨˜äº‹ãªã—", operation="process_new_articles")
            return

        log_with_context(self.logger, logging.INFO, f"AIå‡¦ç†é–‹å§‹ï¼ˆæ–°è¦{len(new_article_ids)}ä»¶ï¼‰", operation="process_new_articles")
        
        articles_to_process = self.db_manager.get_articles_by_ids(new_article_ids)

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_article = {
                executor.submit(process_article_with_ai, self.config.ai.gemini_api_key, article.body): article
                for article in articles_to_process if article.body
            }

            for future in concurrent.futures.as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    ai_result = future.result()
                    if ai_result:
                        self.db_manager.save_ai_analysis(article.id, ai_result)
                        log_with_context(self.logger, logging.DEBUG, "AIåˆ†æçµæœã‚’ä¿å­˜", article_id=article.id)
                except Exception as e:
                    log_with_context(self.logger, logging.ERROR, f"è¨˜äº‹ID {article.id} ã®AIå‡¦ç†ã‚¨ãƒ©ãƒ¼",
                                     operation="process_new_articles", article_id=article.id, error=str(e), exc_info=True)

        log_with_context(self.logger, logging.INFO, "AIå‡¦ç†å®Œäº†", operation="process_new_articles")

    def process_recent_articles_without_ai(self):
        """AIåˆ†æãŒãªã„24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã‚’å‡¦ç†"""
        # AIåˆ†æãŒãªã„è¨˜äº‹ã®IDã‚’å–å¾—
        with self.db_manager.get_session() as session:
            cutoff_time = datetime.utcnow() - timedelta(hours=self.config.scraping.hours_limit)
            unprocessed_ids = session.query(Article.id).outerjoin(AIAnalysis).filter(
                Article.published_at >= cutoff_time,
                AIAnalysis.id.is_(None),
                Article.body.isnot(None),
                Article.body != ''
            ).all()
            unprocessed_ids = [row[0] for row in unprocessed_ids]
        
        if not unprocessed_ids:
            log_with_context(self.logger, logging.INFO, "AIå‡¦ç†å¯¾è±¡ã®æœªå‡¦ç†è¨˜äº‹ãªã—", operation="process_recent_articles")
            return
        
        log_with_context(self.logger, logging.INFO, f"æœªå‡¦ç†è¨˜äº‹ã®AIå‡¦ç†é–‹å§‹ï¼ˆ{len(unprocessed_ids)}ä»¶ï¼‰", operation="process_recent_articles")
        
        # è¨˜äº‹ã‚’å–å¾—ã—ã¦AIå‡¦ç†
        articles_to_process = self.db_manager.get_articles_by_ids(unprocessed_ids)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_article = {
                executor.submit(process_article_with_ai, self.config.ai.gemini_api_key, article.body): article
                for article in articles_to_process if article.body
            }

            for future in concurrent.futures.as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    ai_result = future.result()
                    if ai_result:
                        self.db_manager.save_ai_analysis(article.id, ai_result)
                        log_with_context(self.logger, logging.DEBUG, "AIåˆ†æçµæœã‚’ä¿å­˜", article_id=article.id)
                except Exception as e:
                    log_with_context(self.logger, logging.ERROR, f"è¨˜äº‹ID {article.id} ã®AIå‡¦ç†ã‚¨ãƒ©ãƒ¼",
                                     operation="process_recent_articles", article_id=article.id, error=str(e), exc_info=True)

        log_with_context(self.logger, logging.INFO, "æœªå‡¦ç†è¨˜äº‹ã®AIå‡¦ç†å®Œäº†", operation="process_recent_articles")

    def prepare_current_session_articles_for_html(self, scraped_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã‚’HTMLè¡¨ç¤ºç”¨ã«æº–å‚™ï¼ˆAIåˆ†æçµæœã¨çµ„ã¿åˆã‚ã›ï¼‰
        æ³¨ï¼šä»Šå›ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€éå»ã®è¨˜äº‹ã®æ··å…¥ã‚’é˜²ã
        """
        log_with_context(self.logger, logging.INFO,
                         f"HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ (åˆæœŸè¨˜äº‹æ•°: {len(scraped_articles)}ä»¶)",
                         operation="prepare_current_session_articles")

        final_articles = []
        processed_urls = set()
        ai_analysis_found = 0
        duplicates_skipped = 0

        for i, scraped_article in enumerate(scraped_articles):
            url = scraped_article.get("url")
            if not url:
                log_with_context(self.logger, logging.WARNING, f"è¨˜äº‹ {i} ã«URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", operation="prepare_html_data")
                continue

            # URLã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒã®ç²¾åº¦ã‚’ä¸Šã’ã‚‹
            normalized_url = self.db_manager.url_normalizer.normalize_url(url)

            if normalized_url in processed_urls:
                duplicates_skipped += 1
                log_with_context(self.logger, logging.DEBUG,
                                 f"é‡è¤‡URLã‚’ã‚¹ã‚­ãƒƒãƒ—: å…ƒURL='{url}', æ­£è¦åŒ–URL='{normalized_url}'",
                                 operation="prepare_html_data")
                continue

            processed_urls.add(normalized_url)
            log_with_context(self.logger, logging.DEBUG, f"æ–°è¦URLã‚’å‡¦ç†: å…ƒURL='{url}', æ­£è¦åŒ–URL='{normalized_url}'", operation="prepare_html_data")

            article_data = {
                "title": scraped_article.get("title", ""),
                "url": url, # è¡¨ç¤ºã«ã¯å…ƒã®URLã‚’ä½¿ç”¨
                "source": scraped_article.get("source", ""),
                "published_jst": scraped_article.get("published_jst", ""),
                "summary": "è¦ç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "sentiment_label": "N/A",
                "sentiment_score": 0.0,
            }

            try:
                # DBã‹ã‚‰ã¯æ­£è¦åŒ–æ¸ˆã¿URLã§å•ã„åˆã‚ã›ã‚‹ã®ãŒç¢ºå®Ÿ
                article_with_analysis = self.db_manager.get_article_by_url_with_analysis(normalized_url)
                if article_with_analysis and article_with_analysis.ai_analysis:
                    analysis = article_with_analysis.ai_analysis[0]
                    if analysis.summary:
                        article_data.update({
                            "summary": analysis.summary,
                            "sentiment_label": analysis.sentiment_label if analysis.sentiment_label else "N/A",
                            "sentiment_score": analysis.sentiment_score if analysis.sentiment_score is not None else 0.0,
                        })
                        ai_analysis_found += 1
            except Exception as e:
                log_with_context(self.logger, logging.WARNING,
                                 f"AIåˆ†æçµæœã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {url} - {e}",
                                 operation="prepare_html_data")

            final_articles.append(article_data)

        log_with_context(self.logger, logging.INFO,
                         f"HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† (é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {duplicates_skipped}ä»¶, æœ€çµ‚è¨˜äº‹æ•°: {len(final_articles)}ä»¶, AIåˆ†æã‚ã‚Š: {ai_analysis_found}ä»¶)",
                         operation="prepare_current_session_articles")
        return final_articles

    def generate_final_html(self, current_session_articles: List[Dict[str, Any]] = None):
        """æœ€çµ‚çš„ãªHTMLã‚’ç”Ÿæˆï¼ˆä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã®ã¿ï¼‰"""
        log_with_context(self.logger, logging.INFO, "æœ€çµ‚HTMLç”Ÿæˆé–‹å§‹", operation="generate_final_html")
        
        # ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ãŒæ¸¡ã•ã‚Œãªã„å ´åˆã¯ç©ºã®HTMLã‚’ç”Ÿæˆ
        if not current_session_articles:
            log_with_context(self.logger, logging.WARNING, "HTMLç”Ÿæˆå¯¾è±¡ã®è¨˜äº‹ãªã—", operation="generate_final_html")
            self.html_generator.generate_html_file([], "index.html")
            return

        # ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã‚’HTMLã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ç”¨ã«å¤‰æ›
        articles_for_html = []
        for article_data in current_session_articles:
            articles_for_html.append({
                "title": article_data.get("title", ""),
                "url": article_data.get("url", ""),
                "summary": article_data.get("summary", "è¦ç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"),
                "source": article_data.get("source", ""),
                "published_jst": article_data.get("published_jst", ""),
                "sentiment_label": article_data.get("sentiment_label", "N/A"),
                "sentiment_score": article_data.get("sentiment_score", 0.0),
            })
        
        # è¨˜äº‹ã‚’å…¬é–‹æ™‚åˆ»é †ï¼ˆæœ€æ–°é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        articles_for_html = self._sort_articles_by_time(articles_for_html)
        
        self.html_generator.generate_html_file(articles_for_html, "index.html")
        log_with_context(self.logger, logging.INFO, "æœ€çµ‚HTMLç”Ÿæˆå®Œäº†", operation="generate_final_html", count=len(articles_for_html))

    def _sort_articles_by_time(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¨˜äº‹ã‚’å…¬é–‹æ™‚åˆ»é †ï¼ˆæœ€æ–°é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        
        Args:
            articles: ã‚½ãƒ¼ãƒˆå¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        def get_sort_key(article):
            published_jst = article.get('published_jst')
            if not published_jst:
                return datetime.min  # æ—¥æ™‚ãŒä¸æ˜ãªè¨˜äº‹ã¯æœ€å¾Œå°¾ã«
            
            # datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if hasattr(published_jst, 'year'):
                return published_jst
            
            # æ–‡å­—åˆ—ã®å ´åˆã¯å¤‰æ›ã‚’è©¦è¡Œ
            if isinstance(published_jst, str):
                try:
                    # ISOå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹
                    return datetime.fromisoformat(published_jst.replace('Z', '+00:00'))
                except:
                    return datetime.min
            
            return datetime.min
        
        try:
            sorted_articles = sorted(articles, key=get_sort_key, reverse=True)
            log_with_context(self.logger, logging.INFO, 
                           f"è¨˜äº‹ã‚’æ™‚åˆ»é †ã§ã‚½ãƒ¼ãƒˆå®Œäº† (æœ€æ–°: {get_sort_key(sorted_articles[0]) if sorted_articles else 'N/A'})", 
                           operation="sort_articles")
            return sorted_articles
        except Exception as e:
            log_with_context(self.logger, logging.WARNING, 
                           f"è¨˜äº‹ã‚½ãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e} - å…ƒã®é †åºã‚’ç¶­æŒ", 
                           operation="sort_articles")
            return articles

    def generate_google_docs(self):
        """Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†"""
        log_with_context(self.logger, logging.INFO, "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆé–‹å§‹", operation="generate_google_docs")
        
        # Googleèªè¨¼
        drive_service, docs_service = authenticate_google_services()
        if not drive_service or not docs_service:
            log_with_context(self.logger, logging.ERROR, "Googleèªè¨¼ã«å¤±æ•—", operation="generate_google_docs")
            return

        # Driveå®¹é‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        debug_drive_storage_info(drive_service)
        
        # å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            deleted_count = cleanup_old_drive_documents(
                drive_service,
                self.config.google.drive_output_folder_id,
                self.config.google.docs_retention_days
            )
            log_with_context(self.logger, logging.INFO, 
                            f"å¤ã„Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†", 
                            operation="generate_google_docs", deleted_count=deleted_count)
        except Exception as e:
            log_with_context(self.logger, logging.WARNING, 
                            f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}", 
                            operation="generate_google_docs")
        
        # æ¨©é™ç¢ºèª
        if not test_drive_connection(drive_service, self.config.google.drive_output_folder_id):
            log_with_context(self.logger, logging.ERROR, "Google Driveæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—", operation="generate_google_docs")
            return

        # DBã‹ã‚‰éå»24æ™‚é–“åˆ†ã®å…¨è¨˜äº‹ã‚’å–å¾—
        recent_articles = self.db_manager.get_recent_articles_all(hours=self.config.scraping.hours_limit)
        
        if not recent_articles:
            log_with_context(self.logger, logging.WARNING, "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå¯¾è±¡ã®è¨˜äº‹ãªã—", operation="generate_google_docs")
            return

        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        articles_for_docs = []
        articles_with_summary = []
        
        for a in recent_articles:
            analysis = a.ai_analysis[0] if a.ai_analysis else None
            
            # å…¨è¨˜äº‹ç”¨ï¼ˆè¨˜äº‹æœ¬æ–‡å«ã‚€ï¼‰
            article_data = {
                "title": a.title,
                "url": a.url,
                "source": a.source,
                "published_jst": a.published_at,
                "body": a.body,
                "summary": analysis.summary if analysis else None,
                "sentiment_label": analysis.sentiment_label if analysis else "N/A",
                "sentiment_score": analysis.sentiment_score if analysis else 0.0,
            }
            articles_for_docs.append(article_data)
            
            # AIè¦ç´„ãŒã‚ã‚‹è¨˜äº‹ã®ã¿
            if analysis and analysis.summary:
                articles_with_summary.append(article_data)

        # 1. æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å…¨å‰Šé™¤ãƒ»æ–°è¦è¨˜è¼‰ï¼ˆå…¨è¨˜äº‹æœ¬æ–‡ï¼‰
        if self.config.google.overwrite_doc_id:
            success = update_google_doc_with_full_text(
                docs_service, 
                self.config.google.overwrite_doc_id, 
                articles_for_docs
            )
            if success:
                log_with_context(self.logger, logging.INFO, "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå®Œäº†", 
                                operation="generate_google_docs", doc_id=self.config.google.overwrite_doc_id)
            else:
                log_with_context(self.logger, logging.ERROR, "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå¤±æ•—", 
                                operation="generate_google_docs", doc_id=self.config.google.overwrite_doc_id)

        # 2. AIè¦ç´„ã®æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆå®¹é‡ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ»ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        success = create_daily_summary_doc_with_cleanup_retry(
            drive_service,
            docs_service,
            articles_with_summary,
            self.config.google.drive_output_folder_id,
            self.config.google.docs_retention_days
        )
        if success:
            log_with_context(self.logger, logging.INFO, "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå®Œäº†", 
                            operation="generate_google_docs", ai_articles=len(articles_with_summary))
        else:
            log_with_context(self.logger, logging.ERROR, "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—", 
                            operation="generate_google_docs")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ"""
        self.logger.info("=== ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹å–å¾—ãƒ»å‡¦ç†é–‹å§‹ ===")
        overall_start_time = time.time()
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
        session_id = self.db_manager.start_scraping_session()
        
        try:
            if not self.validate_environment():
                self.db_manager.complete_scraping_session(session_id, status='failed', error_details="ç’°å¢ƒå¤‰æ•°æœªè¨­å®š")
                return

            # 1. è¨˜äº‹åé›†
            scraped_articles = self.collect_articles()
            log_with_context(self.logger, logging.INFO, 
                            f"=== è¨˜äº‹åé›†çµæœ: {len(scraped_articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾— ===", 
                            operation="main_process")
            
            self.db_manager.update_scraping_session(session_id, articles_found=len(scraped_articles))
            if not scraped_articles:
                self.logger.warning("ä»Šå›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§æ–°ã—ã„è¨˜äº‹ãŒå–å¾—ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: éå»24æ™‚é–“åˆ†ã®è¨˜äº‹ã‚’DBã‹ã‚‰å–å¾—ã—ã¦HTMLç”Ÿæˆ
                self.logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†: DBã‹ã‚‰éå»24æ™‚é–“åˆ†ã®è¨˜äº‹ã‚’å–å¾—")
                recent_articles_from_db = self.db_manager.get_recent_articles_all(hours=24)
                
                if recent_articles_from_db:
                    # DBè¨˜äº‹ã‚’HTMLç”¨å½¢å¼ã«å¤‰æ›
                    fallback_articles = []
                    for db_article in recent_articles_from_db:
                        analysis = db_article.ai_analysis[0] if db_article.ai_analysis else None
                        fallback_articles.append({
                            'title': db_article.title,
                            'url': db_article.url,
                            'summary': analysis.summary if analysis else 'è¦ç´„ãªã—',
                            'source': db_article.source,
                            'published_jst': db_article.published_at,
                            'sentiment_label': analysis.sentiment_label if analysis else 'N/A',
                            'sentiment_score': analysis.sentiment_score if analysis else 0.0
                        })
                    
                    self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {len(fallback_articles)}ä»¶ã®éå»è¨˜äº‹ã§HTMLç”Ÿæˆ")
                    self.db_manager.complete_scraping_session(session_id, status='completed_with_fallback')
                    self.generate_final_html(fallback_articles)
                else:
                    self.logger.warning("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨˜äº‹ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    self.db_manager.complete_scraping_session(session_id, status='completed_no_articles')
                    self.generate_final_html([])  # ç©ºãƒªã‚¹ãƒˆã‚’æ¸¡ã™
                return

            # 2. DBã«ä¿å­˜ (é‡è¤‡æ’é™¤)
            new_article_ids = self.save_articles_to_db(scraped_articles)
            log_with_context(self.logger, logging.INFO, 
                            f"=== DBä¿å­˜çµæœ: {len(scraped_articles)}ä»¶ä¸­{len(new_article_ids)}ä»¶ãŒæ–°è¦è¨˜äº‹ ===", 
                            operation="main_process")
            
            self.db_manager.update_scraping_session(session_id, articles_processed=len(new_article_ids))

            # 3. æ–°è¦è¨˜äº‹ã‚’AIã§å‡¦ç†
            self.process_new_articles_with_ai(new_article_ids)
            
            # 3.5. AIåˆ†æãŒãªã„24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã‚‚å‡¦ç†ã™ã‚‹
            self.process_recent_articles_without_ai()

            # 4. ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’AIåˆ†æçµæœã¨çµ„ã¿åˆã‚ã›ã¦æº–å‚™
            current_session_articles = self.prepare_current_session_articles_for_html(scraped_articles)
            log_with_context(self.logger, logging.INFO, 
                            f"=== HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(current_session_articles)}ä»¶ã®è¨˜äº‹ã‚’HTMLã«å‡ºåŠ›äºˆå®š ===", 
                            operation="main_process")

            # 5. æœ€çµ‚çš„ãªHTMLã‚’ç”Ÿæˆï¼ˆä»Šå›å®Ÿè¡Œåˆ†ã®ã¿ï¼‰
            self.generate_final_html(current_session_articles)
            
            # 6. Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆï¼ˆæ™‚åˆ»æ¡ä»¶æº€ãŸã™å ´åˆã®ã¿ï¼‰
            self.generate_google_docs()
            
            # 7. å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.db_manager.cleanup_old_data(days_to_keep=30)

            self.db_manager.complete_scraping_session(session_id, status='completed_ok')
            
        except Exception as e:
            self.logger.error(f"å‡¦ç†å…¨ä½“ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.db_manager.complete_scraping_session(session_id, status='failed', error_details=str(e))
            raise
        finally:
            overall_elapsed_time = time.time() - overall_start_time
            self.logger.info(f"=== å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ (ç·å‡¦ç†æ™‚é–“: {overall_elapsed_time:.2f}ç§’) ===")