# -*- coding: utf-8 -*-

"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
"""

import time
import logging
import concurrent.futures
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import pytz

from src.logging_config import get_logger, log_with_context
from src.config.app_config import get_config, AppConfig
from src.database.database_manager import DatabaseManager
from src.database.models import Article, AIAnalysis
from scrapers import reuters, bloomberg
from src.legacy.ai_summarizer import process_article_with_ai
from scripts.legacy.ai_pro_summarizer import create_integrated_summaries, ProSummaryConfig
from src.legacy.article_grouper import group_articles_for_pro_summary
from tools.performance.cost_manager import check_pro_cost_limits, CostManager
from src.html.html_generator import HTMLGenerator
from gdocs.client import (
    authenticate_google_services,
    test_drive_connection,
    update_google_doc_with_full_text,
    create_daily_summary_doc_with_cleanup_retry,
    debug_drive_storage_info,
    cleanup_old_drive_documents,
    create_debug_spreadsheet,
    update_debug_spreadsheet,
    get_spreadsheet_url,
)


class NewsProcessor:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.config: AppConfig = get_config()
        self.db_manager = DatabaseManager(self.config.database)
        self.html_generator = HTMLGenerator(self.logger)

        # å‹•çš„è¨˜äº‹å–å¾—æ©Ÿèƒ½ã§ä½¿ç”¨ã™ã‚‹å±æ€§
        self.folder_id = self.config.google.drive_output_folder_id
        self.gemini_api_key = self.config.ai.gemini_api_key

        # Proçµ±åˆè¦ç´„é–¢é€£ã®åˆæœŸåŒ–
        self.cost_manager = CostManager()
        self.pro_config = ProSummaryConfig(
            enabled=True,
            min_articles_threshold=10,
            max_daily_executions=3,
            cost_limit_monthly=50.0,
            timeout_seconds=180,
        )

    def validate_environment(self) -> bool:
        """ç’°å¢ƒå¤‰æ•°ã®æ¤œè¨¼"""
        self.logger.info("=== ç’°å¢ƒå¤‰æ•°è¨­å®šçŠ¶æ³ ===")
        self.logger.info(
            f"GOOGLE_DRIVE_OUTPUT_FOLDER_ID: {'è¨­å®šæ¸ˆã¿' if self.folder_id else 'æœªè¨­å®š'}"
        )
        self.logger.info(
            f"GOOGLE_OVERWRITE_DOC_ID: {'è¨­å®šæ¸ˆã¿' if self.config.google.overwrite_doc_id else 'æœªè¨­å®š'}"
        )
        self.logger.info(f"GEMINI_API_KEY: {'è¨­å®šæ¸ˆã¿' if self.gemini_api_key else 'æœªè¨­å®š'}")
        self.logger.info(
            f"GOOGLE_SERVICE_ACCOUNT_JSON: {'è¨­å®šæ¸ˆã¿' if self.config.google.service_account_json else 'æœªè¨­å®š'}"
        )

        # å‹•çš„è¨˜äº‹å–å¾—è¨­å®šã®è¡¨ç¤º
        self.logger.info("=== å‹•çš„è¨˜äº‹å–å¾—è¨­å®š ===")
        self.logger.info(f"åŸºæœ¬æ™‚é–“ç¯„å›²: {self.config.scraping.hours_limit}æ™‚é–“")
        self.logger.info(f"æœ€ä½è¨˜äº‹æ•°é–¾å€¤: {self.config.scraping.minimum_article_count}ä»¶")
        self.logger.info(f"æœ€å¤§æ™‚é–“ç¯„å›²: {self.config.scraping.max_hours_limit}æ™‚é–“")
        self.logger.info(f"é€±æœ«æ‹¡å¼µæ™‚é–“: {self.config.scraping.weekend_hours_extension}æ™‚é–“")

        if not self.gemini_api_key:
            self.logger.error("å¿…è¦ãªç’°å¢ƒå¤‰æ•°ï¼ˆGEMINI_API_KEYï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        return True

    def get_dynamic_hours_limit(self) -> int:
        """
        æ›œæ—¥ã¨è¨˜äº‹æ•°ã«åŸºã¥ãå‹•çš„æ™‚é–“ç¯„å›²æ±ºå®š

        Returns:
            int: å‹•çš„ã«æ±ºå®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ï¼ˆæ™‚é–“ï¼‰
        """
        jst_tz = pytz.timezone("Asia/Tokyo")
        jst_now = datetime.now(jst_tz)
        weekday = jst_now.weekday()  # æœˆæ›œæ—¥=0, æ—¥æ›œæ—¥=6

        # æœˆæ›œæ—¥ãƒ»åœŸæ›œæ—¥ãƒ»æ—¥æ›œæ—¥ã¯è‡ªå‹•çš„ã«æœ€å¤§æ™‚é–“ç¯„å›²ã‚’é©ç”¨ï¼ˆé€±æœ«ã‚„ä¼‘æ—¥æ˜ã‘ã¯è¨˜äº‹ãŒå°‘ãªã„ãŸã‚ï¼‰
        if weekday == 0:  # Monday
            self.logger.info(
                f"æœˆæ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨"
            )
            return self.config.scraping.max_hours_limit
        elif weekday == 5:  # Saturday
            self.logger.info(
                f"åœŸæ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨"
            )
            return self.config.scraping.max_hours_limit
        elif weekday == 6:  # Sunday
            self.logger.info(
                f"æ—¥æ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«{self.config.scraping.max_hours_limit}æ™‚é–“ç¯„å›²ã‚’é©ç”¨"
            )
            return self.config.scraping.max_hours_limit

        # å¹³æ—¥ï¼ˆç«-é‡‘ï¼‰ã¯åŸºæœ¬æ™‚é–“ç¯„å›²ã‹ã‚‰é–‹å§‹
        return self.config.scraping.hours_limit

    def collect_articles_with_dynamic_range(self) -> List[Dict[str, Any]]:
        """
        å‹•çš„æ™‚é–“ç¯„å›²ã‚’ä½¿ç”¨ã—ãŸè¨˜äº‹åé›†
        è¨˜äº‹æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æ®µéšçš„ã«æ™‚é–“ç¯„å›²ã‚’æ‹¡å¼µ
        """
        import datetime
        import pytz

        # å®Ÿè¡Œæ™‚åˆ»ã®è©³ç´°ãƒ­ã‚°
        jst_tz = pytz.timezone("Asia/Tokyo")
        jst_now = datetime.datetime.now(jst_tz)
        weekday_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]

        self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—é–‹å§‹ ===")
        self.logger.info(
            f"å®Ÿè¡Œæ—¥æ™‚: {jst_now.strftime('%Y/%m/%d %H:%M:%S')} ({weekday_names[jst_now.weekday()]}æ›œæ—¥)"
        )

        initial_hours = self.get_dynamic_hours_limit()
        current_hours = initial_hours

        self.logger.info(
            f"åˆæœŸæ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“ (è¨­å®š - åŸºæœ¬: {self.config.scraping.hours_limit}h, æœ€å¤§: {self.config.scraping.max_hours_limit}h)"
        )
        self.logger.info(f"æœ€ä½è¨˜äº‹æ•°é–¾å€¤: {self.config.scraping.minimum_article_count}ä»¶")

        attempts = 0
        while current_hours <= self.config.scraping.max_hours_limit:
            attempts += 1
            self.logger.info(f"--- å–å¾—è©¦è¡Œ {attempts}å›ç›® (æ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“) ---")

            articles = self._collect_articles_with_hours(current_hours)
            article_count = len(articles)

            # è¨˜äº‹ã®è©³ç´°åˆ†æ
            source_breakdown = {}
            for article in articles:
                source = article.get("source", "Unknown")
                source_breakdown[source] = source_breakdown.get(source, 0) + 1

            self.logger.info(f"å–å¾—çµæœ: ç·è¨˜äº‹æ•° {article_count}ä»¶")
            for source, count in source_breakdown.items():
                self.logger.info(f"  - {source}: {count}ä»¶")

            # æœ€ä½è¨˜äº‹æ•°ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if article_count >= self.config.scraping.minimum_article_count:
                self.logger.info(
                    f"âœ… æˆåŠŸ: æœ€ä½è¨˜äº‹æ•°({self.config.scraping.minimum_article_count}ä»¶)ã‚’æº€ãŸã—ã¾ã—ãŸ"
                )
                self.logger.info(
                    f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (è©¦è¡Œå›æ•°: {attempts}å›, æœ€çµ‚æ™‚é–“ç¯„å›²: {current_hours}æ™‚é–“) ==="
                )
                return articles
            elif current_hours >= self.config.scraping.max_hours_limit:
                self.logger.warning(
                    f"âš ï¸  æœ€å¤§æ™‚é–“ç¯„å›²({self.config.scraping.max_hours_limit}æ™‚é–“)ã«åˆ°é”"
                )
                self.logger.warning(
                    f"   æœ€çµ‚è¨˜äº‹æ•°: {article_count}ä»¶ (ç›®æ¨™: {self.config.scraping.minimum_article_count}ä»¶)"
                )
                self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (è¨˜äº‹ä¸è¶³, è©¦è¡Œå›æ•°: {attempts}å›) ===")
                return articles
            else:
                # æ™‚é–“ç¯„å›²ã‚’æ®µéšçš„ã«æ‹¡å¼µ
                next_hours = min(current_hours + 24, self.config.scraping.max_hours_limit)
                self.logger.info(
                    f"ğŸ“ˆ è¨˜äº‹æ•°ä¸è¶³ â†’ æ™‚é–“ç¯„å›²æ‹¡å¼µ: {current_hours}æ™‚é–“ â†’ {next_hours}æ™‚é–“"
                )
                current_hours = next_hours

        self.logger.info(f"=== å‹•çš„è¨˜äº‹å–å¾—å®Œäº† (ãƒ«ãƒ¼ãƒ—çµ‚äº†) ===")
        return articles

    def _collect_articles_with_hours(self, hours_limit: int) -> List[Dict[str, Any]]:
        """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ã§è¨˜äº‹ã‚’åé›†ï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰"""
        all_articles = []

        # å„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # Reutersã«ã¯ hours_limit ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
            reuters_params = self.config.reuters.to_dict()
            reuters_params["hours_limit"] = hours_limit

            # Bloombergç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            bloomberg_params = self.config.bloomberg.to_dict()
            bloomberg_params["hours_limit"] = hours_limit

            future_to_scraper = {
                executor.submit(reuters.scrape_reuters_articles, **reuters_params): "Reuters",
                executor.submit(
                    bloomberg.scrape_bloomberg_top_page_articles, **bloomberg_params
                ): "Bloomberg",
            }

            for future in concurrent.futures.as_completed(future_to_scraper):
                scraper_name = future_to_scraper[future]
                try:
                    articles = future.result()
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"{scraper_name} è¨˜äº‹å–å¾—å®Œäº†",
                        operation="collect_articles",
                        scraper=scraper_name,
                        count=len(articles),
                    )
                    all_articles.extend(articles)
                except Exception as e:
                    log_with_context(
                        self.logger,
                        logging.ERROR,
                        f"{scraper_name} è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼",
                        operation="collect_articles",
                        scraper=scraper_name,
                        error=str(e),
                        exc_info=True,
                    )

        # å…¬é–‹æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
        sorted_articles = sorted(
            all_articles, key=lambda x: x.get("published_jst", datetime.min), reverse=True
        )

        log_with_context(
            self.logger,
            logging.INFO,
            "è¨˜äº‹åé›†å®Œäº†",
            operation="collect_articles",
            total_count=len(sorted_articles),
        )
        return sorted_articles

    def collect_articles(self) -> List[Dict[str, Any]]:
        """è¨˜äº‹ã®åé›†ï¼ˆå‹•çš„æ™‚é–“ç¯„å›²å¯¾å¿œï¼‰"""
        # æ–°ã—ã„å‹•çš„åé›†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        return self.collect_articles_with_dynamic_range()

    def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> List[int]:
        """åé›†ã—ãŸè¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆé‡è¤‡ã¯è‡ªå‹•ã§æ’é™¤ï¼‰"""
        log_with_context(
            self.logger,
            logging.INFO,
            "è¨˜äº‹ã®DBä¿å­˜é–‹å§‹",
            operation="save_articles_to_db",
            count=len(articles),
        )
        new_article_ids = []

        for article_data in articles:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã€æ–°è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            article_id, is_new = self.db_manager.save_article(article_data)
            if article_id and is_new:
                new_article_ids.append(article_id)

        log_with_context(
            self.logger,
            logging.INFO,
            "è¨˜äº‹ã®DBä¿å­˜å®Œäº†",
            operation="save_articles_to_db",
            new_articles=len(new_article_ids),
            total_attempted=len(articles),
        )
        return new_article_ids

    def process_new_articles_with_ai(self, new_article_ids: List[int]):
        """æ–°è¦è¨˜äº‹ã®ã¿ã‚’AIã§å‡¦ç†"""
        if not new_article_ids:
            log_with_context(
                self.logger,
                logging.INFO,
                "AIå‡¦ç†å¯¾è±¡ã®æ–°è¦è¨˜äº‹ãªã—",
                operation="process_new_articles",
            )
            return

        log_with_context(
            self.logger,
            logging.INFO,
            f"AIå‡¦ç†é–‹å§‹ï¼ˆæ–°è¦{len(new_article_ids)}ä»¶ï¼‰",
            operation="process_new_articles",
        )

        articles_to_process = self.db_manager.get_articles_by_ids(new_article_ids)

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_article = {
                executor.submit(
                    process_article_with_ai, self.config.ai.gemini_api_key, article.body
                ): article
                for article in articles_to_process
                if article.body
            }

            for future in concurrent.futures.as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    ai_result = future.result()
                    if ai_result:
                        self.db_manager.save_ai_analysis(article.id, ai_result)
                        log_with_context(
                            self.logger, logging.DEBUG, "AIåˆ†æçµæœã‚’ä¿å­˜", article_id=article.id
                        )
                except Exception as e:
                    log_with_context(
                        self.logger,
                        logging.ERROR,
                        f"è¨˜äº‹ID {article.id} ã®AIå‡¦ç†ã‚¨ãƒ©ãƒ¼",
                        operation="process_new_articles",
                        article_id=article.id,
                        error=str(e),
                        exc_info=True,
                    )

        log_with_context(self.logger, logging.INFO, "AIå‡¦ç†å®Œäº†", operation="process_new_articles")

    def process_recent_articles_without_ai(self):
        """AIåˆ†æãŒãªã„24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã‚’å‡¦ç†"""
        # AIåˆ†æãŒãªã„è¨˜äº‹ã®IDã‚’å–å¾—
        with self.db_manager.get_session() as session:
            cutoff_time = datetime.utcnow() - timedelta(hours=self.config.scraping.hours_limit)
            unprocessed_ids = (
                session.query(Article.id)
                .outerjoin(AIAnalysis)
                .filter(
                    Article.published_at >= cutoff_time,
                    AIAnalysis.id.is_(None),
                    Article.body.isnot(None),
                    Article.body != "",
                )
                .all()
            )
            unprocessed_ids = [row[0] for row in unprocessed_ids]

        if not unprocessed_ids:
            log_with_context(
                self.logger,
                logging.INFO,
                "AIå‡¦ç†å¯¾è±¡ã®æœªå‡¦ç†è¨˜äº‹ãªã—",
                operation="process_recent_articles",
            )
            return

        log_with_context(
            self.logger,
            logging.INFO,
            f"æœªå‡¦ç†è¨˜äº‹ã®AIå‡¦ç†é–‹å§‹ï¼ˆ{len(unprocessed_ids)}ä»¶ï¼‰",
            operation="process_recent_articles",
        )

        # è¨˜äº‹ã‚’å–å¾—ã—ã¦AIå‡¦ç†
        articles_to_process = self.db_manager.get_articles_by_ids(unprocessed_ids)

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_article = {
                executor.submit(
                    process_article_with_ai, self.config.ai.gemini_api_key, article.body
                ): article
                for article in articles_to_process
                if article.body
            }

            for future in concurrent.futures.as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    ai_result = future.result()
                    if ai_result:
                        self.db_manager.save_ai_analysis(article.id, ai_result)
                        log_with_context(
                            self.logger, logging.DEBUG, "AIåˆ†æçµæœã‚’ä¿å­˜", article_id=article.id
                        )
                except Exception as e:
                    log_with_context(
                        self.logger,
                        logging.ERROR,
                        f"è¨˜äº‹ID {article.id} ã®AIå‡¦ç†ã‚¨ãƒ©ãƒ¼",
                        operation="process_recent_articles",
                        article_id=article.id,
                        error=str(e),
                        exc_info=True,
                    )

        log_with_context(
            self.logger, logging.INFO, "æœªå‡¦ç†è¨˜äº‹ã®AIå‡¦ç†å®Œäº†", operation="process_recent_articles"
        )

    def process_pro_integration_summaries(
        self, session_id: int, scraped_articles: List[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """
        Proçµ±åˆè¦ç´„å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰

        Args:
            session_id (int): ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            scraped_articles (List[Dict[str, Any]]): ä»Šå›ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿

        Returns:
            Optional[Dict[str, Any]]: çµ±åˆè¦ç´„çµæœã€å¤±æ•—æ™‚ã¯None
        """
        # å‰ææ¡ä»¶ã®æ¤œè¨¼
        if not self._validate_pro_integration_prerequisites():
            return None

        if len(scraped_articles) < self.pro_config.min_articles_threshold:
            log_with_context(
                self.logger,
                logging.INFO,
                f"è¨˜äº‹æ•°ãŒé–¾å€¤æœªæº€ã®ãŸã‚Proçµ±åˆè¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ— ({len(scraped_articles)} < {self.pro_config.min_articles_threshold})",
                operation="pro_integration",
            )
            return None

        # ã‚³ã‚¹ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        cost_check_config = {
            "monthly_limit": self.pro_config.cost_limit_monthly,
            "daily_limit": self.pro_config.cost_limit_monthly / 30,  # æ—¥æ¬¡åˆ¶é™ã¯æœˆæ¬¡åˆ¶é™ã®1/30
        }

        if not check_pro_cost_limits(cost_check_config):
            log_with_context(
                self.logger,
                logging.WARNING,
                "ã‚³ã‚¹ãƒˆåˆ¶é™ã«é”ã—ã¦ã„ã‚‹ãŸã‚Proçµ±åˆè¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                operation="pro_integration",
            )
            return None

        try:
            log_with_context(
                self.logger,
                logging.INFO,
                f"Proçµ±åˆè¦ç´„å‡¦ç†é–‹å§‹ (è¨˜äº‹æ•°: {len(scraped_articles)})",
                operation="pro_integration",
            )

            # è¨˜äº‹ã‚’HTMLè¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã—ã¦AIåˆ†æçµæœã‚’å«ã‚ã‚‹
            enriched_articles = []
            for article_data in scraped_articles:
                try:
                    url = article_data.get("url")
                    if not url:
                        continue

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰AIåˆ†æçµæœã‚’å–å¾—
                    normalized_url = self.db_manager.url_normalizer.normalize_url(url)
                    article_with_analysis = self.db_manager.get_article_by_url_with_analysis(
                        normalized_url
                    )

                    if article_with_analysis and article_with_analysis.ai_analysis:
                        analysis = article_with_analysis.ai_analysis[0]
                        enriched_article = {
                            "title": article_data.get("title", ""),
                            "url": url,
                            "summary": (
                                analysis.summary
                                if analysis.summary
                                else article_data.get("summary", "")
                            ),
                            "category": article_data.get("category", "ãã®ä»–"),
                            "region": self._determine_article_region(article_data),
                            "source": article_data.get("source", ""),
                        }
                        enriched_articles.append(enriched_article)

                except Exception as e:
                    self._handle_pro_integration_error(
                        e, f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ (URL: {article_data.get('url', 'N/A')})"
                    )
                    continue  # å€‹åˆ¥è¨˜äº‹ã®ã‚¨ãƒ©ãƒ¼ã¯ç¶šè¡Œ

            if len(enriched_articles) < self.pro_config.min_articles_threshold:
                log_with_context(
                    self.logger,
                    logging.WARNING,
                    f"AIåˆ†ææ¸ˆã¿è¨˜äº‹æ•°ãŒä¸è¶³ ({len(enriched_articles)} < {self.pro_config.min_articles_threshold})",
                    operation="pro_integration",
                )
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ
                self._pro_integration_fallback_mode(session_id)
                return None

            # è¨˜äº‹ã‚’åœ°åŸŸåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            try:
                grouped_articles = group_articles_for_pro_summary(enriched_articles)
                if not grouped_articles:
                    raise ValueError("è¨˜äº‹ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–çµæœãŒç©ºã§ã™")
            except Exception as e:
                self._handle_pro_integration_error(e, "è¨˜äº‹ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å‡¦ç†ä¸­")
                self._pro_integration_fallback_mode(session_id)
                return None

            # Proçµ±åˆè¦ç´„ã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾å¿œï¼‰
            try:
                integration_result = create_integrated_summaries(
                    self.gemini_api_key, grouped_articles, self.pro_config
                )

                if not integration_result:
                    raise ValueError("çµ±åˆè¦ç´„ã®ç”ŸæˆçµæœãŒç©ºã§ã™")

            except Exception as e:
                self._handle_pro_integration_error(e, "Proçµ±åˆè¦ç´„ç”Ÿæˆä¸­")
                self._pro_integration_fallback_mode(session_id)
                return None

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            try:
                self._save_integrated_summaries_to_db(session_id, integration_result)
            except Exception as e:
                self._handle_pro_integration_error(e, "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­")
                # ä¿å­˜ã«å¤±æ•—ã—ã¦ã‚‚è¦ç´„çµæœã¯è¿”ã™ï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã§ã¯æˆåŠŸï¼‰
                log_with_context(
                    self.logger,
                    logging.WARNING,
                    "çµ±åˆè¦ç´„ã®ä¿å­˜ã¯å¤±æ•—ã—ã¾ã—ãŸãŒã€å‡¦ç†çµæœã¯æ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ",
                    operation="pro_integration",
                )

            # æ–°æ§‹é€ ã«å¯¾å¿œã—ãŸçµ±è¨ˆæƒ…å ±è¡¨ç¤º
            if "unified_summary" in integration_result:
                unified_summary = integration_result["unified_summary"]
                section_count = sum(1 for section in unified_summary.values() if section)
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"Proçµ±åˆè¦ç´„å‡¦ç†å®Œäº† (çµ±åˆè¦ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {section_count})",
                    operation="pro_integration",
                )
            else:
                # å¾“æ¥æ§‹é€ ã¸ã®å¯¾å¿œ
                regional_count = len(integration_result.get("regional_summaries", {}))
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"Proçµ±åˆè¦ç´„å‡¦ç†å®Œäº† (åœ°åŸŸæ•°: {regional_count}, å…¨ä½“è¦ç´„: 1ä»¶)",
                    operation="pro_integration",
                )

            return integration_result

        except Exception as e:
            # äºˆæœŸã—ãªã„ä¾‹å¤–ã«å¯¾ã™ã‚‹æœ€çµ‚çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            self._handle_pro_integration_error(e, "Proçµ±åˆè¦ç´„å‡¦ç†å…¨ä½“")
            self._pro_integration_fallback_mode(session_id)
            return None

    def _determine_article_region(self, article_data: Dict[str, Any]) -> str:
        """è¨˜äº‹ã®åœ°åŸŸã‚’æ±ºå®šï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        title = article_data.get("title", "").lower()
        summary = article_data.get("summary", "").lower()
        text = f"{title} {summary}"

        # æ—¥æœ¬é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        japan_keywords = ["æ—¥æœ¬", "æ—¥éŠ€", "æ±äº¬", "å††", "toyota", "sony", "nintendo", "softbank", "nissan", "honda", "japan", "yen", "boj", "tokyo", "nikkei"]
        if any(keyword in text for keyword in japan_keywords):
            return "japan"
        
        # ã‚¢ãƒ¡ãƒªã‚«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        usa_keywords = ["ç±³å›½", "fed", "dollar", "apple", "microsoft", "google", "amazon", "tesla", "nvidia", "usa", "us", "america", "washington", "wall street", "nasdaq", "s&p"]
        if any(keyword in text for keyword in usa_keywords):
            return "usa"
        
        # ä¸­å›½é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        china_keywords = ["ä¸­å›½", "yuan", "china", "beijing", "shanghai", "alibaba", "tencent", "baidu", "pboc", "renminbi", "hong kong"]
        if any(keyword in text for keyword in china_keywords):
            return "china"
        
        # æ¬§å·é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        europe_keywords = ["æ¬§å·", "ecb", "euro", "europe", "germany", "france", "uk", "britain", "london", "frankfurt", "european", "brexit"]
        if any(keyword in text for keyword in europe_keywords):
            return "europe"
        
        # ãã®ä»–
        return "other"

    def _determine_article_category(self, article_data: Dict[str, Any]) -> str:
        """è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’æ±ºå®š"""
        title = article_data.get("title", "").lower()
        summary = article_data.get("summary", "").lower()
        text = f"{title} {summary}"

        # å¸‚å ´ãƒ»é‡‘è
        if any(keyword in text for keyword in ["market", "stock", "share", "trading", "investment", "fund", "bond", "currency", "forex", "é‡‘åˆ©", "æ ªå¼", "å¸‚å ´", "æŠ•è³‡", "å‚µåˆ¸", "ç‚ºæ›¿"]):
            return "market"
        
        # ä¼æ¥­ãƒ»æ¥­ç¸¾
        elif any(keyword in text for keyword in ["earnings", "revenue", "profit", "company", "corporate", "business", "enterprise", "è²¡å‹™", "æ¥­ç¸¾", "ä¼æ¥­", "å£²ä¸Š", "åˆ©ç›Š"]):
            return "corporate"
        
        # æ”¿æ²»ãƒ»æ”¿ç­–
        elif any(keyword in text for keyword in ["policy", "government", "political", "regulation", "law", "election", "æ”¿æ²»", "æ”¿ç­–", "æ”¿åºœ", "è¦åˆ¶", "æ³•å¾‹", "é¸æŒ™"]):
            return "politics"
        
        # ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼
        elif any(keyword in text for keyword in ["technology", "tech", "ai", "artificial intelligence", "software", "hardware", "digital", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "æŠ€è¡“", "AI", "äººå·¥çŸ¥èƒ½", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢"]):
            return "technology"
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»è³‡æº
        elif any(keyword in text for keyword in ["energy", "oil", "gas", "renewable", "solar", "wind", "coal", "ã‚¨ãƒãƒ«ã‚®ãƒ¼", "çŸ³æ²¹", "ã‚¬ã‚¹", "å†ç”Ÿå¯èƒ½", "å¤ªé™½å…‰"]):
            return "energy"
        
        # ãã®ä»–
        else:
            return "other"

    def _save_integrated_summaries_to_db(self, session_id: int, integration_result: Dict[str, Any]):
        """çµ±åˆè¦ç´„çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆæ–°æ§‹é€ å¯¾å¿œï¼‰"""
        try:
            from src.database.models import IntegratedSummary

            with self.db_manager.get_session() as session:
                # æ–°æ§‹é€ ï¼ˆunified_summaryï¼‰ã«å¯¾å¿œ
                if "unified_summary" in integration_result:
                    unified_summary = integration_result["unified_summary"]
                    metadata = integration_result.get("metadata", {})

                    # çµ±åˆè¦ç´„å…¨ä½“ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«è¦ç´„ã¨ã—ã¦ä¿å­˜
                    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµåˆã—ã¦ä¸€ã¤ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
                    summary_sections = []

                    # åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³
                    if unified_summary.get("regional_summaries"):
                        summary_sections.append(
                            f"ã€åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³ã€‘\n{unified_summary['regional_summaries']}"
                        )

                    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬
                    if unified_summary.get("global_overview"):
                        summary_sections.append(
                            f"ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬ã€‘\n{unified_summary['global_overview']}"
                        )

                    # åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æï¼ˆæœ€é‡è¦ï¼‰
                    if unified_summary.get("cross_regional_analysis"):
                        summary_sections.append(
                            f"ã€åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã€‘\n{unified_summary['cross_regional_analysis']}"
                        )

                    # æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›
                    if unified_summary.get("key_trends"):
                        summary_sections.append(
                            f"ã€æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›ã€‘\n{unified_summary['key_trends']}"
                        )

                    # ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼š
                    if unified_summary.get("risk_factors"):
                        summary_sections.append(
                            f"ã€ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼šã€‘\n{unified_summary['risk_factors']}"
                        )

                    combined_summary_text = "\n\n".join(summary_sections)

                    global_summary = IntegratedSummary(
                        session_id=session_id,
                        summary_type="global",
                        region=None,
                        summary_text=combined_summary_text,
                        articles_count=metadata.get("total_articles", 0),
                        model_version=metadata.get("model_version", "gemini-2.5-pro"),
                        processing_time_ms=metadata.get("processing_time_ms", 0),
                    )
                    session.add(global_summary)

                else:
                    # å¾“æ¥æ§‹é€ ï¼ˆglobal_summary + regional_summariesï¼‰ã¸ã®å¯¾å¿œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if "global_summary" in integration_result:
                        global_summary_data = integration_result["global_summary"]
                        global_summary = IntegratedSummary(
                            session_id=session_id,
                            summary_type="global",
                            region=None,
                            summary_text=global_summary_data["summary_text"],
                            articles_count=global_summary_data["articles_count"],
                            model_version=global_summary_data["model_version"],
                            processing_time_ms=global_summary_data["processing_time_ms"],
                        )
                        session.add(global_summary)

                    # åœ°åŸŸåˆ¥è¦ç´„ã‚’ä¿å­˜ï¼ˆå¾“æ¥æ§‹é€ ï¼‰
                    if "regional_summaries" in integration_result:
                        for region, summary_data in integration_result[
                            "regional_summaries"
                        ].items():
                            regional_summary = IntegratedSummary(
                                session_id=session_id,
                                summary_type="regional",
                                region=region,
                                summary_text=summary_data["summary_text"],
                                articles_count=summary_data["articles_count"],
                                model_version=summary_data["model_version"],
                                processing_time_ms=summary_data["processing_time_ms"],
                            )
                            session.add(regional_summary)

                session.commit()

            log_with_context(
                self.logger,
                logging.INFO,
                f"çµ±åˆè¦ç´„ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜å®Œäº† (ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session_id})",
                operation="save_integrated_summaries",
            )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"çµ±åˆè¦ç´„ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}",
                operation="save_integrated_summaries",
                exc_info=True,
            )

    def _handle_pro_integration_error(self, error: Exception, context: str) -> None:
        """
        Proçµ±åˆè¦ç´„ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

        Args:
            error (Exception): ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼
            context (str): ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        error_details = {
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context,
            "timestamp": datetime.utcnow().isoformat(),
        }

        log_with_context(
            self.logger,
            logging.ERROR,
            f"Proçµ±åˆè¦ç´„ã‚¨ãƒ©ãƒ¼ ({context}): {error}",
            operation="pro_integration_error",
            error_details=error_details,
            exc_info=True,
        )

        # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹å¯¾å‡¦
        if "rate limit" in str(error).lower() or "quota" in str(error).lower():
            log_with_context(
                self.logger,
                logging.WARNING,
                "APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„",
                operation="pro_integration_error",
            )
        elif "timeout" in str(error).lower():
            log_with_context(
                self.logger,
                logging.WARNING,
                "APIå¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚è¨˜äº‹æ•°ã‚’æ¸›ã‚‰ã™ã‹è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                operation="pro_integration_error",
            )
        elif "authentication" in str(error).lower() or "api key" in str(error).lower():
            log_with_context(
                self.logger,
                logging.ERROR,
                "APIèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                operation="pro_integration_error",
            )

    def _pro_integration_fallback_mode(self, session_id: int) -> bool:
        """
        Proçµ±åˆè¦ç´„å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†

        Args:
            session_id (int): ã‚»ãƒƒã‚·ãƒ§ãƒ³ID

        Returns:
            bool: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            log_with_context(
                self.logger,
                logging.INFO,
                "Proçµ±åˆè¦ç´„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†é–‹å§‹",
                operation="pro_fallback",
            )

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Flash-Liteã«ã‚ˆã‚‹ç°¡æ˜“çµ±åˆè¦ç´„ã‚’ä½œæˆ
            with self.db_manager.get_session() as db_session:
                from src.database.models import IntegratedSummary

                # ç°¡æ˜“ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ã‚’ä½œæˆ
                fallback_summary = IntegratedSummary(
                    session_id=session_id,
                    summary_type="global",
                    region=None,
                    summary_text="Proçµ±åˆè¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚å€‹åˆ¥è¨˜äº‹ã®è¦ç´„ã‚’ã”ç¢ºèªãã ã•ã„ã€‚",
                    articles_count=0,
                    model_version="fallback",
                    processing_time_ms=0,
                )
                db_session.add(fallback_summary)
                db_session.commit()

            log_with_context(
                self.logger,
                logging.INFO,
                "Proçµ±åˆè¦ç´„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œäº†",
                operation="pro_fallback",
            )
            return True

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}",
                operation="pro_fallback",
                exc_info=True,
            )
            return False

    def _validate_pro_integration_prerequisites(self) -> bool:
        """
        Proçµ±åˆè¦ç´„ã®å‰ææ¡ä»¶ã‚’æ¤œè¨¼

        Returns:
            bool: å‰ææ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã©ã†ã‹
        """
        # APIã‚­ãƒ¼ã®æ¤œè¨¼
        if not self.gemini_api_key:
            log_with_context(
                self.logger,
                logging.ERROR,
                "Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                operation="pro_validation",
            )
            return False

        # è¨­å®šã®æ¤œè¨¼
        if not self.pro_config.enabled:
            log_with_context(
                self.logger,
                logging.INFO,
                "Proçµ±åˆè¦ç´„æ©Ÿèƒ½ãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™",
                operation="pro_validation",
            )
            return False

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®æ¤œè¨¼
        try:
            from sqlalchemy import text

            with self.db_manager.get_session() as session:
                session.execute(text("SELECT 1"))
        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—: {e}",
                operation="pro_validation",
            )
            return False

        return True

    def _log_pro_integration_statistics(
        self,
        integration_result: Optional[Dict[str, Any]],
        session_id: int,
        scraped_articles_count: int,
    ):
        """
        Proçµ±åˆè¦ç´„ã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²

        Args:
            integration_result: çµ±åˆè¦ç´„çµæœ
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            scraped_articles_count: å‡¦ç†å¯¾è±¡è¨˜äº‹æ•°
        """
        try:
            log_with_context(
                self.logger, logging.INFO, "=== Proçµ±åˆè¦ç´„å‡¦ç†çµ±è¨ˆ ===", operation="pro_statistics"
            )

            log_with_context(
                self.logger, logging.INFO, f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}", operation="pro_statistics"
            )

            log_with_context(
                self.logger,
                logging.INFO,
                f"å‡¦ç†å¯¾è±¡è¨˜äº‹æ•°: {scraped_articles_count}ä»¶",
                operation="pro_statistics",
            )

            if integration_result:
                # æ–°æ§‹é€ ã«å¯¾å¿œã—ãŸçµ±è¨ˆæƒ…å ±
                metadata = integration_result.get("metadata", {})

                log_with_context(
                    self.logger, logging.INFO, f"çµ±åˆè¦ç´„ç”Ÿæˆ: æˆåŠŸ", operation="pro_statistics"
                )

                # æ–°æ§‹é€ ï¼ˆunified_summaryï¼‰ã®å ´åˆ
                if "unified_summary" in integration_result:
                    unified_summary = integration_result["unified_summary"]
                    total_chars = sum(
                        len(str(section)) for section in unified_summary.values() if section
                    )

                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"çµ±åˆè¦ç´„æ–‡å­—æ•°: {total_chars}å­—",
                        operation="pro_statistics",
                    )

                    # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ–‡å­—æ•°
                    section_names = {
                        "regional_summaries": "åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³",
                        "global_overview": "ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬",
                        "cross_regional_analysis": "åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æ",
                        "key_trends": "æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›",
                        "risk_factors": "ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼š",
                    }

                    for section_key, section_name in section_names.items():
                        if section_key in unified_summary and unified_summary[section_key]:
                            char_count = len(unified_summary[section_key])
                            log_with_context(
                                self.logger,
                                logging.INFO,
                                f"  {section_name}: {char_count}å­—",
                                operation="pro_statistics",
                            )

                else:
                    # å¾“æ¥æ§‹é€ ã¸ã®å¯¾å¿œ
                    global_summary = integration_result.get("global_summary", {})
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"å…¨ä½“è¦ç´„æ–‡å­—æ•°: {len(global_summary.get('summary_text', ''))}å­—",
                        operation="pro_statistics",
                    )

                # å‡¦ç†æ™‚é–“çµ±è¨ˆ
                processing_time_ms = metadata.get("processing_time_ms", 0)
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"çµ±åˆè¦ç´„å‡¦ç†æ™‚é–“: {processing_time_ms}ms",
                    operation="pro_statistics",
                )

                # è¨˜äº‹åˆ†å¸ƒçµ±è¨ˆ
                articles_by_region = metadata.get("articles_by_region", {})
                if articles_by_region:
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"è¨˜äº‹åœ°åŸŸåˆ†å¸ƒ: {articles_by_region}",
                        operation="pro_statistics",
                    )

                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"ç·å‡¦ç†æ™‚é–“: {processing_time_ms}ms ({processing_time_ms/1000:.1f}ç§’)",
                    operation="pro_statistics",
                )

            else:
                # å¤±æ•—æ™‚ã®çµ±è¨ˆ
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"çµ±åˆè¦ç´„ç”Ÿæˆ: å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—",
                    operation="pro_statistics",
                )

                # ã‚³ã‚¹ãƒˆçµ±è¨ˆã‚’è¡¨ç¤º
                cost_stats = self.cost_manager.get_cost_statistics(days=1)  # æœ¬æ—¥åˆ†
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"æœ¬æ—¥ã®APIä½¿ç”¨çŠ¶æ³: ${cost_stats.get('daily_cost', 0):.6f}",
                    operation="pro_statistics",
                )

            log_with_context(
                self.logger, logging.INFO, "=== Proçµ±åˆè¦ç´„çµ±è¨ˆçµ‚äº† ===", operation="pro_statistics"
            )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"çµ±è¨ˆæƒ…å ±ãƒ­ã‚°è¨˜éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}",
                operation="pro_statistics",
                exc_info=True,
            )

    def _monitor_system_performance(self, start_time: float, operation: str):
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›£è¦–

        Args:
            start_time: å‡¦ç†é–‹å§‹æ™‚åˆ»
            operation: æ“ä½œå
        """
        try:
            import psutil
            import os

            elapsed_time = time.time() - start_time

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024

            # CPUä½¿ç”¨ç‡
            cpu_percent = process.cpu_percent()

            log_with_context(
                self.logger,
                logging.INFO,
                f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦– ({operation}): "
                f"å®Ÿè¡Œæ™‚é–“={elapsed_time:.2f}ç§’, "
                f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={memory_mb:.1f}MB, "
                f"CPUä½¿ç”¨ç‡={cpu_percent:.1f}%",
                operation="performance_monitoring",
            )

        except ImportError:
            # psutilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åŸºæœ¬æƒ…å ±ã®ã¿
            elapsed_time = time.time() - start_time
            log_with_context(
                self.logger,
                logging.INFO,
                f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦– ({operation}): å®Ÿè¡Œæ™‚é–“={elapsed_time:.2f}ç§’",
                operation="performance_monitoring",
            )
        except Exception as e:
            log_with_context(
                self.logger,
                logging.DEBUG,
                f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="performance_monitoring",
            )

    def _get_integrated_summaries_for_html(self, session_id: int) -> Optional[Dict[str, Any]]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’HTMLè¡¨ç¤ºç”¨ã«å–å¾—

        Args:
            session_id (int): ã‚»ãƒƒã‚·ãƒ§ãƒ³ID

        Returns:
            Optional[Dict[str, Any]]: HTMLè¡¨ç¤ºç”¨ã®çµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿
        """
        if not session_id:
            return None

        try:
            from src.database.models import IntegratedSummary

            with self.db_manager.get_session() as session:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã«åŸºã¥ã„ã¦çµ±åˆè¦ç´„ã‚’å–å¾—
                summaries = (
                    session.query(IntegratedSummary)
                    .filter(IntegratedSummary.session_id == session_id)
                    .all()
                )

                if not summaries:
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ã®çµ±åˆè¦ç´„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                        operation="get_integrated_summaries",
                    )
                    return None

                # DBã‹ã‚‰å–å¾—ã—ãŸçµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’HTMLè¡¨ç¤ºç”¨ã«æ•´ç†
                # æ–°æ§‹é€ ï¼ˆunified_summaryï¼‰ã¨å¾“æ¥æ§‹é€ ã®ä¸¡æ–¹ã«å¯¾å¿œ

                unified_summary_dict = {}
                metadata = {"total_articles": 0, "processing_time_ms": 0}

                for summary in summaries:
                    if summary.summary_type == "global":
                        # çµ±åˆè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºã™ã‚‹è©¦è¡Œ
                        summary_text = summary.summary_text

                        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã‚’è©¦è¡Œï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã«åŸºã¥ãï¼‰
                        sections = self._parse_combined_summary_text(summary_text)

                        if sections:
                            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã«æˆåŠŸã—ãŸå ´åˆ
                            unified_summary_dict = sections
                        else:
                            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†å‰²ã«å¤±æ•—ã—ãŸå ´åˆã¯å…¨ä½“ã‚’global_overviewã¨ã—ã¦æ‰±ã†
                            unified_summary_dict = {
                                "global_overview": summary_text,
                                "regional_summaries": "",
                                "cross_regional_analysis": "",
                                "key_trends": "",
                                "risk_factors": "",
                            }

                        metadata["total_articles"] = summary.articles_count
                        metadata["processing_time_ms"] += summary.processing_time_ms

                html_summaries = {"unified_summary": unified_summary_dict, "metadata": metadata}

                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"çµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’HTMLç”¨ã«æº–å‚™å®Œäº† (çµ±åˆè¦ç´„: {'ã‚ã‚Š' if unified_summary_dict.get('global_overview') else 'ãªã—'})",
                    operation="get_integrated_summaries",
                )

                return html_summaries if unified_summary_dict.get("global_overview") else None

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"çµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="get_integrated_summaries",
                exc_info=True,
            )
            return None

    def _parse_combined_summary_text(self, combined_text: str) -> Optional[Dict[str, str]]:
        """
        çµåˆã•ã‚ŒãŸçµ±åˆè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²

        Args:
            combined_text (str): çµåˆã•ã‚ŒãŸçµ±åˆè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            Optional[Dict[str, str]]: åˆ†å‰²ã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³è¾æ›¸ã€å¤±æ•—æ™‚ã¯None
        """
        if not combined_text:
            return None

        try:
            sections = {
                "regional_summaries": "",
                "global_overview": "",
                "cross_regional_analysis": "",
                "key_trends": "",
                "risk_factors": "",
            }

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            section_markers = {
                "ã€åœ°åŸŸåˆ¥å¸‚å ´æ¦‚æ³ã€‘": "regional_summaries",
                "ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸‚å ´ç·æ‹¬ã€‘": "global_overview",
                "ã€åœ°åŸŸé–“ç›¸äº’å½±éŸ¿åˆ†æã€‘": "cross_regional_analysis",
                "ã€æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å°†æ¥å±•æœ›ã€‘": "key_trends",
                "ã€ãƒªã‚¹ã‚¯è¦å› ãƒ»æŠ•è³‡æ©Ÿä¼šã€‘": "risk_factors",
            }

            current_section = None
            current_content = []

            lines = combined_text.split("\n")

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—ã‚’æ¤œå‡º
                section_found = False
                for marker, section_key in section_markers.items():
                    if marker in line:
                        # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                        if current_section and current_content:
                            sections[current_section] = "\n".join(current_content).strip()

                        current_section = section_key
                        current_content = []
                        section_found = True
                        break

                if not section_found and current_section:
                    current_content.append(line)

            # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
            if current_section and current_content:
                sections[current_section] = "\n".join(current_content).strip()

            # æœ‰åŠ¹ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿è¿”ã™
            if any(sections.values()):
                return sections
            else:
                return None

        except Exception as e:
            log_with_context(
                self.logger,
                logging.WARNING,
                f"çµ±åˆè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="parse_combined_summary",
            )
            return None

    def _log_session_summary(
        self, session_id: int, start_time: float, integration_result: Optional[Dict[str, Any]]
    ):
        """
        ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²

        Args:
            session_id: ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
            start_time: å‡¦ç†é–‹å§‹æ™‚åˆ»
            integration_result: çµ±åˆè¦ç´„çµæœ
        """
        try:
            total_elapsed = time.time() - start_time

            log_with_context(
                self.logger,
                logging.INFO,
                f"=== ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} å®Œäº†ã‚µãƒãƒªãƒ¼ ===",
                operation="session_summary",
            )

            log_with_context(
                self.logger,
                logging.INFO,
                f"ç·å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’",
                operation="session_summary",
            )

            # Proçµ±åˆè¦ç´„ã®çŠ¶æ³
            if integration_result:
                log_with_context(
                    self.logger, logging.INFO, "Proçµ±åˆè¦ç´„: æˆåŠŸ", operation="session_summary"
                )

                # æ–°æ§‹é€ ã«å¯¾å¿œã—ãŸè¦ç´„æ¦‚è¦
                if "unified_summary" in integration_result:
                    unified_summary = integration_result["unified_summary"]
                    total_chars = sum(
                        len(str(section)) for section in unified_summary.values() if section
                    )
                    section_count = sum(1 for section in unified_summary.values() if section)

                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"  çµ±åˆè¦ç´„: {total_chars}å­— ({section_count}ã‚»ã‚¯ã‚·ãƒ§ãƒ³)",
                        operation="session_summary",
                    )
                else:
                    # å¾“æ¥æ§‹é€ ã¸ã®å¯¾å¿œ
                    global_chars = len(
                        integration_result.get("global_summary", {}).get("summary_text", "")
                    )
                    regional_count = len(integration_result.get("regional_summaries", {}))

                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"  å…¨ä½“è¦ç´„: {global_chars}å­—",
                        operation="session_summary",
                    )
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        f"  åœ°åŸŸåˆ¥è¦ç´„: {regional_count}ä»¶",
                        operation="session_summary",
                    )
            else:
                log_with_context(
                    self.logger,
                    logging.INFO,
                    "Proçµ±åˆè¦ç´„: ã‚¹ã‚­ãƒƒãƒ—ã¾ãŸã¯å¤±æ•—",
                    operation="session_summary",
                )

            # ã‚³ã‚¹ãƒˆæƒ…å ±
            try:
                monthly_cost = self.cost_manager.get_monthly_cost()
                daily_cost = self.cost_manager.get_daily_cost()

                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"APIä½¿ç”¨ã‚³ã‚¹ãƒˆ - ä»Šæœˆ: ${monthly_cost:.6f}, ä»Šæ—¥: ${daily_cost:.6f}",
                    operation="session_summary",
                )
            except Exception:
                pass  # ã‚³ã‚¹ãƒˆæƒ…å ±å–å¾—ã§ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

            log_with_context(
                self.logger,
                logging.INFO,
                f"=== ã‚»ãƒƒã‚·ãƒ§ãƒ³ {session_id} ã‚µãƒãƒªãƒ¼çµ‚äº† ===",
                operation="session_summary",
            )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼è¨˜éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}",
                operation="session_summary",
                exc_info=True,
            )

    def prepare_current_session_articles_for_html(
        self, scraped_articles: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã‚’HTMLè¡¨ç¤ºç”¨ã«æº–å‚™ï¼ˆAIåˆ†æçµæœã¨çµ„ã¿åˆã‚ã›ï¼‰
        æ³¨ï¼šä»Šå›ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€éå»ã®è¨˜äº‹ã®æ··å…¥ã‚’é˜²ã
        """
        log_with_context(
            self.logger,
            logging.INFO,
            f"HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹ (åˆæœŸè¨˜äº‹æ•°: {len(scraped_articles)}ä»¶)",
            operation="prepare_current_session_articles",
        )

        final_articles = []
        processed_urls = set()
        ai_analysis_found = 0
        duplicates_skipped = 0

        for i, scraped_article in enumerate(scraped_articles):
            url = scraped_article.get("url")
            if not url:
                log_with_context(
                    self.logger,
                    logging.WARNING,
                    f"è¨˜äº‹ {i} ã«URLãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚",
                    operation="prepare_html_data",
                )
                continue

            # URLã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒã®ç²¾åº¦ã‚’ä¸Šã’ã‚‹
            normalized_url = self.db_manager.url_normalizer.normalize_url(url)

            if normalized_url in processed_urls:
                duplicates_skipped += 1
                log_with_context(
                    self.logger,
                    logging.DEBUG,
                    f"é‡è¤‡URLã‚’ã‚¹ã‚­ãƒƒãƒ—: å…ƒURL='{url}', æ­£è¦åŒ–URL='{normalized_url}'",
                    operation="prepare_html_data",
                )
                continue

            processed_urls.add(normalized_url)
            log_with_context(
                self.logger,
                logging.DEBUG,
                f"æ–°è¦URLã‚’å‡¦ç†: å…ƒURL='{url}', æ­£è¦åŒ–URL='{normalized_url}'",
                operation="prepare_html_data",
            )

            article_data = {
                "title": scraped_article.get("title", ""),
                "url": url,  # è¡¨ç¤ºã«ã¯å…ƒã®URLã‚’ä½¿ç”¨
                "source": scraped_article.get("source", ""),
                "published_jst": scraped_article.get("published_jst", ""),
                "summary": "è¦ç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "sentiment_label": "N/A",
                "sentiment_score": 0.0,
                "category": "ãã®ä»–",
                "region": "ãã®ä»–",
            }

            try:
                # DBã‹ã‚‰ã¯æ­£è¦åŒ–æ¸ˆã¿URLã§å•ã„åˆã‚ã›ã‚‹ã®ãŒç¢ºå®Ÿ
                log_with_context(
                    self.logger,
                    logging.DEBUG,
                    f"AIåˆ†æçµæœã‚’æ¤œç´¢ä¸­: æ­£è¦åŒ–URL='{normalized_url}'",
                    operation="prepare_html_data",
                )
                article_with_analysis = self.db_manager.get_article_by_url_with_analysis(
                    normalized_url
                )
                
                if article_with_analysis:
                    log_with_context(
                        self.logger,
                        logging.DEBUG,
                        f"è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: title='{article_with_analysis.title}', ai_analysis_count={len(article_with_analysis.ai_analysis) if article_with_analysis.ai_analysis else 0}",
                        operation="prepare_html_data",
                    )
                    
                    if article_with_analysis.ai_analysis:
                        analysis = article_with_analysis.ai_analysis[0]
                        log_with_context(
                            self.logger,
                            logging.DEBUG,
                            f"AIåˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: category='{analysis.category}', region='{analysis.region}', summary_length={len(analysis.summary) if analysis.summary else 0}",
                            operation="prepare_html_data",
                        )
                        
                        if analysis.summary:
                            article_data.update(
                                {
                                    "summary": analysis.summary,
                                    "sentiment_label": (
                                        analysis.sentiment_label if analysis.sentiment_label else "N/A"
                                    ),
                                    "sentiment_score": (
                                        analysis.sentiment_score
                                        if analysis.sentiment_score is not None
                                        else 0.0
                                    ),
                                    "category": analysis.category if analysis.category else "ãã®ä»–",
                                    "region": analysis.region if analysis.region else "ãã®ä»–",
                                }
                            )
                            ai_analysis_found += 1
                            log_with_context(
                                self.logger,
                                logging.INFO,
                                f"AIåˆ†æçµæœã‚’è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã«è¨­å®šå®Œäº†: URL='{url}', category='{analysis.category}', region='{analysis.region}'",
                                operation="prepare_html_data",
                            )
                        else:
                            log_with_context(
                                self.logger,
                                logging.WARNING,
                                f"AIåˆ†æã¯å­˜åœ¨ã™ã‚‹ãŒè¦ç´„ãŒç©ºã§ã™: URL='{url}'",
                                operation="prepare_html_data",
                            )
                    else:
                        log_with_context(
                            self.logger,
                            logging.WARNING,
                            f"è¨˜äº‹ã¯è¦‹ã¤ã‹ã£ãŸãŒAIåˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“: URL='{url}'",
                            operation="prepare_html_data",
                        )
                else:
                    log_with_context(
                        self.logger,
                        logging.WARNING,
                        f"è¨˜äº‹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: æ­£è¦åŒ–URL='{normalized_url}'",
                        operation="prepare_html_data",
                    )
            except Exception as e:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    f"AIåˆ†æçµæœã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: URL='{url}', æ­£è¦åŒ–URL='{normalized_url}' - {e}",
                    operation="prepare_html_data",
                    exc_info=True,
                )

            final_articles.append(article_data)

        log_with_context(
            self.logger,
            logging.INFO,
            f"HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† (é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {duplicates_skipped}ä»¶, æœ€çµ‚è¨˜äº‹æ•°: {len(final_articles)}ä»¶, AIåˆ†æã‚ã‚Š: {ai_analysis_found}ä»¶)",
            operation="prepare_current_session_articles",
        )
        return final_articles

    def generate_final_html(
        self, current_session_articles: List[Dict[str, Any]] = None, session_id: int = None
    ):
        """æœ€çµ‚çš„ãªHTMLã‚’ç”Ÿæˆï¼ˆä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã®ã¿ï¼‰"""
        log_with_context(
            self.logger, logging.INFO, "æœ€çµ‚HTMLç”Ÿæˆé–‹å§‹", operation="generate_final_html"
        )

        # Proçµ±åˆè¦ç´„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        integrated_summaries = (
            self._get_integrated_summaries_for_html(session_id) if session_id else None
        )

        # ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ãŒæ¸¡ã•ã‚Œãªã„å ´åˆã¯ç©ºã®HTMLã‚’ç”Ÿæˆ
        if not current_session_articles:
            log_with_context(
                self.logger,
                logging.WARNING,
                "HTMLç”Ÿæˆå¯¾è±¡ã®è¨˜äº‹ãªã—",
                operation="generate_final_html",
            )
            self.html_generator.generate_html_file(
                [], "index.html", integrated_summaries=integrated_summaries
            )
            return

        # ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ã‚’HTMLã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ç”¨ã«å¤‰æ›
        articles_for_html = []
        for article_data in current_session_articles:
            articles_for_html.append(
                {
                    "title": article_data.get("title", ""),
                    "url": article_data.get("url", ""),
                    "summary": article_data.get("summary", "è¦ç´„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"),
                    "source": article_data.get("source", ""),
                    "published_jst": article_data.get("published_jst", ""),
                    "sentiment_label": article_data.get("sentiment_label", "N/A"),
                    "sentiment_score": article_data.get("sentiment_score", 0.0),
                    "category": article_data.get("category", "ãã®ä»–"),
                    "region": article_data.get("region", "ãã®ä»–"),
                }
            )

        # è¨˜äº‹ã‚’å…¬é–‹æ™‚åˆ»é †ï¼ˆæœ€æ–°é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
        articles_for_html = self._sort_articles_by_time(articles_for_html)

        self.html_generator.generate_html_file(
            articles_for_html, "index.html", integrated_summaries=integrated_summaries
        )
        log_with_context(
            self.logger,
            logging.INFO,
            "æœ€çµ‚HTMLç”Ÿæˆå®Œäº†",
            operation="generate_final_html",
            count=len(articles_for_html),
        )

    def _sort_articles_by_time(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¨˜äº‹ã‚’å…¬é–‹æ™‚åˆ»é †ï¼ˆæœ€æ–°é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ

        Args:
            articles: ã‚½ãƒ¼ãƒˆå¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ

        Returns:
            ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
        """

        def get_sort_key(article):
            published_jst = article.get("published_jst")
            if not published_jst:
                return datetime.min  # æ—¥æ™‚ãŒä¸æ˜ãªè¨˜äº‹ã¯æœ€å¾Œå°¾ã«

            # datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if hasattr(published_jst, "year"):
                return published_jst

            # æ–‡å­—åˆ—ã®å ´åˆã¯å¤‰æ›ã‚’è©¦è¡Œ
            if isinstance(published_jst, str):
                try:
                    # ISOå½¢å¼ã®ãƒ‘ãƒ¼ã‚¹
                    return datetime.fromisoformat(published_jst.replace("Z", "+00:00"))
                except:
                    return datetime.min

            return datetime.min

        try:
            sorted_articles = sorted(articles, key=get_sort_key, reverse=True)
            log_with_context(
                self.logger,
                logging.INFO,
                f"è¨˜äº‹ã‚’æ™‚åˆ»é †ã§ã‚½ãƒ¼ãƒˆå®Œäº† (æœ€æ–°: {get_sort_key(sorted_articles[0]) if sorted_articles else 'N/A'})",
                operation="sort_articles",
            )
            return sorted_articles
        except Exception as e:
            log_with_context(
                self.logger,
                logging.WARNING,
                f"è¨˜äº‹ã‚½ãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e} - å…ƒã®é †åºã‚’ç¶­æŒ",
                operation="sort_articles",
            )
            return articles

    def generate_google_docs_and_sheets(self, session_id: int, current_session_articles: List[Dict[str, Any]]):
        """Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆå‡¦ç†"""
        log_with_context(
            self.logger,
            logging.INFO,
            "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆé–‹å§‹",
            operation="generate_google_docs_and_sheets",
        )

        # Googleèªè¨¼ï¼ˆSheets APIã‚‚å«ã‚€ï¼‰
        drive_service, docs_service, sheets_service = authenticate_google_services()
        if not drive_service or not docs_service:
            log_with_context(
                self.logger, logging.ERROR, "Googleèªè¨¼ã«å¤±æ•—", operation="generate_google_docs_and_sheets"
            )
            return
        
        # Sheetsã‚µãƒ¼ãƒ“ã‚¹ãŒå–å¾—ã§ããªã„å ´åˆã®è­¦å‘Š
        if not sheets_service:
            log_with_context(
                self.logger, 
                logging.WARNING, 
                "Sheets APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚",
                operation="generate_google_docs_and_sheets"
            )

        # 1. æ—¢å­˜ã®Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        self._generate_google_docs_internal(drive_service, docs_service)

        # 2. è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ç”Ÿæˆ
        if sheets_service and current_session_articles:
            self._generate_articles_spreadsheet(
                drive_service, sheets_service, session_id, current_session_articles
            )
        elif not sheets_service:
            log_with_context(
                self.logger,
                logging.INFO,
                "Sheets APIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚",
                operation="generate_google_docs_and_sheets",
            )

        # 3. APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°
        if sheets_service:
            self._update_api_usage_spreadsheet(
                drive_service, sheets_service, session_id
            )
        else:
            log_with_context(
                self.logger,
                logging.INFO,
                "Sheets APIãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚",
                operation="generate_google_docs_and_sheets",
            )

    def _generate_articles_spreadsheet(
        self, drive_service, sheets_service, session_id: int, articles: List[Dict[str, Any]]
    ):
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        try:
            log_with_context(
                self.logger,
                logging.INFO,
                f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆé–‹å§‹ (è¨˜äº‹æ•°: {len(articles)})",
                operation="generate_articles_spreadsheet",
            )

            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ
            spreadsheet_id = create_debug_spreadsheet(
                sheets_service, drive_service, self.folder_id, session_id
            )
            
            if not spreadsheet_id:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    "è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆã«å¤±æ•—",
                    operation="generate_articles_spreadsheet",
                )
                return

            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            headers = [
                "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«", "å…¬é–‹æ™‚åˆ»(JST)", "AIè¦ç´„", "åœ°åŸŸ", "ã‚«ãƒ†ã‚´ãƒªãƒ¼", 
                "æ„Ÿæƒ…åˆ†æ", "æ„Ÿæƒ…ã‚¹ã‚³ã‚¢", "ã‚½ãƒ¼ã‚¹", "URL", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID", "å‡¦ç†æ—¥æ™‚"
            ]
            
            jst_tz = pytz.timezone("Asia/Tokyo")
            process_time = datetime.now(jst_tz).strftime('%Y-%m-%d %H:%M:%S')
            
            data_rows = [headers]
            
            for article in articles:
                # åœ°åŸŸã¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’è‡ªå‹•åˆ†é¡
                region = self._determine_article_region(article)
                category = self._determine_article_category(article)
                
                # å…¬é–‹æ™‚åˆ»ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                published_jst = article.get("published_jst", "")
                if hasattr(published_jst, "strftime"):
                    published_str = published_jst.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    published_str = str(published_jst)
                
                row = [
                    article.get("title", ""),
                    published_str,
                    article.get("summary", "è¦ç´„ãªã—"),
                    region,
                    category,
                    article.get("sentiment_label", "N/A"),
                    article.get("sentiment_score", 0.0),
                    article.get("source", ""),
                    article.get("url", ""),
                    session_id,
                    process_time
                ]
                data_rows.append(row)

            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
            success = update_debug_spreadsheet(sheets_service, spreadsheet_id, data_rows)
            
            if success:
                spreadsheet_url = get_spreadsheet_url(spreadsheet_id)
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {spreadsheet_url}",
                    operation="generate_articles_spreadsheet",
                )
            else:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    "è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã«å¤±æ•—",
                    operation="generate_articles_spreadsheet",
                )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="generate_articles_spreadsheet",
                exc_info=True,
            )

    def _update_api_usage_spreadsheet(self, drive_service, sheets_service, session_id: int):
        """APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’æ›´æ–°"""
        try:
            log_with_context(
                self.logger,
                logging.INFO,
                "APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°é–‹å§‹",
                operation="update_api_usage_spreadsheet",
            )

            # ã‚³ã‚¹ãƒˆç®¡ç†ã‹ã‚‰è©³ç´°çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            cost_stats = self.cost_manager.get_spreadsheet_cost_data(session_id)
            cost_alert = self.cost_manager.generate_cost_alert(monthly_limit=50.0, daily_limit=5.0)
            
            jst_tz = pytz.timezone("Asia/Tokyo")
            current_time = datetime.now(jst_tz).strftime('%Y-%m-%d %H:%M:%S')
            
            headers = [
                "æ—¥æ™‚(JST)", "ãƒ¢ãƒ‡ãƒ«å", "APIç¨®åˆ¥", "ãƒªã‚¯ã‚¨ã‚¹ãƒˆå›æ•°", "å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", 
                "å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", "æ¨å®šã‚³ã‚¹ãƒˆ(USD)", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID", "ç´¯ç©æœˆé–“ã‚³ã‚¹ãƒˆ(USD)"
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¯ã®APIä½¿ç”¨é‡ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            api_data_rows = [headers]
            
            # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®APIä½¿ç”¨é‡ã‚’è¨˜éŒ²
            model_breakdown = cost_stats.get('model_breakdown', {})
            for model_name, model_data in model_breakdown.items():
                api_data_rows.append([
                    current_time,
                    model_name,
                    "è¨˜äº‹è¦ç´„ãƒ»çµ±åˆè¦ç´„",
                    model_data.get('requests', 0),
                    model_data.get('input_tokens', 0),
                    model_data.get('output_tokens', 0),
                    model_data.get('cost', 0.0),
                    session_id,
                    cost_stats.get('monthly_cost', 0.0)
                ])
            
            # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ä»£æ›¿
            if not model_breakdown:
                api_data_rows.append([
                    current_time,
                    "gemini-1.5-flash",
                    "è¨˜äº‹è¦ç´„",
                    cost_stats.get('total_requests', 0),
                    cost_stats.get('total_input_tokens', 0),
                    cost_stats.get('total_output_tokens', 0),
                    cost_stats.get('total_cost', 0.0),
                    session_id,
                    cost_stats.get('monthly_cost', 0.0)
                ])
            
            # æ—¢å­˜ã®APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’æ¤œç´¢ã¾ãŸã¯ä½œæˆ
            api_spreadsheet_id = self._get_or_create_api_usage_spreadsheet(
                drive_service, sheets_service
            )
            
            if api_spreadsheet_id:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½è¨˜
                self._append_api_usage_data(sheets_service, api_spreadsheet_id, api_data_rows[1:])
                
                api_spreadsheet_url = get_spreadsheet_url(api_spreadsheet_id)
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†: {api_spreadsheet_url}",
                    operation="update_api_usage_spreadsheet",
                )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="update_api_usage_spreadsheet",
                exc_info=True,
            )

    def _get_or_create_api_usage_spreadsheet(self, drive_service, sheets_service) -> str:
        """APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        try:
            # æ—¢å­˜ã®APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’æ¤œç´¢
            spreadsheet_name = "Market_News_API_Usage_Tracker"
            query = f"name='{spreadsheet_name}' and '{self.folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet' and trashed=false"
            
            response = drive_service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
            existing_files = response.get('files', [])

            if existing_files:
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                spreadsheet_id = existing_files[0].get('id')
                log_with_context(
                    self.logger,
                    logging.INFO,
                    f"æ—¢å­˜APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½¿ç”¨: {spreadsheet_id}",
                    operation="get_or_create_api_usage_spreadsheet",
                )
                return spreadsheet_id
            else:
                # æ–°è¦ä½œæˆ
                return self._create_new_api_usage_spreadsheet(drive_service, sheets_service, spreadsheet_name)

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå–å¾—ãƒ»ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="get_or_create_api_usage_spreadsheet",
                exc_info=True,
            )
            return None

    def _create_new_api_usage_spreadsheet(self, drive_service, sheets_service, title: str) -> str:
        """æ–°è¦APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ"""
        try:
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ
            spreadsheet = {
                'properties': {
                    'title': title
                },
                'sheets': [{
                    'properties': {
                        'title': 'API_Usage_Log'
                    }
                }]
            }
            
            spreadsheet_result = sheets_service.spreadsheets().create(body=spreadsheet).execute()
            spreadsheet_id = spreadsheet_result['spreadsheetId']
            
            # æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•
            if self.folder_id:
                drive_service.files().update(
                    fileId=spreadsheet_id,
                    addParents=self.folder_id,
                    removeParents='root'
                ).execute()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¨­å®š
            headers = [
                "æ—¥æ™‚(JST)", "ãƒ¢ãƒ‡ãƒ«å", "APIç¨®åˆ¥", "ãƒªã‚¯ã‚¨ã‚¹ãƒˆå›æ•°", "å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³", 
                "å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", "æ¨å®šã‚³ã‚¹ãƒˆ(USD)", "ã‚»ãƒƒã‚·ãƒ§ãƒ³ID", "ç´¯ç©æœˆé–“ã‚³ã‚¹ãƒˆ(USD)"
            ]
            
            range_name = 'API_Usage_Log!A1:I1'
            body = {'values': [headers]}
            
            sheets_service.spreadsheets().values().update(
                spreadsheetId=spreadsheet_id,
                range=range_name,
                valueInputOption='RAW',
                body=body
            ).execute()
            
            log_with_context(
                self.logger,
                logging.INFO,
                f"æ–°è¦APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆå®Œäº†: {spreadsheet_id}",
                operation="create_new_api_usage_spreadsheet",
            )
            
            return spreadsheet_id

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"æ–°è¦APIä½¿ç”¨é‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="create_new_api_usage_spreadsheet",
                exc_info=True,
            )
            return None

    def _append_api_usage_data(self, sheets_service, spreadsheet_id: str, data_rows: List[List]):
        """APIä½¿ç”¨é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜"""
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚è¡Œã‚’å–å¾—
            range_name = 'API_Usage_Log!A:I'
            result = sheets_service.spreadsheets().values().get(
                spreadsheetId=spreadsheet_id, 
                range=range_name
            ).execute()
            
            existing_values = result.get('values', [])
            next_row = len(existing_values) + 1
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜
            append_range = f'API_Usage_Log!A{next_row}:I{next_row + len(data_rows) - 1}'
            body = {'values': data_rows}
            
            sheets_service.spreadsheets().values().update(
                spreadsheetId=spreadsheet_id,
                range=append_range,
                valueInputOption='RAW',
                body=body
            ).execute()
            
            log_with_context(
                self.logger,
                logging.INFO,
                f"APIä½¿ç”¨é‡ãƒ‡ãƒ¼ã‚¿è¿½è¨˜å®Œäº†: {len(data_rows)}è¡Œ",
                operation="append_api_usage_data",
            )

        except Exception as e:
            log_with_context(
                self.logger,
                logging.ERROR,
                f"APIä½¿ç”¨é‡ãƒ‡ãƒ¼ã‚¿è¿½è¨˜ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="append_api_usage_data",
                exc_info=True,
            )

    def _generate_google_docs_internal(self, drive_service, docs_service):
        """å†…éƒ¨Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†"""
        # Driveå®¹é‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        debug_drive_storage_info(drive_service)

        # å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            deleted_count = cleanup_old_drive_documents(
                drive_service,
                self.config.google.drive_output_folder_id,
                self.config.google.docs_retention_days,
            )
            log_with_context(
                self.logger,
                logging.INFO,
                f"å¤ã„Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†",
                operation="generate_google_docs_internal",
                deleted_count=deleted_count,
            )
        except Exception as e:
            log_with_context(
                self.logger,
                logging.WARNING,
                f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="generate_google_docs_internal",
            )

        # æ¨©é™ç¢ºèª
        if not test_drive_connection(drive_service, self.config.google.drive_output_folder_id):
            log_with_context(
                self.logger,
                logging.ERROR,
                "Google Driveæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—",
                operation="generate_google_docs_internal",
            )
            return

        # DBã‹ã‚‰éå»24æ™‚é–“åˆ†ã®å…¨è¨˜äº‹ã‚’å–å¾—
        recent_articles = self.db_manager.get_recent_articles_all(
            hours=self.config.scraping.hours_limit
        )

        if not recent_articles:
            log_with_context(
                self.logger,
                logging.WARNING,
                "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå¯¾è±¡ã®è¨˜äº‹ãªã—",
                operation="generate_google_docs_internal",
            )
            return

        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        articles_for_docs = []
        articles_with_summary = []

        for a in recent_articles:
            analysis = a.ai_analysis[0] if a.ai_analysis else None

            # å…¨è¨˜äº‹ç”¨ï¼ˆè¨˜äº‹æœ¬æ–‡å«ã‚€ï¼‰
            article_data = {
                "title": a.title,
                "url": a.url,
                "source": a.source,
                "published_jst": a.published_at,
                "body": a.body,
                "summary": analysis.summary if analysis else None,
                "sentiment_label": analysis.sentiment_label if analysis else "N/A",
                "sentiment_score": analysis.sentiment_score if analysis else 0.0,
            }
            articles_for_docs.append(article_data)

            # AIè¦ç´„ãŒã‚ã‚‹è¨˜äº‹ã®ã¿
            if analysis and analysis.summary:
                articles_with_summary.append(article_data)

        # 1. æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å…¨å‰Šé™¤ãƒ»æ–°è¦è¨˜è¼‰ï¼ˆå…¨è¨˜äº‹æœ¬æ–‡ï¼‰
        if self.config.google.overwrite_doc_id:
            success = update_google_doc_with_full_text(
                docs_service, self.config.google.overwrite_doc_id, articles_for_docs
            )
            if success:
                log_with_context(
                    self.logger,
                    logging.INFO,
                    "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå®Œäº†",
                    operation="generate_google_docs_internal",
                    doc_id=self.config.google.overwrite_doc_id,
                )
            else:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå¤±æ•—",
                    operation="generate_google_docs_internal",
                    doc_id=self.config.google.overwrite_doc_id,
                )

        # 2. AIè¦ç´„ã®æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆå®¹é‡ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ»ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        success = create_daily_summary_doc_with_cleanup_retry(
            drive_service,
            docs_service,
            articles_with_summary,
            self.config.google.drive_output_folder_id,
            self.config.google.docs_retention_days,
        )
        if success:
            log_with_context(
                self.logger,
                logging.INFO,
                "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå®Œäº†",
                operation="generate_google_docs_internal",
                ai_articles=len(articles_with_summary),
            )
        else:
            log_with_context(
                self.logger,
                logging.ERROR,
                "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—",
                operation="generate_google_docs_internal",
            )

    def generate_google_docs(self):
        """Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå‡¦ç†ï¼ˆæ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼‰"""
        log_with_context(
            self.logger,
            logging.INFO,
            "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆé–‹å§‹",
            operation="generate_google_docs",
        )

        # Googleèªè¨¼
        drive_service, docs_service, _ = authenticate_google_services()
        if not drive_service or not docs_service:
            log_with_context(
                self.logger, logging.ERROR, "Googleèªè¨¼ã«å¤±æ•—", operation="generate_google_docs"
            )
            return

        # Driveå®¹é‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        debug_drive_storage_info(drive_service)

        # å¤ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            deleted_count = cleanup_old_drive_documents(
                drive_service,
                self.config.google.drive_output_folder_id,
                self.config.google.docs_retention_days,
            )
            log_with_context(
                self.logger,
                logging.INFO,
                f"å¤ã„Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†",
                operation="generate_google_docs",
                deleted_count=deleted_count,
            )
        except Exception as e:
            log_with_context(
                self.logger,
                logging.WARNING,
                f"Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼: {e}",
                operation="generate_google_docs",
            )

        # æ¨©é™ç¢ºèª
        if not test_drive_connection(drive_service, self.config.google.drive_output_folder_id):
            log_with_context(
                self.logger,
                logging.ERROR,
                "Google Driveæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—",
                operation="generate_google_docs",
            )
            return

        # DBã‹ã‚‰éå»24æ™‚é–“åˆ†ã®å…¨è¨˜äº‹ã‚’å–å¾—
        recent_articles = self.db_manager.get_recent_articles_all(
            hours=self.config.scraping.hours_limit
        )

        if not recent_articles:
            log_with_context(
                self.logger,
                logging.WARNING,
                "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå¯¾è±¡ã®è¨˜äº‹ãªã—",
                operation="generate_google_docs",
            )
            return

        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        articles_for_docs = []
        articles_with_summary = []

        for a in recent_articles:
            analysis = a.ai_analysis[0] if a.ai_analysis else None

            # å…¨è¨˜äº‹ç”¨ï¼ˆè¨˜äº‹æœ¬æ–‡å«ã‚€ï¼‰
            article_data = {
                "title": a.title,
                "url": a.url,
                "source": a.source,
                "published_jst": a.published_at,
                "body": a.body,
                "summary": analysis.summary if analysis else None,
                "sentiment_label": analysis.sentiment_label if analysis else "N/A",
                "sentiment_score": analysis.sentiment_score if analysis else 0.0,
            }
            articles_for_docs.append(article_data)

            # AIè¦ç´„ãŒã‚ã‚‹è¨˜äº‹ã®ã¿
            if analysis and analysis.summary:
                articles_with_summary.append(article_data)

        # 1. æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å…¨å‰Šé™¤ãƒ»æ–°è¦è¨˜è¼‰ï¼ˆå…¨è¨˜äº‹æœ¬æ–‡ï¼‰
        if self.config.google.overwrite_doc_id:
            success = update_google_doc_with_full_text(
                docs_service, self.config.google.overwrite_doc_id, articles_for_docs
            )
            if success:
                log_with_context(
                    self.logger,
                    logging.INFO,
                    "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå®Œäº†",
                    operation="generate_google_docs",
                    doc_id=self.config.google.overwrite_doc_id,
                )
            else:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    "æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸Šæ›¸ãå¤±æ•—",
                    operation="generate_google_docs",
                    doc_id=self.config.google.overwrite_doc_id,
                )

        # 2. AIè¦ç´„ã®æ–°è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆï¼ˆå®¹é‡ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ»ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
        success = create_daily_summary_doc_with_cleanup_retry(
            drive_service,
            docs_service,
            articles_with_summary,
            self.config.google.drive_output_folder_id,
            self.config.google.docs_retention_days,
        )
        if success:
            log_with_context(
                self.logger,
                logging.INFO,
                "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå®Œäº†",
                operation="generate_google_docs",
                ai_articles=len(articles_with_summary),
            )
        else:
            log_with_context(
                self.logger,
                logging.ERROR,
                "æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆå¤±æ•—",
                operation="generate_google_docs",
            )

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ"""
        self.logger.info("=== ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹å–å¾—ãƒ»å‡¦ç†é–‹å§‹ ===")
        overall_start_time = time.time()

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
        session_id = self.db_manager.start_scraping_session()

        try:
            if not self.validate_environment():
                self.db_manager.complete_scraping_session(
                    session_id, status="failed", error_details="ç’°å¢ƒå¤‰æ•°æœªè¨­å®š"
                )
                return

            # 1. è¨˜äº‹åé›†
            scraped_articles = self.collect_articles()
            log_with_context(
                self.logger,
                logging.INFO,
                f"=== è¨˜äº‹åé›†çµæœ: {len(scraped_articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾— ===",
                operation="main_process",
            )

            self.db_manager.update_scraping_session(
                session_id, articles_found=len(scraped_articles)
            )
            if not scraped_articles:
                self.logger.error("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§æ–°ã—ã„è¨˜äº‹ãŒå–å¾—ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                self.db_manager.complete_scraping_session(
                    session_id, status="failed", error_details="No articles scraped"
                )
                raise RuntimeError("è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚½ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®æ§‹é€ å¤‰æ›´ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

            # 2. DBã«ä¿å­˜ (é‡è¤‡æ’é™¤)
            new_article_ids = self.save_articles_to_db(scraped_articles)
            log_with_context(
                self.logger,
                logging.INFO,
                f"=== DBä¿å­˜çµæœ: {len(scraped_articles)}ä»¶ä¸­{len(new_article_ids)}ä»¶ãŒæ–°è¦è¨˜äº‹ ===",
                operation="main_process",
            )

            self.db_manager.update_scraping_session(
                session_id, articles_processed=len(new_article_ids)
            )

            # 3. æ–°è¦è¨˜äº‹ã‚’AIã§å‡¦ç†
            self.process_new_articles_with_ai(new_article_ids)

            # 3.5. AIåˆ†æãŒãªã„24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã‚‚å‡¦ç†ã™ã‚‹
            self.process_recent_articles_without_ai()

            # 3.7. Proçµ±åˆè¦ç´„å‡¦ç†ï¼ˆæ–°è¦è¿½åŠ ï¼‰
            integration_result = None
            pro_integration_start_time = time.time()

            try:
                integration_result = self.process_pro_integration_summaries(
                    session_id, scraped_articles
                )
                if integration_result:
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        "Proçµ±åˆè¦ç´„ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ",
                        operation="main_process",
                    )
                else:
                    log_with_context(
                        self.logger,
                        logging.INFO,
                        "Proçµ±åˆè¦ç´„ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ",
                        operation="main_process",
                    )
            except Exception as e:
                log_with_context(
                    self.logger,
                    logging.ERROR,
                    f"Proçµ±åˆè¦ç´„å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ç¶™ç¶š): {e}",
                    operation="main_process",
                    exc_info=True,
                )
            finally:
                # Proçµ±åˆè¦ç´„ã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°è¨˜éŒ²
                self._log_pro_integration_statistics(
                    integration_result, session_id, len(scraped_articles)
                )
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
                self._monitor_system_performance(pro_integration_start_time, "Proçµ±åˆè¦ç´„å‡¦ç†")

            # 4. ä»Šå›å®Ÿè¡Œåˆ†ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’AIåˆ†æçµæœã¨çµ„ã¿åˆã‚ã›ã¦æº–å‚™
            current_session_articles = self.prepare_current_session_articles_for_html(
                scraped_articles
            )
            log_with_context(
                self.logger,
                logging.INFO,
                f"=== HTMLç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(current_session_articles)}ä»¶ã®è¨˜äº‹ã‚’HTMLã«å‡ºåŠ›äºˆå®š ===",
                operation="main_process",
            )

            # 5. æœ€çµ‚çš„ãªHTMLã‚’ç”Ÿæˆï¼ˆä»Šå›å®Ÿè¡Œåˆ†ã®ã¿ï¼‰
            self.generate_final_html(current_session_articles, session_id)

            # 6. Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”Ÿæˆï¼ˆæ™‚åˆ»æ¡ä»¶æº€ãŸã™å ´åˆã®ã¿ï¼‰
            # ç’°å¢ƒå¤‰æ•°ã§Google Serviceså‡¦ç†ã‚’ON/OFFåˆ¶å¾¡
            import os
            enable_google_services = os.getenv('ENABLE_GOOGLE_SERVICES', 'true').lower() == 'true'
            
            if enable_google_services:
                self.generate_google_docs_and_sheets(session_id, current_session_articles)
            else:
                log_with_context(
                    self.logger,
                    logging.INFO,
                    "ENABLE_GOOGLE_SERVICES=falseã«ã‚ˆã‚ŠGoogle Serviceså‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                    operation="main_process",
                )

            # 7. å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.db_manager.cleanup_old_data(days_to_keep=30)

            self.db_manager.complete_scraping_session(session_id, status="completed_ok")

        except Exception as e:
            self.logger.error(f"å‡¦ç†å…¨ä½“ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
            self.db_manager.complete_scraping_session(
                session_id, status="failed", error_details=str(e)
            )
            raise
        finally:
            overall_elapsed_time = time.time() - overall_start_time

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            self._log_session_summary(
                session_id,
                overall_start_time,
                integration_result if "integration_result" in locals() else None,
            )

            # å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
            self._monitor_system_performance(overall_start_time, "å‡¦ç†å…¨ä½“")

            self.logger.info(
                f"=== å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ (ç·å‡¦ç†æ™‚é–“: {overall_elapsed_time:.2f}ç§’) ==="
            )
