# スクレイピング処理の安定性向上に関する要件定義

## 1. 背景と問題点

現在、本システムのニュース記事スクレイピング処理は、GitHub Actions上の定時実行ワークフローにおいて、頻繁にタイムアウトエラーを発生させ、処理が停止する問題が確認されている。

**エラーログ:**
```
"Retrying (...) after connection broken by 'ReadTimeoutError(...)'"
"Max retries exceeded with url: ... (Caused by ReadTimeoutError(...))"
```

**原因分析:**
- **静的な待機時間:** `scrapers/reuters.py`において、`time.sleep()`による固定時間の待機が使用されている。これは、ページの読み込み速度が変動するサーバー環境では非効率かつ不安定であり、ページの描画が完了する前に次の処理へ進んでしまう、あるいは不要に長く待機してしまう問題がある。
- **ブラウザ通信のタイムアウト:** エラーログの`HTTPConnectionPool(host='localhost', ...)`は、Selenium WebDriverとブラウザ間の内部通信がタイムアウトしていることを示唆している。これは、ウェブページの読み込みに時間がかかりすぎ、ブラウザが応答不能になっていることが主な原因と考えられる。
- **逐次的な本文取得:** 記事一覧を取得した後、各記事の本文を一つずつ順番に`requests.get()`で取得している。記事数が多い場合、この処理が全体の実行時間を大幅に増加させ、GitHub Actionsのジョブタイムアウトのリスクを高めている。

## 2. ゴール

本改善の目的は、スクレイピング処理の堅牢性と効率性を向上させ、GitHub Actionsのワークフローを安定的かつ高速に完了させることである。

- **安定性の確保:** タイムアウトエラーの発生を抑制し、万が一エラーが発生した場合でも処理全体が停止することなく、可能な限り多くの記事を収集できるようにする。
- **処理性能の向上:** 全体の処理時間を現状の半分以下に短縮することを目標とする。

## 3. 機能要件

### 3.1. スクレイピングの安定性向上

- **要件1.1: 動的な待機処理への変更**
  - `time.sleep()`による固定時間待機を撤廃する。
  - 代わりに、Seleniumの`WebDriverWait`と`expected_conditions`を使用し、特定のHTML要素（例：記事リストコンテナ）が描画されるまで動的に待機するロジックに変更する。

- **要件1.2: タイムアウトとエラーハンドリングの強化**
  - `driver.get()`でページ読み込みがタイムアウトした場合に、例外を捕捉し、数回のリトライ処理を実装する。
  - リトライにすべて失敗した場合は、そのページをスキップし、エラーをログに記録した上で、次のページの処理を継続する。

### 3.2. 処理性能の向上

- **要件2.1: 記事本文取得の並列化**
  - 現在の逐次的な本文取得処理を、並列処理に変更する。
  - `concurrent.futures.ThreadPoolExecutor`などを利用し、複数の記事本文を同時に取得することで、I/O待機時間を大幅に削減する。

### 3.3. 設定の柔軟性向上

- **要件3.1: 主要パラメータの設定ファイルへの移行**
  - Seleniumのタイムアウト時間、リトライ回数、並列処理のスレッド数などの主要なパラメータを、`src/config/app_config.py`で設定可能にする。
  - これにより、将来的な環境の変化やチューニングの際に、コードを直接編集することなく柔軟に対応できるようになる。

## 4. 非機能要件

- **保守性:**
  - コードの可読性を高め、各処理の役割分担を明確にする。
  - ロイターやブルームバーグなど、各スクレイパーのロジックをより独立させ、一方のサイト構造変更が他方に影響を与えにくい設計を目指す。
- **堅牢性:**
  - 特定の記事の取得や解析に失敗しても、その記事をスキップして処理を継続する。
  - 処理の開始、終了、主要なステップ、エラー発生時には、詳細なログを出力する。

## 5. 実装計画（案）

本改善は、以下のステップで段階的に実施することを推奨する。

- **ステップ1: 待機処理の改善とエラーハンドリング強化（最優先）**
  - `scrapers/reuters.py`の`time.sleep()`を`WebDriverWait`に置き換える。
  - ページ読み込み部分に`try-except`ブロックを追加し、タイムアウト時のリトライとスキップ処理を実装する。

- **ステップ2: 設定の外部化**
  - タイムアウト値やリトライ回数を`src/config/app_config.py`に追加し、スクレイパーがそれを参照するように修正する。

- **ステップ3: 本文取得の並列化**
  - 記事URLのリストを収集した後、`ThreadPoolExecutor`を使って本文取得を並列化するロジックを実装する。
