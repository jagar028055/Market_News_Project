# -*- coding: utf-8 -*-

"""
å‹•çš„è¨˜äº‹å–å¾—æ©Ÿèƒ½ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timedelta
import pytz
from src.core.news_processor import NewsProcessor
from src.config.app_config import AppConfig, ScrapingConfig


class TestDynamicArticleCollection(unittest.TestCase):
    """å‹•çš„è¨˜äº‹å–å¾—æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    
    def setUp(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.processor = NewsProcessor()
        self.processor.logger = Mock()
        
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        self.processor.config.scraping = ScrapingConfig(
            hours_limit=24,
            minimum_article_count=100,
            max_hours_limit=72,
            weekend_hours_extension=48
        )
    
    def test_get_dynamic_hours_limit_monday(self):
        """æœˆæ›œæ—¥ã®ãƒ†ã‚¹ãƒˆ: è‡ªå‹•çš„ã«æœ€å¤§æ™‚é–“ç¯„å›²ã‚’è¿”ã™"""
        with patch('src.core.news_processor.datetime') as mock_datetime, \
             patch('src.core.news_processor.pytz') as mock_pytz:
            
            # æœˆæ›œæ—¥ã®ãƒ¢ãƒƒã‚¯è¨­å®š
            mock_jst_now = Mock()
            mock_jst_now.weekday.return_value = 0  # Monday
            mock_pytz.timezone.return_value.localize.return_value = mock_jst_now
            mock_datetime.now.return_value = mock_jst_now
            
            result = self.processor.get_dynamic_hours_limit()
            
            self.assertEqual(result, 72)  # max_hours_limit
            self.processor.logger.info.assert_called_with(
                "æœˆæ›œæ—¥æ¤œå‡º: è‡ªå‹•çš„ã«72æ™‚é–“ç¯„å›²ã‚’é©ç”¨"
            )
    
    def test_get_dynamic_hours_limit_weekday(self):
        """å¹³æ—¥ï¼ˆç«-é‡‘ï¼‰ã®ãƒ†ã‚¹ãƒˆ: åŸºæœ¬æ™‚é–“ç¯„å›²ã‚’è¿”ã™"""
        with patch('src.core.news_processor.datetime') as mock_datetime, \
             patch('src.core.news_processor.pytz') as mock_pytz:
            
            # ç«æ›œæ—¥ã®ãƒ¢ãƒƒã‚¯è¨­å®š
            mock_jst_now = Mock()
            mock_jst_now.weekday.return_value = 1  # Tuesday
            mock_pytz.timezone.return_value.localize.return_value = mock_jst_now
            mock_datetime.now.return_value = mock_jst_now
            
            result = self.processor.get_dynamic_hours_limit()
            
            self.assertEqual(result, 24)  # hours_limit
    
    @patch('src.core.news_processor.NewsProcessor._collect_articles_with_hours')
    def test_collect_articles_with_dynamic_range_sufficient_articles(self, mock_collect):
        """ååˆ†ãªè¨˜äº‹æ•°ã®å ´åˆã®ãƒ†ã‚¹ãƒˆ"""
        # ååˆ†ãªè¨˜äº‹æ•°ã‚’ãƒ¢ãƒƒã‚¯
        mock_articles = [{'title': f'Article {i}'} for i in range(120)]
        mock_collect.return_value = mock_articles
        
        with patch.object(self.processor, 'get_dynamic_hours_limit', return_value=24):
            result = self.processor.collect_articles_with_dynamic_range()
            
            self.assertEqual(len(result), 120)
            mock_collect.assert_called_once_with(24)
            self.processor.logger.info.assert_any_call("âœ… æˆåŠŸ: æœ€ä½è¨˜äº‹æ•°(100ä»¶)ã‚’æº€ãŸã—ã¾ã—ãŸ")
    
    @patch('src.core.news_processor.NewsProcessor._collect_articles_with_hours')
    def test_collect_articles_with_dynamic_range_insufficient_articles(self, mock_collect):
        """è¨˜äº‹æ•°ä¸è¶³ã®å ´åˆã®æ™‚é–“ç¯„å›²æ‹¡å¼µãƒ†ã‚¹ãƒˆ"""
        # æœ€åˆã¯å°‘ãªã„è¨˜äº‹æ•°ã€æ¬¡ã«ååˆ†ãªè¨˜äº‹æ•°
        mock_collect.side_effect = [
            [{'title': f'Article {i}'} for i in range(50)],  # 50ä»¶ï¼ˆä¸è¶³ï¼‰
            [{'title': f'Article {i}'} for i in range(120)]  # 120ä»¶ï¼ˆååˆ†ï¼‰
        ]
        
        with patch.object(self.processor, 'get_dynamic_hours_limit', return_value=24):
            result = self.processor.collect_articles_with_dynamic_range()
            
            self.assertEqual(len(result), 120)
            self.assertEqual(mock_collect.call_count, 2)
            mock_collect.assert_any_call(24)  # æœ€åˆã®å‘¼ã³å‡ºã—
            mock_collect.assert_any_call(48)  # æ‹¡å¼µå¾Œã®å‘¼ã³å‡ºã—
            
            self.processor.logger.info.assert_any_call(
                "ğŸ“ˆ è¨˜äº‹æ•°ä¸è¶³ â†’ æ™‚é–“ç¯„å›²æ‹¡å¼µ: 24æ™‚é–“ â†’ 48æ™‚é–“"
            )
    
    @patch('src.core.news_processor.NewsProcessor._collect_articles_with_hours')
    def test_collect_articles_with_dynamic_range_max_limit_reached(self, mock_collect):
        """æœ€å¤§æ™‚é–“ç¯„å›²ã«åˆ°é”ã™ã‚‹å ´åˆã®ãƒ†ã‚¹ãƒˆ"""
        # å¸¸ã«è¨˜äº‹æ•°ä¸è¶³
        mock_articles = [{'title': f'Article {i}'} for i in range(30)]
        mock_collect.return_value = mock_articles
        
        with patch.object(self.processor, 'get_dynamic_hours_limit', return_value=24):
            result = self.processor.collect_articles_with_dynamic_range()
            
            self.assertEqual(len(result), 30)
            # 24h â†’ 48h â†’ 72h ã®3å›å‘¼ã³å‡ºã—
            self.assertEqual(mock_collect.call_count, 3)
            mock_collect.assert_any_call(72)  # æœ€çµ‚çš„ã«æœ€å¤§å€¤
            
            self.processor.logger.warning.assert_any_call(
                "âš ï¸  æœ€å¤§æ™‚é–“ç¯„å›²(72æ™‚é–“)ã«åˆ°é”"
            )
            self.processor.logger.warning.assert_any_call(
                "   æœ€çµ‚è¨˜äº‹æ•°: 30ä»¶ (ç›®æ¨™: 100ä»¶)"
            )
    
    @patch('scrapers.reuters.scrape_reuters_articles')
    @patch('scrapers.bloomberg.scrape_bloomberg_top_page_articles')
    def test_collect_articles_with_hours(self, mock_bloomberg, mock_reuters):
        """æŒ‡å®šæ™‚é–“ç¯„å›²ã§ã®è¨˜äº‹åé›†ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯è¨˜äº‹ã‚’è¨­å®š
        mock_reuters_articles = [
            {'title': 'Reuters 1', 'published_jst': datetime.now()},
            {'title': 'Reuters 2', 'published_jst': datetime.now() - timedelta(hours=1)}
        ]
        mock_bloomberg_articles = [
            {'title': 'Bloomberg 1', 'published_jst': datetime.now() - timedelta(hours=2)}
        ]
        
        mock_reuters.return_value = mock_reuters_articles
        mock_bloomberg.return_value = mock_bloomberg_articles
        
        result = self.processor._collect_articles_with_hours(48)
        
        self.assertEqual(len(result), 3)
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãŒæ­£ã—ã„å¼•æ•°ã§å‘¼ã°ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
        mock_reuters.assert_called_once_with(
            query=self.processor.config.reuters.query,
            hours_limit=48,
            max_pages=self.processor.config.reuters.max_pages,
            items_per_page=self.processor.config.reuters.items_per_page,
            target_categories=self.processor.config.reuters.target_categories,
            exclude_keywords=self.processor.config.reuters.exclude_keywords
        )
        
        mock_bloomberg.assert_called_once_with(
            hours_limit=48,
            exclude_keywords=self.processor.config.bloomberg.exclude_keywords
        )
        
        # ãƒ­ã‚°å‡ºåŠ›ã®ç¢ºèªã¯ã€log_with_contextã®ãƒ¢ãƒƒã‚¯ãŒè¤‡é›‘ãªãŸã‚ã€ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯çœç•¥ã™ã‚‹
    
    @patch('src.core.news_processor.log_with_context')
    @patch('scrapers.reuters.scrape_reuters_articles')
    @patch('scrapers.bloomberg.scrape_bloomberg_top_page_articles')
    def test_collect_articles_with_hours_error_handling(self, mock_bloomberg, mock_reuters, mock_log):
        """è¨˜äº‹åé›†ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
        import logging
        # ãƒ­ã‚¤ã‚¿ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
        mock_reuters.side_effect = Exception("Reuters Error")
        mock_bloomberg.return_value = [{'title': 'Bloomberg 1'}]
        
        result = self.processor._collect_articles_with_hours(24)
        
        self.assertEqual(len(result), 1)  # Bloombergã®è¨˜äº‹ã®ã¿

        # log_with_contextãŒã‚¨ãƒ©ãƒ¼æƒ…å ±ã¨ã¨ã‚‚ã«å‘¼ã°ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        mock_log.assert_any_call(
            self.processor.logger,
            logging.ERROR,
            "Reuters è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼",
            operation="collect_articles",
            scraper="Reuters",
            error="Reuters Error",
            exc_info=True
        )
    
    def test_integration_collect_articles(self):
        """collect_articlesãƒ¡ã‚½ãƒƒãƒ‰ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
        with patch.object(self.processor, 'collect_articles_with_dynamic_range') as mock_dynamic:
            mock_dynamic.return_value = [{'title': 'Test Article'}]
            
            result = self.processor.collect_articles()
            
            self.assertEqual(len(result), 1)
            mock_dynamic.assert_called_once()


class TestScrapingConfigExtension(unittest.TestCase):
    """ScrapingConfigæ‹¡å¼µã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_scraping_config_defaults(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ãƒ†ã‚¹ãƒˆ"""
        config = ScrapingConfig()
        
        self.assertEqual(config.hours_limit, 24)
        self.assertEqual(config.minimum_article_count, 100)
        self.assertEqual(config.max_hours_limit, 72)
        self.assertEqual(config.weekend_hours_extension, 48)
    
    def test_scraping_config_custom_values(self):
        """ã‚«ã‚¹ã‚¿ãƒ å€¤ã®ãƒ†ã‚¹ãƒˆ"""
        config = ScrapingConfig(
            hours_limit=12,
            minimum_article_count=50,
            max_hours_limit=96,
            weekend_hours_extension=60
        )
        
        self.assertEqual(config.hours_limit, 12)
        self.assertEqual(config.minimum_article_count, 50)
        self.assertEqual(config.max_hours_limit, 96)
        self.assertEqual(config.weekend_hours_extension, 60)
    
    @patch.dict('os.environ', {
        'SCRAPING_MINIMUM_ARTICLE_COUNT': '150',
        'SCRAPING_MAX_HOURS_LIMIT': '96',
        'SCRAPING_WEEKEND_HOURS_EXTENSION': '60'
    })
    def test_app_config_environment_override(self):
        """ç’°å¢ƒå¤‰æ•°ã§ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        config = AppConfig()
        
        self.assertEqual(config.scraping.minimum_article_count, 150)
        self.assertEqual(config.scraping.max_hours_limit, 96)
        self.assertEqual(config.scraping.weekend_hours_extension, 60)


if __name__ == '__main__':
    unittest.main()