# スクレイピング処理時間長期化調査レポート

## 調査概要

**調査期間**: 2025年7月19日  
**対象期間**: 2024年7月17日前後のスクレイピング処理時間の変化  
**調査対象**: GitHub Actions上でのMarket Newsスクレイピング処理  

## 問題の概要

GitHub Actions上のワークフローにおいて、スクレイピング処理時間が以下のように変化：
- **7月17日まで**: 約3分で完了
- **7月17日以降**: 約10分かかるように延長（約3.3倍の増加）

## 調査結果

### 1. 根本原因の特定

**主要原因**: 2025年7月19日のコミット「feat: スクレイピング処理の安定性と性能を向上」（コミットID: 1429163）において、安定性向上を目的とした設定変更が処理時間の大幅な増加を招いた。

### 2. 具体的な変更内容と影響

#### 2.1 タイムアウト設定の大幅増加
```python
# 変更前（推定）
selenium_timeout: 45秒

# 変更後（現在）
selenium_timeout: 120秒  # 2.7倍に増加
```

#### 2.2 リトライメカニズムの追加
```python
# 新規追加
selenium_max_retries: 3回  # 最大3回のリトライ処理
```

**影響計算**:
- 最悪ケース: 120秒 × 4回（初回 + リトライ3回）= 480秒/ページ
- Bloomberg: 複数ページ処理で最大24分（480秒 × 3ページ）
- Reuters: 検索結果5ページで最大40分（480秒 × 5ページ）

#### 2.3 動的待機時間の実装
- 固定待機時間から可変待機時間に変更
- JavaScript読み込み完了まで最大120秒待機

#### 2.4 設定参照エラー
BloombergスクレイパーでReutersの並列処理設定を誤って参照している可能性（コード分析より推定）

### 3. プロジェクト構造の分析

#### 3.1 スクレイピング対象サイト
1. **Bloomberg Japan** (`scrapers/bloomberg.py`)
   - Selenium WebDriver使用
   - トップページから記事リスト取得
   
2. **Reuters Japan** (`scrapers/reuters.py`)
   - Selenium WebDriver使用  
   - サイト内検索機能利用
   - 最大5ページ、並列5スレッドで記事本文取得

#### 3.2 現在の設定値
```python
@dataclass
class ScrapingConfig:
    hours_limit: int = 24
    selenium_timeout: int = 120      # ← 問題の設定
    selenium_max_retries: int = 3    # ← 問題の設定

@dataclass  
class ReutersConfig:
    max_pages: int = 5
    items_per_page: int = 20
    num_parallel_requests: int = 5
```

### 4. GitHub Actions環境の分析

#### 4.1 実行スケジュール
- **頻度**: 1日3回（JST 7:00, 13:00, 23:00）
- **環境**: Ubuntu latest, Python 3.11
- **制限**: 1実行あたり最大6時間、月2000分の無料枠

#### 4.2 処理フロー
1. スクレイピング（Bloomberg + Reuters）
2. AI感情分析（Gemini API）
3. HTML生成
4. Google Docs出力
5. GitHub Pages デプロイ

## 対策案

### 即座に実施すべき緊急対策

#### 1. タイムアウト値の最適化
```python
# 推奨設定
selenium_timeout: 30秒     # 120秒 → 30秒に短縮
selenium_max_retries: 2    # 3回 → 2回に削減
```

#### 2. 段階的タイムアウトの実装
```python
# 初回アクセス: 15秒
# リトライ時: 30秒
# 最終リトライ: 60秒
```

#### 3. 並列処理の最適化
```python
# Reuters
num_parallel_requests: 8   # 5 → 8に増加

# Bloomberg用設定の追加
@dataclass
class BloombergConfig:
    num_parallel_requests: int = 6
```

### 中長期的な改善案

#### 1. エラーハンドリングの改善
- 早期スキップ条件の追加
- 部分的失敗時の継続処理
- 段階的バックオフの実装

#### 2. キャッシング機能の導入
- 過去24時間の記事重複チェック
- 記事URLベースのキャッシュ

#### 3. 処理時間の可視化
- 各ステップの実行時間計測
- ボトルネック特定のためのログ改善

## 推定効果

### 緊急対策実施後の予想処理時間
- **Bloomberg**: 3-4分（現在: 8-12分）
- **Reuters**: 4-6分（現在: 10-15分）
- **合計**: 7-10分（現在: 18-27分）

**改善率**: 約60-70%の処理時間短縮

### リスク分析
- **安定性**: タイムアウト短縮により一部記事の取得失敗率が増加する可能性
- **品質**: 処理速度優先により記事の完全性に影響する可能性

## 結論

スクレイピング処理時間の長期化は、安定性向上を目的とした設定変更が原因でした。適切なタイムアウト値の調整と並列処理の最適化により、処理時間を大幅に短縮できる見込みです。

## 推奨実施順序

1. **即座実施**: タイムアウト値とリトライ回数の調整
2. **1週間以内**: 並列処理設定の最適化
3. **2週間以内**: エラーハンドリングとログ改善
4. **1ヶ月以内**: キャッシング機能の導入

---

**報告者**: Claude Code  
**作成日**: 2025年7月19日