# AIå˜ç‹¬å®Ÿè£…ã‚¿ã‚¹ã‚¯ - è©³ç´°è¦ä»¶å®šç¾©æ›¸

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€AIï¼ˆClaudeï¼‰ãŒå˜ç‹¬ã§å®Ÿè£…å¯èƒ½ãªã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€è©³ç´°ãªè¦ä»¶å®šç¾©ã¨æŠ€è¡“ä»•æ§˜ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

---

## ğŸ—ï¸ 1. ã‚³ãƒ¼ãƒ‰å“è³ªæ”¹å–„

### 1.1 å‹ãƒ’ãƒ³ãƒˆå®Œå…¨åŒ–

#### **è¦ä»¶**
- ã™ã¹ã¦ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`.py`ï¼‰ã«å®Œå…¨ãªå‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
- Python 3.8+ ã®å‹ãƒ’ãƒ³ãƒˆæ§‹æ–‡ã‚’ä½¿ç”¨
- mypyé™çš„å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãªã—ã«ãªã‚‹ã“ã¨

#### **æŠ€è¡“ä»•æ§˜**
```python
# å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
- main.py
- config.py
- ai_summarizer.py
- html_generator.py
- scrapers/reuters.py
- scrapers/bloomberg.py
- gdocs/client.py
- gdocs/__init__.py
- scrapers/__init__.py

# å‹ãƒ’ãƒ³ãƒˆè¦ä»¶
from typing import List, Dict, Optional, Union, Callable, Tuple, Any
from datetime import datetime
from pathlib import Path

# é–¢æ•°ã®å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹
def process_article_with_ai(
    api_key: str, 
    text: str
) -> Optional[Dict[str, Any]]:
    pass

# ã‚¯ãƒ©ã‚¹ã®å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹
class ArticleData:
    title: str
    url: str
    published_jst: Optional[datetime]
    body: str
    summary: Optional[str]
    sentiment_label: Optional[str]
    sentiment_score: Optional[float]
```

#### **å®Ÿè£…åŸºæº–**
- é–¢æ•°ã®å¼•æ•°ãƒ»æˆ»ã‚Šå€¤ã™ã¹ã¦ã«å‹ãƒ’ãƒ³ãƒˆ
- ã‚¯ãƒ©ã‚¹å±æ€§ã«ã‚‚å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- Unionå‹ã€Optionalå‹ã®é©åˆ‡ãªä½¿ç”¨
- è¤‡é›‘ãªå‹ã¯TypeAliasä½¿ç”¨ã‚’æ¤œè¨

#### **æ¤œè¨¼æ–¹æ³•**
```bash
# mypyå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ã‚¼ãƒ­
mypy main.py config.py ai_summarizer.py html_generator.py scrapers/ gdocs/

# å‹ã‚«ãƒãƒ¬ãƒƒã‚¸100%ã‚’ç›®æ¨™
mypy --html-report mypy_report .
```

---

### 1.2 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–

#### **è¦ä»¶**
- æ±ç”¨çš„ãª`except Exception`ã‚’å…·ä½“çš„ãªä¾‹å¤–ã‚¯ãƒ©ã‚¹ã«ç½®ãæ›ãˆ
- ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½å®Ÿè£…ï¼ˆexponential backoffï¼‰
- æ§‹é€ åŒ–ãƒ­ã‚°ï¼ˆJSONå½¢å¼ï¼‰å°å…¥
- ã‚¨ãƒ©ãƒ¼çŠ¶æ³ã®è©³ç´°è¨˜éŒ²

#### **æŠ€è¡“ä»•æ§˜**

##### **1.2.1 å…·ä½“çš„ä¾‹å¤–å‡¦ç†**
```python
# ç¾åœ¨ï¼ˆæ”¹å–„å‰ï¼‰
try:
    response = requests.get(url)
except Exception as e:
    print(f"ã‚¨ãƒ©ãƒ¼: {e}")

# æ”¹å–„å¾Œ
try:
    response = requests.get(url, timeout=30)
    response.raise_for_status()
except requests.exceptions.Timeout:
    logger.error("Request timeout", extra={"url": url, "timeout": 30})
except requests.exceptions.HTTPError as e:
    logger.error("HTTP error", extra={"url": url, "status_code": e.response.status_code})
except requests.exceptions.RequestException as e:
    logger.error("Request failed", extra={"url": url, "error": str(e)})
```

##### **1.2.2 ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½**
```python
import time
import random
from functools import wraps

def retry_with_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exceptions: Tuple = (Exception,)
):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    if attempt == max_retries:
                        raise
                    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
                    logger.warning(f"Retry {attempt + 1}/{max_retries} after {delay:.2f}s", 
                                 extra={"function": func.__name__, "error": str(e)})
                    time.sleep(delay)
        return wrapper
    return decorator
```

##### **1.2.3 æ§‹é€ åŒ–ãƒ­ã‚°**
```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        handler = logging.StreamHandler()
        handler.setFormatter(self.JSONFormatter())
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    class JSONFormatter(logging.Formatter):
        def format(self, record):
            log_entry = {
                "timestamp": datetime.utcnow().isoformat(),
                "level": record.levelname,
                "logger": record.name,
                "message": record.getMessage(),
                "module": record.module,
                "function": record.funcName,
                "line": record.lineno
            }
            if hasattr(record, 'extra_data'):
                log_entry.update(record.extra_data)
            return json.dumps(log_entry, ensure_ascii=False)
```

#### **å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»æ©Ÿèƒ½**
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡ï¼ˆrequests, seleniumï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«I/Oæ“ä½œ
- APIå‘¼ã³å‡ºã—ï¼ˆGemini, Google APIsï¼‰
- ãƒ‡ãƒ¼ã‚¿è§£æå‡¦ç†

---

### 1.3 è¨­å®šç®¡ç†æ”¹å–„ï¼ˆPydanticåŒ–ï¼‰

#### **è¦ä»¶**
- `config.py`ã‚’Pydanticãƒ™ãƒ¼ã‚¹ã®è¨­å®šã‚¯ãƒ©ã‚¹ã«å¤‰æ›´
- ç’°å¢ƒå¤‰æ•°ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–
- è¨­å®šå€¤ã®å‹å®‰å…¨æ€§ç¢ºä¿
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨å¿…é ˆé …ç›®ã®æ˜ç¢ºåŒ–

#### **æŠ€è¡“ä»•æ§˜**

##### **1.3.1 Pydanticè¨­å®šã‚¯ãƒ©ã‚¹**
```python
from pydantic import BaseSettings, Field, validator
from typing import List, Optional
from pathlib import Path

class ScrapingConfig(BaseSettings):
    hours_limit: int = Field(24, ge=1, le=168, description="è¨˜äº‹åé›†æ™‚é–“ç¯„å›²ï¼ˆæ™‚é–“ï¼‰")
    sentiment_analysis_enabled: bool = Field(True, description="æ„Ÿæƒ…åˆ†ææœ‰åŠ¹åŒ–")
    
    class Config:
        env_prefix = "SCRAPING_"

class ReutersConfig(BaseSettings):
    query: str = Field(..., description="æ¤œç´¢ã‚¯ã‚¨ãƒª")
    max_pages: int = Field(5, ge=1, le=20)
    items_per_page: int = Field(20, ge=10, le=100)
    target_categories: List[str] = Field(default_factory=list)
    exclude_keywords: List[str] = Field(default_factory=list)
    
    @validator('query')
    def query_not_empty(cls, v):
        if not v.strip():
            raise ValueError('æ¤œç´¢ã‚¯ã‚¨ãƒªã¯ç©ºã«ã§ãã¾ã›ã‚“')
        return v.strip()

class GoogleConfig(BaseSettings):
    drive_output_folder_id: str = Field(..., description="Google Driveå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ID")
    overwrite_doc_id: Optional[str] = Field(None, description="ä¸Šæ›¸ãæ›´æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID")
    service_account_json: str = Field(..., description="ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSON")
    
    @validator('service_account_json')
    def validate_json(cls, v):
        try:
            json.loads(v)
            return v
        except json.JSONDecodeError:
            raise ValueError('ç„¡åŠ¹ãªJSONå½¢å¼ã§ã™')

class AIConfig(BaseSettings):
    gemini_api_key: str = Field(..., description="Gemini APIã‚­ãƒ¼")
    model_name: str = Field("gemini-2.0-flash-lite-001", description="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«")
    max_output_tokens: int = Field(1024, ge=256, le=8192)
    temperature: float = Field(0.2, ge=0.0, le=2.0)
    
    class Config:
        env_prefix = "AI_"

class AppConfig(BaseSettings):
    scraping: ScrapingConfig = ScrapingConfig()
    reuters: ReutersConfig = ReutersConfig()
    bloomberg: BloombergConfig = BloombergConfig()
    google: GoogleConfig = GoogleConfig()
    ai: AIConfig = AIConfig()
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
```

##### **1.3.2 è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ **
```
config/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py          # åŸºæœ¬è¨­å®šã‚¯ãƒ©ã‚¹
â”œâ”€â”€ scraping.py      # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
â”œâ”€â”€ ai.py           # AIé–¢é€£è¨­å®š
â”œâ”€â”€ google.py       # Google APIsè¨­å®š
â””â”€â”€ validation.py   # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼
```

---

### 1.4 é–¢æ•°åˆ†å‰²ãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

#### **è¦ä»¶**
- é•·å¤§ãªé–¢æ•°ï¼ˆ50è¡Œä»¥ä¸Šï¼‰ã‚’é©åˆ‡ãªå˜ä½ã«åˆ†å‰²
- å˜ä¸€è²¬ä»»åŸå‰‡ã®é©ç”¨
- é–¢æ•°ã®å¯èª­æ€§ãƒ»ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£å‘ä¸Š
- ã‚³ãƒ¼ãƒ‰é‡è¤‡ã®é™¤å»

#### **æŠ€è¡“ä»•æ§˜**

##### **1.4.1 main.py ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**
```python
# ç¾åœ¨ã®main()é–¢æ•°ã‚’ä»¥ä¸‹ã«åˆ†å‰²

class NewsAggregator:
    def __init__(self, config: AppConfig):
        self.config = config
        self.logger = StructuredLogger(__name__)
    
    async def run(self) -> None:
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œå‡¦ç†"""
        await self._validate_environment()
        articles = await self._collect_articles()
        processed_articles = await self._process_articles_with_ai(articles)
        await self._generate_outputs(processed_articles)
    
    async def _validate_environment(self) -> None:
        """ç’°å¢ƒè¨­å®šæ¤œè¨¼"""
        pass
    
    async def _collect_articles(self) -> List[ArticleData]:
        """è¨˜äº‹åé›†"""
        pass
    
    async def _process_articles_with_ai(self, articles: List[ArticleData]) -> List[ProcessedArticle]:
        """AIå‡¦ç†"""
        pass
    
    async def _generate_outputs(self, articles: List[ProcessedArticle]) -> None:
        """å‡ºåŠ›ç”Ÿæˆ"""
        pass
```

##### **1.4.2 ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**
```python
# åŸºåº•ã‚¯ãƒ©ã‚¹ä½œæˆ
from abc import ABC, abstractmethod

class NewsScraper(ABC):
    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.logger = StructuredLogger(self.__class__.__name__)
    
    @abstractmethod
    async def scrape_articles(self) -> List[ArticleData]:
        """è¨˜äº‹åé›†ã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass
    
    def _setup_webdriver(self) -> webdriver.Chrome:
        """WebDriverè¨­å®šã®å…±é€šåŒ–"""
        pass
    
    def _parse_article_time(self, time_str: str) -> Optional[datetime]:
        """æ—¥æ™‚è§£æã®å…±é€šåŒ–"""
        pass

class ReutersScraper(NewsScraper):
    async def scrape_articles(self) -> List[ArticleData]:
        pass

class BloombergScraper(NewsScraper):
    async def scrape_articles(self) -> List[ArticleData]:
        pass
```

---

## ğŸ—„ï¸ 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½

### 2.1 SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å°å…¥

#### **è¦ä»¶**
- è»½é‡ãªSQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šåŒ–
- é‡è¤‡è¨˜äº‹ã®åŠ¹ç‡çš„ãªæ¤œå‡ºãƒ»æ’é™¤
- éå»ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒãƒ»åˆ†ææ©Ÿèƒ½
- ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½

#### **æŠ€è¡“ä»•æ§˜**

##### **2.1.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ**
```sql
-- articles ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE NOT NULL,
    url_hash TEXT UNIQUE NOT NULL,  -- URLæ­£è¦åŒ–å¾Œã®ãƒãƒƒã‚·ãƒ¥
    title TEXT NOT NULL,
    body TEXT,
    source TEXT NOT NULL,  -- 'Reuters', 'Bloomberg'
    category TEXT,
    published_at DATETIME,
    scraped_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    content_hash TEXT,  -- æœ¬æ–‡ã®ãƒãƒƒã‚·ãƒ¥ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰
    
    -- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    INDEX idx_url_hash (url_hash),
    INDEX idx_content_hash (content_hash),
    INDEX idx_published_at (published_at),
    INDEX idx_source (source)
);

-- ai_analysis ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE ai_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    article_id INTEGER NOT NULL,
    summary TEXT,
    sentiment_label TEXT,  -- 'Positive', 'Negative', 'Neutral'
    sentiment_score REAL,
    analyzed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    model_version TEXT,  -- ä½¿ç”¨ã—ãŸAIãƒ¢ãƒ‡ãƒ«
    
    FOREIGN KEY (article_id) REFERENCES articles(id) ON DELETE CASCADE,
    INDEX idx_article_id (article_id),
    INDEX idx_sentiment_label (sentiment_label),
    INDEX idx_analyzed_at (analyzed_at)
);

-- scraping_sessions ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE scraping_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    started_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    articles_found INTEGER DEFAULT 0,
    articles_processed INTEGER DEFAULT 0,
    errors_count INTEGER DEFAULT 0,
    status TEXT DEFAULT 'running'  -- 'running', 'completed', 'failed'
);
```

##### **2.1.2 ORMãƒ¢ãƒ‡ãƒ«ï¼ˆSQLAlchemyï¼‰**
```python
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, Float, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from datetime import datetime

Base = declarative_base()

class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    url = Column(String, unique=True, nullable=False)
    url_hash = Column(String, unique=True, nullable=False)
    title = Column(String, nullable=False)
    body = Column(Text)
    source = Column(String, nullable=False)
    category = Column(String)
    published_at = Column(DateTime)
    scraped_at = Column(DateTime, default=datetime.utcnow)
    content_hash = Column(String)
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    ai_analysis = relationship("AIAnalysis", back_populates="article", cascade="all, delete-orphan")

class AIAnalysis(Base):
    __tablename__ = 'ai_analysis'
    
    id = Column(Integer, primary_key=True)
    article_id = Column(Integer, ForeignKey('articles.id'), nullable=False)
    summary = Column(Text)
    sentiment_label = Column(String)
    sentiment_score = Column(Float)
    analyzed_at = Column(DateTime, default=datetime.utcnow)
    model_version = Column(String)
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    article = relationship("Article", back_populates="ai_analysis")

class ScrapingSession(Base):
    __tablename__ = 'scraping_sessions'
    
    id = Column(Integer, primary_key=True)
    started_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)
    articles_found = Column(Integer, default=0)
    articles_processed = Column(Integer, default=0)
    errors_count = Column(Integer, default=0)
    status = Column(String, default='running')
```

##### **2.1.3 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã‚¯ãƒ©ã‚¹**
```python
class DatabaseManager:
    def __init__(self, db_path: str = "market_news.db"):
        self.engine = create_engine(f"sqlite:///{db_path}")
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)
    
    def get_session(self):
        return self.SessionLocal()
    
    def save_article(self, article_data: dict) -> Optional[Article]:
        """è¨˜äº‹ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        with self.get_session() as session:
            # URLæ­£è¦åŒ–ã¨ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
            normalized_url = self._normalize_url(article_data['url'])
            url_hash = hashlib.sha256(normalized_url.encode()).hexdigest()
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            existing = session.query(Article).filter_by(url_hash=url_hash).first()
            if existing:
                return existing
            
            # æ–°è¦è¨˜äº‹ä¿å­˜
            article = Article(
                url=article_data['url'],
                url_hash=url_hash,
                title=article_data['title'],
                body=article_data.get('body', ''),
                source=article_data['source'],
                category=article_data.get('category'),
                published_at=article_data.get('published_jst'),
                content_hash=self._generate_content_hash(article_data.get('body', ''))
            )
            session.add(article)
            session.commit()
            return article
    
    def save_ai_analysis(self, article_id: int, analysis_data: dict) -> AIAnalysis:
        """AIåˆ†æçµæœä¿å­˜"""
        with self.get_session() as session:
            analysis = AIAnalysis(
                article_id=article_id,
                summary=analysis_data.get('summary'),
                sentiment_label=analysis_data.get('sentiment_label'),
                sentiment_score=analysis_data.get('sentiment_score'),
                model_version=analysis_data.get('model_version', 'gemini-2.0-flash-lite-001')
            )
            session.add(analysis)
            session.commit()
            return analysis
    
    def get_recent_articles(self, hours: int = 24) -> List[Article]:
        """æœ€æ–°è¨˜äº‹å–å¾—"""
        with self.get_session() as session:
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            return session.query(Article).filter(
                Article.published_at >= cutoff_time
            ).order_by(Article.published_at.desc()).all()
    
    def detect_duplicate_articles(self) -> List[Tuple[Article, Article]]:
        """é‡è¤‡è¨˜äº‹æ¤œå‡º"""
        with self.get_session() as session:
            # åŒä¸€content_hashã®è¨˜äº‹ã‚’æ¤œå‡º
            duplicates = session.query(Article).filter(
                Article.content_hash.in_(
                    session.query(Article.content_hash).group_by(Article.content_hash).having(func.count() > 1)
                )
            ).order_by(Article.content_hash, Article.scraped_at).all()
            
            # ãƒšã‚¢ã¨ã—ã¦è¿”ã™
            result = []
            current_hash = None
            current_group = []
            
            for article in duplicates:
                if article.content_hash != current_hash:
                    if len(current_group) > 1:
                        for i in range(len(current_group) - 1):
                            result.append((current_group[0], current_group[i + 1]))
                    current_hash = article.content_hash
                    current_group = [article]
                else:
                    current_group.append(article)
            
            return result
```

---

### 2.2 è¨˜äº‹é‡è¤‡æ’é™¤æ©Ÿèƒ½

#### **è¦ä»¶**
- URLæ­£è¦åŒ–ã«ã‚ˆã‚‹é‡è¤‡æ¤œå‡º
- è¨˜äº‹æœ¬æ–‡ã®é¡ä¼¼åº¦åˆ¤å®š
- åŠ¹ç‡çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯
- é‡è¤‡è¨˜äº‹ã®çµ±åˆãƒ»ç®¡ç†

#### **æŠ€è¡“ä»•æ§˜**

##### **2.2.1 URLæ­£è¦åŒ–**
```python
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

class URLNormalizer:
    @staticmethod
    def normalize_url(url: str) -> str:
        """URLæ­£è¦åŒ–å‡¦ç†"""
        parsed = urlparse(url.lower())
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        query_params = parse_qs(parsed.query)
        # ä¸è¦ãªãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
        tracking_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content', 'fbclid', 'gclid'}
        filtered_params = {k: v for k, v in query_params.items() if k not in tracking_params}
        normalized_query = urlencode(sorted(filtered_params.items()), doseq=True)
        
        # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
        normalized = urlunparse((
            parsed.scheme,
            parsed.netloc,
            parsed.path.rstrip('/'),
            parsed.params,
            normalized_query,
            ''  # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»
        ))
        
        return normalized
    
    @staticmethod
    def extract_article_id(url: str) -> Optional[str]:
        """è¨˜äº‹IDã®æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆåˆ¥å¯¾å¿œï¼‰"""
        patterns = {
            'reuters.com': r'/article/([a-zA-Z0-9\-]+)',
            'bloomberg.co.jp': r'/news/articles/([a-zA-Z0-9]+)',
        }
        
        for domain, pattern in patterns.items():
            if domain in url:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
        return None
```

##### **2.2.2 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é¡ä¼¼åº¦åˆ¤å®š**
```python
import hashlib
from difflib import SequenceMatcher
from typing import Set

class ContentDeduplicator:
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
    
    def generate_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""
        # æ­£è¦åŒ–å‡¦ç†
        normalized = self._normalize_content(content)
        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()
    
    def _normalize_content(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ­£è¦åŒ–"""
        # ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
        normalized = re.sub(r'\s+', ' ', content.strip())
        # å¥èª­ç‚¹ã®çµ±ä¸€
        normalized = normalized.replace('ã€', ',').replace('ã€‚', '.')
        return normalized.lower()
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—"""
        norm1 = self._normalize_content(text1)
        norm2 = self._normalize_content(text2)
        return SequenceMatcher(None, norm1, norm2).ratio()
    
    def is_duplicate(self, article1: dict, article2: dict) -> bool:
        """é‡è¤‡è¨˜äº‹åˆ¤å®š"""
        # URLæ­£è¦åŒ–å¾Œã®æ¯”è¼ƒ
        url1 = URLNormalizer.normalize_url(article1['url'])
        url2 = URLNormalizer.normalize_url(article2['url'])
        if url1 == url2:
            return True
        
        # è¨˜äº‹IDæ¯”è¼ƒï¼ˆåŒä¸€ã‚µã‚¤ãƒˆå†…ï¼‰
        id1 = URLNormalizer.extract_article_id(article1['url'])
        id2 = URLNormalizer.extract_article_id(article2['url'])
        if id1 and id2 and id1 == id2:
            return True
        
        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦
        title_similarity = self.calculate_similarity(article1['title'], article2['title'])
        if title_similarity > 0.9:
            return True
        
        # æœ¬æ–‡é¡ä¼¼åº¦ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if article1.get('body') and article2.get('body'):
            content_similarity = self.calculate_similarity(article1['body'], article2['body'])
            if content_similarity > self.similarity_threshold:
                return True
        
        return False
    
    def remove_duplicates(self, articles: List[dict]) -> List[dict]:
        """é‡è¤‡è¨˜äº‹é™¤å»"""
        unique_articles = []
        seen_hashes = set()
        
        for article in articles:
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹é«˜é€Ÿãƒã‚§ãƒƒã‚¯
            content_hash = self.generate_content_hash(article.get('body', article['title']))
            if content_hash in seen_hashes:
                continue
            
            # è©³ç´°é‡è¤‡ãƒã‚§ãƒƒã‚¯
            is_duplicate = any(
                self.is_duplicate(article, existing)
                for existing in unique_articles
            )
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_hashes.add(content_hash)
        
        return unique_articles
```

---

## ğŸ§ª 3. ãƒ†ã‚¹ãƒˆå®Ÿè£…

### 3.1 pyteståŸºç›¤

#### **è¦ä»¶**
- pytest + pytest-asyncio ã«ã‚ˆã‚‹éåŒæœŸãƒ†ã‚¹ãƒˆå¯¾å¿œ
- å˜ä½“ãƒ†ã‚¹ãƒˆãƒ»çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã®å®Ÿè£…
- ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ãŸå¤–éƒ¨ä¾å­˜ã®åˆ†é›¢
- é«˜ã„ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ã®é”æˆ

#### **æŠ€è¡“ä»•æ§˜**

##### **3.1.1 ãƒ†ã‚¹ãƒˆæ§‹é€ **
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              # pytestè¨­å®šãƒ»ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
â”œâ”€â”€ unit/                    # å˜ä½“ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_ai_summarizer.py
â”‚   â”œâ”€â”€ test_html_generator.py
â”‚   â”œâ”€â”€ test_scrapers/
â”‚   â”‚   â”œâ”€â”€ test_reuters.py
â”‚   â”‚   â””â”€â”€ test_bloomberg.py
â”‚   â””â”€â”€ test_gdocs/
â”‚       â””â”€â”€ test_client.py
â”œâ”€â”€ integration/             # çµ±åˆãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_scraping_flow.py
â”‚   â””â”€â”€ test_ai_processing.py
â”œâ”€â”€ e2e/                     # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_full_pipeline.py
â”œâ”€â”€ fixtures/                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ sample_articles.json
â”‚   â”œâ”€â”€ mock_responses/
â”‚   â””â”€â”€ test_config.json
â””â”€â”€ utils/                   # ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ mock_helpers.py
    â””â”€â”€ test_data_generators.py
```

##### **3.1.2 conftest.py**
```python
import pytest
import asyncio
from unittest.mock import MagicMock, patch
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.database import Base, DatabaseManager
from src.config import AppConfig

@pytest.fixture(scope="session")
def event_loop():
    """Session-scoped event loop for async tests"""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def test_config():
    """ãƒ†ã‚¹ãƒˆç”¨è¨­å®š"""
    return AppConfig(
        scraping=ScrapingConfig(hours_limit=1),
        ai=AIConfig(gemini_api_key="test_key"),
        google=GoogleConfig(
            drive_output_folder_id="test_folder",
            service_account_json='{"test": "data"}'
        )
    )

@pytest.fixture
def test_db():
    """ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"""
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    SessionLocal = sessionmaker(bind=engine)
    
    db = DatabaseManager(":memory:")
    yield db
    engine.dispose()

@pytest.fixture
def mock_webdriver():
    """MockedWebDriver"""
    with patch('selenium.webdriver.Chrome') as mock_driver:
        driver_instance = MagicMock()
        mock_driver.return_value = driver_instance
        driver_instance.page_source = "<html>Mock HTML</html>"
        yield driver_instance

@pytest.fixture
def mock_gemini_api():
    """Mocked Gemini API"""
    with patch('google.generativeai.GenerativeModel') as mock_model:
        mock_response = MagicMock()
        mock_response.text = '{"summary": "Test summary", "sentiment_label": "Positive", "sentiment_score": 0.8}'
        mock_model.return_value.generate_content.return_value = mock_response
        yield mock_model

@pytest.fixture
def sample_articles():
    """ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹ãƒ‡ãƒ¼ã‚¿"""
    return [
        {
            'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹1',
            'url': 'https://example.com/article1',
            'body': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹ã®æœ¬æ–‡ã§ã™ã€‚',
            'source': 'Reuters',
            'category': 'ãƒ“ã‚¸ãƒã‚¹',
            'published_jst': datetime(2024, 1, 1, 12, 0, 0)
        },
        {
            'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹2', 
            'url': 'https://example.com/article2',
            'body': 'ã‚‚ã†ä¸€ã¤ã®ãƒ†ã‚¹ãƒˆè¨˜äº‹ã§ã™ã€‚',
            'source': 'Bloomberg',
            'category': 'ãƒãƒ¼ã‚±ãƒƒãƒˆ',
            'published_jst': datetime(2024, 1, 1, 13, 0, 0)
        }
    ]
```

##### **3.1.3 å˜ä½“ãƒ†ã‚¹ãƒˆä¾‹**
```python
# tests/unit/test_ai_summarizer.py
import pytest
from unittest.mock import patch, MagicMock

from src.ai_summarizer import process_article_with_ai

@pytest.mark.asyncio
async def test_process_article_with_ai_success(mock_gemini_api):
    """AIå‡¦ç†æˆåŠŸãƒ†ã‚¹ãƒˆ"""
    api_key = "test_key"
    text = "ãƒ†ã‚¹ãƒˆè¨˜äº‹æœ¬æ–‡"
    
    result = await process_article_with_ai(api_key, text)
    
    assert result is not None
    assert result['summary'] == "Test summary"
    assert result['sentiment_label'] == "Positive"
    assert result['sentiment_score'] == 0.8

@pytest.mark.asyncio
async def test_process_article_with_ai_api_error():
    """API ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    with patch('google.generativeai.GenerativeModel') as mock_model:
        mock_model.return_value.generate_content.side_effect = Exception("API Error")
        
        result = await process_article_with_ai("test_key", "text")
        assert result is None

@pytest.mark.parametrize("invalid_response", [
    "",  # ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    "Invalid JSON",  # ç„¡åŠ¹ãªJSON
    '{"summary": ""}',  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸è¶³
])
@pytest.mark.asyncio
async def test_process_article_with_ai_invalid_response(invalid_response):
    """ç„¡åŠ¹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    with patch('google.generativeai.GenerativeModel') as mock_model:
        mock_response = MagicMock()
        mock_response.text = invalid_response
        mock_model.return_value.generate_content.return_value = mock_response
        
        result = await process_article_with_ai("test_key", "text")
        assert result is None
```

##### **3.1.4 çµ±åˆãƒ†ã‚¹ãƒˆä¾‹**
```python
# tests/integration/test_database.py
import pytest
from datetime import datetime

from src.database import DatabaseManager, Article, AIAnalysis

@pytest.mark.asyncio
async def test_save_and_retrieve_article(test_db):
    """è¨˜äº‹ä¿å­˜ãƒ»å–å¾—ãƒ†ã‚¹ãƒˆ"""
    article_data = {
        'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹',
        'url': 'https://example.com/test',
        'body': 'ãƒ†ã‚¹ãƒˆæœ¬æ–‡',
        'source': 'Reuters',
        'published_jst': datetime.now()
    }
    
    # ä¿å­˜
    saved_article = test_db.save_article(article_data)
    assert saved_article.id is not None
    
    # å–å¾—
    retrieved = test_db.get_recent_articles(hours=24)
    assert len(retrieved) == 1
    assert retrieved[0].title == 'ãƒ†ã‚¹ãƒˆè¨˜äº‹'

@pytest.mark.asyncio
async def test_duplicate_detection(test_db):
    """é‡è¤‡æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    # åŒã˜URLè¨˜äº‹ã‚’2å›ä¿å­˜
    article_data = {
        'title': 'ãƒ†ã‚¹ãƒˆè¨˜äº‹',
        'url': 'https://example.com/test',
        'body': 'ãƒ†ã‚¹ãƒˆæœ¬æ–‡',
        'source': 'Reuters'
    }
    
    first_save = test_db.save_article(article_data)
    second_save = test_db.save_article(article_data)
    
    # åŒã˜è¨˜äº‹ãŒè¿”ã•ã‚Œã‚‹ï¼ˆé‡è¤‡ãªã—ï¼‰
    assert first_save.id == second_save.id
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã¯1ä»¶ã®ã¿
    all_articles = test_db.get_recent_articles(hours=24)
    assert len(all_articles) == 1
```

---

## ğŸ¨ 4. UI/UXæ”¹å–„

### 4.1 HTMLãƒšãƒ¼ã‚¸æ”¹å–„

#### **è¦ä»¶**
- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³å¯¾å¿œ
- ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰/ãƒ©ã‚¤ãƒˆãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
- æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### **æŠ€è¡“ä»•æ§˜**

##### **4.1.1 ãƒ¢ãƒ€ãƒ³HTMLæ§‹é€ **
```html
<!DOCTYPE html>
<html lang="ja" data-theme="auto">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Market News Dashboard</title>
    
    <!-- CSS Framework -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@2.0.6/css/pico.min.css">
    
    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/css/custom.css">
    
    <!-- PWAå¯¾å¿œ -->
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#1976d2">
    
    <!-- OG Tags -->
    <meta property="og:title" content="Market News Dashboard">
    <meta property="og:description" content="AIãŒè¦ç´„ã—ãŸæœ€æ–°ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹">
    <meta property="og:type" content="website">
</head>
<body>
    <header class="container">
        <nav>
            <ul>
                <li><h1>ğŸ“Š Market News</h1></li>
            </ul>
            <ul>
                <li>
                    <button id="theme-toggle" class="outline" aria-label="ãƒ†ãƒ¼ãƒåˆ‡ã‚Šæ›¿ãˆ">
                        ğŸŒ™
                    </button>
                </li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <!-- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»æ¤œç´¢ãƒãƒ¼ -->
        <section class="filter-section">
            <div class="grid">
                <div>
                    <input type="search" id="search-input" placeholder="è¨˜äº‹ã‚’æ¤œç´¢..." />
                </div>
                <div>
                    <select id="source-filter">
                        <option value="">å…¨ã¦ã®ã‚½ãƒ¼ã‚¹</option>
                        <option value="Reuters">Reuters</option>
                        <option value="Bloomberg">Bloomberg</option>
                    </select>
                </div>
                <div>
                    <select id="sentiment-filter">
                        <option value="">å…¨ã¦ã®æ„Ÿæƒ…</option>
                        <option value="Positive">ãƒã‚¸ãƒ†ã‚£ãƒ–</option>
                        <option value="Negative">ãƒã‚¬ãƒ†ã‚£ãƒ–</option>
                        <option value="Neutral">ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«</option>
                    </select>
                </div>
            </div>
        </section>

        <!-- çµ±è¨ˆæƒ…å ± -->
        <section class="stats-section">
            <div class="grid">
                <div class="stat-card">
                    <h3>ç·è¨˜äº‹æ•°</h3>
                    <p class="stat-number" id="total-articles">0</p>
                </div>
                <div class="stat-card">
                    <h3>æ„Ÿæƒ…åˆ†å¸ƒ</h3>
                    <canvas id="sentiment-chart" width="200" height="100"></canvas>
                </div>
                <div class="stat-card">
                    <h3>æœ€çµ‚æ›´æ–°</h3>
                    <p class="stat-number" id="last-updated">-</p>
                </div>
            </div>
        </section>

        <!-- è¨˜äº‹ä¸€è¦§ -->
        <section id="articles-container">
            <!-- JavaScript ã§å‹•çš„ç”Ÿæˆ -->
        </section>

        <!-- ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼ -->
        <div id="loading" class="text-center" style="display: none;">
            <div class="spinner"></div>
        </div>
    </main>

    <footer class="container">
        <p><small>Powered by Gemini AI & Python Scrapers</small></p>
    </footer>

    <!-- JavaScript -->
    <script src="assets/js/chart.min.js"></script>
    <script src="assets/js/app.js"></script>
</body>
</html>
```

##### **4.1.2 CSSè¨­è¨ˆ**
```css
/* assets/css/custom.css */

:root {
  --sentiment-positive: #22c55e;
  --sentiment-negative: #ef4444;
  --sentiment-neutral: #6b7280;
  --transition-fast: 150ms ease-in-out;
  --shadow-card: 0 1px 3px rgba(0, 0, 0, 0.1);
}

/* ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œ */
[data-theme="dark"] {
  --sentiment-positive: #16a34a;
  --sentiment-negative: #dc2626;
  --sentiment-neutral: #9ca3af;
}

/* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã‚°ãƒªãƒƒãƒ‰ */
.grid {
  display: grid;
  gap: 1rem;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
}

/* çµ±è¨ˆã‚«ãƒ¼ãƒ‰ */
.stat-card {
  background: var(--pico-card-background-color);
  border-radius: var(--pico-border-radius);
  padding: 1.5rem;
  box-shadow: var(--shadow-card);
  text-align: center;
  transition: transform var(--transition-fast);
}

.stat-card:hover {
  transform: translateY(-2px);
}

.stat-number {
  font-size: 2rem;
  font-weight: bold;
  margin: 0.5rem 0;
  color: var(--pico-primary);
}

/* è¨˜äº‹ã‚«ãƒ¼ãƒ‰ */
.article-card {
  background: var(--pico-card-background-color);
  border-radius: var(--pico-border-radius);
  padding: 1.5rem;
  margin-bottom: 1.5rem;
  box-shadow: var(--shadow-card);
  border-left: 4px solid transparent;
  transition: all var(--transition-fast);
}

.article-card:hover {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

.article-card.positive {
  border-left-color: var(--sentiment-positive);
}

.article-card.negative {
  border-left-color: var(--sentiment-negative);
}

.article-card.neutral {
  border-left-color: var(--sentiment-neutral);
}

/* æ„Ÿæƒ…ã‚¢ã‚¤ã‚³ãƒ³ */
.sentiment-icon {
  display: inline-flex;
  align-items: center;
  gap: 0.5rem;
  font-size: 1.2rem;
  margin-bottom: 0.5rem;
}

.sentiment-score {
  font-size: 0.8rem;
  opacity: 0.7;
}

/* æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ */
.filter-section {
  background: var(--pico-card-background-color);
  border-radius: var(--pico-border-radius);
  padding: 1.5rem;
  margin-bottom: 2rem;
  box-shadow: var(--shadow-card);
}

/* ã‚¹ãƒ”ãƒŠãƒ¼ */
.spinner {
  border: 3px solid var(--pico-form-element-border-color);
  border-top: 3px solid var(--pico-primary);
  border-radius: 50%;
  width: 40px;
  height: 40px;
  animation: spin 1s linear infinite;
  margin: 0 auto;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

/* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ */
@media (max-width: 768px) {
  .grid {
    grid-template-columns: 1fr;
  }
  
  .container {
    padding: 0 1rem;
  }
  
  .stat-card {
    padding: 1rem;
  }
  
  .article-card {
    padding: 1rem;
  }
}

/* ãƒ—ãƒªãƒ³ãƒˆå¯¾å¿œ */
@media print {
  .filter-section,
  #theme-toggle,
  .sentiment-icon {
    display: none;
  }
  
  .article-card {
    break-inside: avoid;
    margin-bottom: 1rem;
    box-shadow: none;
    border: 1px solid #ddd;
  }
}
```

##### **4.1.3 JavaScriptæ©Ÿèƒ½**
```javascript
// assets/js/app.js

class MarketNewsApp {
    constructor() {
        this.articles = [];
        this.filteredArticles = [];
        this.currentPage = 1;
        this.articlesPerPage = 10;
        this.sentimentChart = null;
        
        this.init();
    }
    
    async init() {
        this.setupEventListeners();
        await this.loadArticles();
        this.renderStats();
        this.renderArticles();
        this.initChart();
        this.loadTheme();
    }
    
    setupEventListeners() {
        // ãƒ†ãƒ¼ãƒåˆ‡ã‚Šæ›¿ãˆ
        document.getElementById('theme-toggle').addEventListener('click', () => {
            this.toggleTheme();
        });
        
        // æ¤œç´¢
        const searchInput = document.getElementById('search-input');
        searchInput.addEventListener('input', debounce(() => {
            this.filterArticles();
        }, 300));
        
        // ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        document.getElementById('source-filter').addEventListener('change', () => {
            this.filterArticles();
        });
        
        document.getElementById('sentiment-filter').addEventListener('change', () => {
            this.filterArticles();
        });
        
        // ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        window.addEventListener('scroll', () => {
            if (this.isNearBottom()) {
                this.loadMoreArticles();
            }
        });
    }
    
    async loadArticles() {
        try {
            // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å–å¾—
            const response = await fetch('/api/articles');
            this.articles = await response.json();
            this.filteredArticles = [...this.articles];
        } catch (error) {
            console.error('è¨˜äº‹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—:', error);
            // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            this.articles = window.articlesData || [];
            this.filteredArticles = [...this.articles];
        }
    }
    
    filterArticles() {
        const searchTerm = document.getElementById('search-input').value.toLowerCase();
        const sourceFilter = document.getElementById('source-filter').value;
        const sentimentFilter = document.getElementById('sentiment-filter').value;
        
        this.filteredArticles = this.articles.filter(article => {
            const matchesSearch = !searchTerm || 
                article.title.toLowerCase().includes(searchTerm) ||
                article.summary.toLowerCase().includes(searchTerm);
            
            const matchesSource = !sourceFilter || article.source === sourceFilter;
            const matchesSentiment = !sentimentFilter || article.sentiment_label === sentimentFilter;
            
            return matchesSearch && matchesSource && matchesSentiment;
        });
        
        this.currentPage = 1;
        this.renderArticles();
        this.renderStats();
    }
    
    renderArticles() {
        const container = document.getElementById('articles-container');
        const startIndex = 0;
        const endIndex = this.currentPage * this.articlesPerPage;
        const articlesToShow = this.filteredArticles.slice(startIndex, endIndex);
        
        if (this.currentPage === 1) {
            container.innerHTML = '';
        }
        
        articlesToShow.forEach((article, index) => {
            if (index >= (this.currentPage - 1) * this.articlesPerPage) {
                container.appendChild(this.createArticleElement(article));
            }
        });
    }
    
    createArticleElement(article) {
        const element = document.createElement('article');
        element.className = `article-card ${article.sentiment_label?.toLowerCase() || 'neutral'}`;
        
        const sentimentIcon = this.getSentimentIcon(article.sentiment_label);
        const publishedDate = new Date(article.published_jst).toLocaleDateString('ja-JP');
        
        element.innerHTML = `
            <div class="sentiment-icon">
                ${sentimentIcon}
                <span class="sentiment-score">${article.sentiment_score?.toFixed(2) || 'N/A'}</span>
            </div>
            <h3><a href="${article.url}" target="_blank" rel="noopener">${article.title}</a></h3>
            <small><strong>[${article.source}]</strong> ${publishedDate}</small>
            <p>${article.summary || 'ã‚µãƒãƒªãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“'}</p>
        `;
        
        return element;
    }
    
    getSentimentIcon(sentiment) {
        const icons = {
            'Positive': 'ğŸ˜Š',
            'Negative': 'ğŸ˜ ', 
            'Neutral': 'ğŸ˜'
        };
        return icons[sentiment] || 'ğŸ¤”';
    }
    
    renderStats() {
        document.getElementById('total-articles').textContent = this.filteredArticles.length;
        
        if (this.articles.length > 0) {
            const lastUpdated = new Date(Math.max(...this.articles.map(a => new Date(a.published_jst))));
            document.getElementById('last-updated').textContent = lastUpdated.toLocaleString('ja-JP');
        }
        
        this.updateChart();
    }
    
    initChart() {
        const ctx = document.getElementById('sentiment-chart').getContext('2d');
        this.sentimentChart = new Chart(ctx, {
            type: 'doughnut',
            data: {
                labels: ['ãƒã‚¸ãƒ†ã‚£ãƒ–', 'ãƒã‚¬ãƒ†ã‚£ãƒ–', 'ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«'],
                datasets: [{
                    data: [0, 0, 0],
                    backgroundColor: [
                        'var(--sentiment-positive)',
                        'var(--sentiment-negative)',
                        'var(--sentiment-neutral)'
                    ]
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: {
                        position: 'bottom',
                        labels: {
                            font: { size: 10 }
                        }
                    }
                }
            }
        });
    }
    
    updateChart() {
        const sentimentCounts = {
            'Positive': 0,
            'Negative': 0,
            'Neutral': 0
        };
        
        this.filteredArticles.forEach(article => {
            const sentiment = article.sentiment_label || 'Neutral';
            sentimentCounts[sentiment]++;
        });
        
        this.sentimentChart.data.datasets[0].data = [
            sentimentCounts.Positive,
            sentimentCounts.Negative,
            sentimentCounts.Neutral
        ];
        this.sentimentChart.update();
    }
    
    toggleTheme() {
        const html = document.documentElement;
        const currentTheme = html.getAttribute('data-theme');
        const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
        
        html.setAttribute('data-theme', newTheme);
        localStorage.setItem('theme', newTheme);
        
        // ã‚¢ã‚¤ã‚³ãƒ³æ›´æ–°
        const button = document.getElementById('theme-toggle');
        button.textContent = newTheme === 'dark' ? 'â˜€ï¸' : 'ğŸŒ™';
    }
    
    loadTheme() {
        const savedTheme = localStorage.getItem('theme') || 'auto';
        document.documentElement.setAttribute('data-theme', savedTheme);
        
        const button = document.getElementById('theme-toggle');
        button.textContent = savedTheme === 'dark' ? 'â˜€ï¸' : 'ğŸŒ™';
    }
    
    isNearBottom() {
        return window.innerHeight + window.scrollY >= document.body.offsetHeight - 1000;
    }
    
    loadMoreArticles() {
        if (this.currentPage * this.articlesPerPage < this.filteredArticles.length) {
            this.currentPage++;
            this.renderArticles();
        }
    }
}

// ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
function debounce(func, wait) {
    let timeout;
    return function executedFunction(...args) {
        const later = () => {
            clearTimeout(timeout);
            func(...args);
        };
        clearTimeout(timeout);
        timeout = setTimeout(later, wait);
    };
}

// ã‚¢ãƒ—ãƒªåˆæœŸåŒ–
document.addEventListener('DOMContentLoaded', () => {
    new MarketNewsApp();
});
```

---

## âš¡ 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 5.1 éåŒæœŸå‡¦ç†å°å…¥

#### **è¦ä»¶**
- ThreadPoolExecutorã‹ã‚‰asyncio/aiohttpã¸ã®ç§»è¡Œ
- ä¸¦è¡Œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŠ¹ç‡åŒ–
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
- ã‚¨ãƒ©ãƒ¼æ™‚ã®å½±éŸ¿ç¯„å›²æœ€å°åŒ–

#### **æŠ€è¡“ä»•æ§˜**

##### **5.1.1 éåŒæœŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹**
```python
import asyncio
import aiohttp
from abc import ABC, abstractmethod
from typing import List, Optional, AsyncIterator
import logging

class AsyncNewsScraper(ABC):
    def __init__(self, config: ScrapingConfig, session: aiohttp.ClientSession):
        self.config = config
        self.session = session
        self.logger = logging.getLogger(self.__class__.__name__)
        self.semaphore = asyncio.Semaphore(5)  # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
    
    @abstractmethod
    async def scrape_articles(self) -> List[ArticleData]:
        """è¨˜äº‹åé›†ã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass
    
    async def _fetch_with_retry(
        self, 
        url: str, 
        max_retries: int = 3,
        **kwargs
    ) -> Optional[aiohttp.ClientResponse]:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        async with self.semaphore:
            for attempt in range(max_retries + 1):
                try:
                    async with self.session.get(url, **kwargs) as response:
                        response.raise_for_status()
                        return response
                except aiohttp.ClientError as e:
                    if attempt == max_retries:
                        self.logger.error(f"Failed to fetch {url} after {max_retries} retries: {e}")
                        return None
                    await asyncio.sleep(2 ** attempt)  # exponential backoff
    
    async def _fetch_article_content(self, url: str) -> Optional[str]:
        """è¨˜äº‹æœ¬æ–‡å–å¾—"""
        response = await self._fetch_with_retry(url)
        if response:
            content = await response.text()
            return await self._parse_article_body(content)
        return None
    
    @abstractmethod
    async def _parse_article_body(self, html_content: str) -> str:
        """HTMLè§£æã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰"""
        pass
```

##### **5.1.2 éåŒæœŸReuters ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼**
```python
from playwright.async_api import async_playwright
import asyncio

class AsyncReutersScraper(AsyncNewsScraper):
    async def scrape_articles(self) -> List[ArticleData]:
        """éåŒæœŸã§ãƒ­ã‚¤ã‚¿ãƒ¼è¨˜äº‹ã‚’åé›†"""
        articles = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            try:
                # è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ä¸¦è¡Œå‡¦ç†
                tasks = []
                for page_num in range(self.config.reuters.max_pages):
                    task = self._scrape_page(browser, page_num)
                    tasks.append(task)
                
                # å…¨ãƒšãƒ¼ã‚¸ã®çµæœã‚’ã¾ã¨ã‚ã‚‹
                page_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in page_results:
                    if isinstance(result, list):
                        articles.extend(result)
                    elif isinstance(result, Exception):
                        self.logger.error(f"Page scraping failed: {result}")
                
            finally:
                await browser.close()
        
        return articles
    
    async def _scrape_page(self, browser, page_num: int) -> List[ArticleData]:
        """å˜ä¸€ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        context = await browser.new_context()
        page = await context.new_page()
        
        try:
            # ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
            offset = page_num * self.config.reuters.items_per_page
            url = f"https://jp.reuters.com/site-search/?query={self.config.reuters.query}&offset={offset}"
            
            await page.goto(url, wait_until='networkidle')
            
            # è¨˜äº‹è¦ç´ å–å¾—
            article_elements = await page.query_selector_all('[data-testid="StoryCard"]')
            
            # å„è¨˜äº‹ã®è©³ç´°ã‚’ä¸¦è¡Œå–å¾—
            tasks = []
            for element in article_elements:
                task = self._extract_article_data(element, context)
                tasks.append(task)
            
            articles = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æˆåŠŸã—ãŸçµæœã®ã¿è¿”ã™
            return [article for article in articles if isinstance(article, ArticleData)]
            
        finally:
            await context.close()
    
    async def _extract_article_data(self, element, context) -> Optional[ArticleData]:
        """è¨˜äº‹è¦ç´ ã‹ã‚‰æƒ…å ±æŠ½å‡º"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLå–å¾—
            title_element = await element.query_selector('[data-testid="TitleLink"]')
            if not title_element:
                return None
            
            title = await title_element.text_content()
            url = await title_element.get_attribute('href')
            
            if url.startswith('/'):
                url = f"https://jp.reuters.com{url}"
            
            # æœ¬æ–‡ã‚’ä¸¦è¡Œå–å¾—
            body = await self._fetch_article_content(url)
            
            # ãã®ä»–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
            time_element = await element.query_selector('[data-testid="DateLineText"]')
            published_jst = await self._parse_publish_time(time_element) if time_element else None
            
            return ArticleData(
                title=title,
                url=url,
                body=body or "",
                source="Reuters",
                published_jst=published_jst
            )
            
        except Exception as e:
            self.logger.error(f"Failed to extract article data: {e}")
            return None
```

##### **5.1.3 éåŒæœŸAIå‡¦ç†**
```python
class AsyncAIProcessor:
    def __init__(self, api_key: str, max_concurrent: int = 10):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = aiohttp.ClientSession()
        
    async def process_articles_batch(
        self, 
        articles: List[ArticleData]
    ) -> List[ProcessedArticle]:
        """è¨˜äº‹ã®ä¸€æ‹¬AIå‡¦ç†"""
        tasks = []
        for article in articles:
            task = self._process_single_article(article)
            tasks.append(task)
        
        # ä¸¦è¡Œå‡¦ç†å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        processed_articles = []
        for article, result in zip(articles, results):
            if isinstance(result, dict):
                processed_article = ProcessedArticle(
                    **article.__dict__,
                    **result
                )
                processed_articles.append(processed_article)
            else:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                processed_article = ProcessedArticle(
                    **article.__dict__,
                    summary="AIå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    sentiment_label="Error",
                    sentiment_score=0.0
                )
                processed_articles.append(processed_article)
        
        return processed_articles
    
    async def _process_single_article(self, article: ArticleData) -> dict:
        """å˜ä¸€è¨˜äº‹ã®AIå‡¦ç†"""
        async with self.semaphore:
            try:
                # Gemini APIå‘¼ã³å‡ºã—ï¼ˆéåŒæœŸï¼‰
                result = await self._call_gemini_api(article.body)
                return result
            except Exception as e:
                logger.error(f"AI processing failed for article {article.url}: {e}")
                raise
    
    async def _call_gemini_api(self, text: str) -> dict:
        """Gemini APIéåŒæœŸå‘¼ã³å‡ºã—"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€google-generativeaiã®éåŒæœŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨
        # ã¾ãŸã¯ã€HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ç›´æ¥APIå‘¼ã³å‡ºã—
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}'
        }
        
        payload = {
            'contents': [{
                'parts': [{'text': config.AI_PROCESS_PROMPT_TEMPLATE.format(text=text)}]
            }],
            'generationConfig': {
                'maxOutputTokens': 1024,
                'temperature': 0.2
            }
        }
        
        async with self.session.post(
            'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite-001:generateContent',
            headers=headers,
            json=payload
        ) as response:
            response.raise_for_status()
            data = await response.json()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
            content = data['candidates'][0]['content']['parts'][0]['text']
            return self._parse_ai_response(content)
    
    def _parse_ai_response(self, response_text: str) -> dict:
        """AI ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ"""
        try:
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            json_str = response_text[json_start:json_end]
            
            result = json.loads(json_str)
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            required_fields = ['summary', 'sentiment_label', 'sentiment_score']
            if not all(field in result for field in required_fields):
                raise ValueError("Required fields missing in AI response")
            
            return result
            
        except (json.JSONDecodeError, ValueError) as e:
            raise ValueError(f"Invalid AI response format: {e}")
    
    async def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        await self.session.close()
```

---

## ğŸ“Š 6. ç›£è¦–ãƒ»ãƒ­ã‚°æ”¹å–„

### 6.1 æ§‹é€ åŒ–ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 

#### **è¦ä»¶**
- JSONå½¢å¼ã®æ§‹é€ åŒ–ãƒ­ã‚°
- ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®é©åˆ‡ãªåˆ†é›¢
- æ¤œç´¢ãƒ»é›†è¨ˆå¯èƒ½ãªå½¢å¼
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®ï¼ˆæ©Ÿå¯†æƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°ï¼‰

#### **æŠ€è¡“ä»•æ§˜**

##### **6.1.1 ãƒ­ã‚°è¨­å®š**
```python
import logging
import json
import sys
from datetime import datetime
from typing import Dict, Any, Optional
import os

class StructuredFormatter(logging.Formatter):
    """æ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼"""
    
    def format(self, record: logging.LogRecord) -> str:
        # åŸºæœ¬ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
        log_entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno,
            'process_id': os.getpid(),
            'thread_id': record.thread
        }
        
        # ä¾‹å¤–æƒ…å ±
        if record.exc_info:
            log_entry['exception'] = {
                'type': record.exc_info[0].__name__,
                'message': str(record.exc_info[1]),
                'traceback': self.formatException(record.exc_info)
            }
        
        # è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        if hasattr(record, 'extra_data'):
            log_entry['context'] = record.extra_data
        
        # æ©Ÿå¯†æƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        log_entry = self._mask_sensitive_data(log_entry)
        
        return json.dumps(log_entry, ensure_ascii=False)
    
    def _mask_sensitive_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ©Ÿå¯†æƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°"""
        sensitive_keys = {
            'api_key', 'token', 'password', 'secret', 'credentials',
            'gemini_api_key', 'service_account_json'
        }
        
        def mask_recursive(obj):
            if isinstance(obj, dict):
                return {
                    key: '***MASKED***' if any(sensitive in key.lower() for sensitive in sensitive_keys)
                    else mask_recursive(value)
                    for key, value in obj.items()
                }
            elif isinstance(obj, list):
                return [mask_recursive(item) for item in obj]
            else:
                return obj
        
        return mask_recursive(data)

class LoggerManager:
    """ãƒ­ã‚°ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, app_name: str = "market_news"):
        self.app_name = app_name
        self.setup_logging()
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®šåˆæœŸåŒ–"""
        # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®š
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)
        
        # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å‰Šé™¤
        for handler in root_logger.handlers[:]:
            root_logger.removeHandler(handler)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(StructuredFormatter())
        root_logger.addHandler(console_handler)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler(
            f'logs/{self.app_name}.log',
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setFormatter(StructuredFormatter())
        root_logger.addHandler(file_handler)
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«
        error_handler = RotatingFileHandler(
            f'logs/{self.app_name}_errors.log',
            maxBytes=10*1024*1024,
            backupCount=10
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(StructuredFormatter())
        root_logger.addHandler(error_handler)
    
    def get_logger(self, name: str) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼å–å¾—"""
        return logging.getLogger(f"{self.app_name}.{name}")

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ­ã‚°é–¢æ•°
def log_with_context(logger: logging.Logger, level: int, message: str, **context):
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ä»˜ããƒ­ã‚°å‡ºåŠ›"""
    extra_data = {
        'operation': context.get('operation'),
        'url': context.get('url'),
        'duration_ms': context.get('duration_ms'),
        'status': context.get('status'),
        'count': context.get('count'),
        **{k: v for k, v in context.items() if k not in ['operation', 'url', 'duration_ms', 'status', 'count']}
    }
    
    # Noneå€¤ã‚’é™¤å»
    extra_data = {k: v for k, v in extra_data.items() if v is not None}
    
    logger.log(level, message, extra={'extra_data': extra_data})

# ä½¿ç”¨ä¾‹
logger_manager = LoggerManager()
scraper_logger = logger_manager.get_logger('scraper')

# ä½¿ç”¨æ–¹æ³•
log_with_context(
    scraper_logger, 
    logging.INFO, 
    "è¨˜äº‹åé›†å®Œäº†",
    operation="scrape_articles",
    source="Reuters",
    count=25,
    duration_ms=5420
)
```

---

## ğŸ”§ å®Ÿè£…ä¾å­˜é–¢ä¿‚ã¨é †åº

### å®Ÿè£…é †åºï¼ˆä¾å­˜é–¢ä¿‚è€ƒæ…®ï¼‰

#### **Phase 1: åŸºç›¤æ•´å‚™ï¼ˆ1-2é€±é–“ï¼‰**
1. **å‹ãƒ’ãƒ³ãƒˆè¿½åŠ ** â†’ å…¨ãƒ•ã‚¡ã‚¤ãƒ«
2. **è¨­å®šç®¡ç†æ”¹å–„** â†’ Pydanticå°å…¥
3. **ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰** â†’ æ§‹é€ åŒ–ãƒ­ã‚°
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–** â†’ ãƒªãƒˆãƒ©ã‚¤ãƒ»ä¾‹å¤–å‡¦ç†

#### **Phase 2: ãƒ‡ãƒ¼ã‚¿åŸºç›¤ï¼ˆ1-2é€±é–“ï¼‰**  
1. **SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** â†’ ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆãƒ»ORM
2. **é‡è¤‡æ’é™¤æ©Ÿèƒ½** â†’ URLæ­£è¦åŒ–ãƒ»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¯”è¼ƒ
3. **åŸºæœ¬ãƒ†ã‚¹ãƒˆ** â†’ pytestè¨­å®šãƒ»å˜ä½“ãƒ†ã‚¹ãƒˆ

#### **Phase 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼ˆ2-3é€±é–“ï¼‰**
1. **éåŒæœŸå‡¦ç†** â†’ asyncio/aiohttpå°å…¥
2. **UIæ”¹å–„** â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ»ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰
3. **çµ±åˆãƒ†ã‚¹ãƒˆ** â†’ ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ

### å¿…è¦ãªæ–°ã—ã„ä¾å­˜é–¢ä¿‚

```txt
# requirements_new.txtï¼ˆè¿½åŠ åˆ†ï¼‰
pydantic>=2.0.0
sqlalchemy>=2.0.0
alembic>=1.12.0
pytest>=7.4.0
pytest-asyncio>=0.21.0
aiohttp>=3.8.0
playwright>=1.40.0
structlog>=23.1.0
```

---

## ğŸ“‹ å®Ÿè£…å®Œäº†æ™‚ã®æ¤œè¨¼é …ç›®

### **ã‚³ãƒ¼ãƒ‰å“è³ª**
- [ ] mypy ã‚¨ãƒ©ãƒ¼ã‚¼ãƒ­
- [ ] pytest ã‚«ãƒãƒ¬ãƒƒã‚¸ >80%
- [ ] flake8/black æº–æ‹ 
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³é€šé

### **æ©Ÿèƒ½æ¤œè¨¼**
- [ ] è¨˜äº‹åé›†ã®æ­£å¸¸å‹•ä½œ
- [ ] AIå‡¦ç†ã®ä¸¦è¡Œå®Ÿè¡Œ
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
- [ ] é‡è¤‡æ’é™¤æ©Ÿèƒ½
- [ ] HTMLç”Ÿæˆãƒ»è¡¨ç¤º

### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
- [ ] å‡¦ç†æ™‚é–“æ¸¬å®š
- [ ] ã‚¨ãƒ©ãƒ¼ç‡ç›£è¦–
- [ ] ãƒ­ã‚°å‡ºåŠ›ç¢ºèª

---

*ã“ã®è¦ä»¶å®šç¾©æ›¸ã«åŸºã¥ã„ã¦ã€æ®µéšçš„ã«å®Ÿè£…ã‚’é€²ã‚ã¦ã„ãã¾ã™ã€‚å„ãƒ•ã‚§ãƒ¼ã‚ºã®å®Œäº†æ™‚ã«å‹•ä½œç¢ºèªã¨å“è³ªãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã«é€²ã¿ã¾ã™ã€‚*